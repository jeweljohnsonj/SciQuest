[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nUsing Obsidian as a note-taking software for academic purposes\n\n\n\nPhD\n\n\nObsidian\n\n\n\nLearn how to use Obsidian to write notes on literature articles\n\n\n\nJewel Johnson\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning starts in the embryo for fathead minnow fish\n\n\n\nresearch article\n\n\nscicom\n\n\n\nEmbryonic learning occurs when an organism can learn while in its embryo stage. Learn how researchers showed that fathead minnow embryos (Pimephales promelas) can detect‚Ä¶\n\n\n\nJewel Johnson\n\n\nJul 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe world‚Äôs largest bacteria is bigger than a housefly\n\n\n\nresearch article\n\n\nscicom\n\n\n\nMeet Thiomargarita magnifica, a recently discovered species of bacteria that is a whopping 1 cm in length, making it the largest bacteria ever till now.\n\n\n\nJewel Johnson\n\n\nJul 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion to keep a movie and a book list tracker\n\n\n\nNotion\n\n\n\nThis a tutorial on how you can use Notion to maintain reading and watch list tracker\n\n\n\nJewel Johnson\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe story behind one carat blog\n\n\n\nnews\n\n\n\nIn this article, I share my experiences on how this blog was made. Spoiler alter: it involved a lot of R package hitchhiking\n\n\n\nJewel Johnson\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion as your task manager\n\n\n\nPhD\n\n\nNotion\n\n\n\nIn this article I will showcase to you a template that I made, which can intelligently record and track tasks: you very own personal task manager with the help of Notion\n\n\n\nJewel Johnson\n\n\nJun 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion and Zotero to build a literature tracker\n\n\n\nPhD\n\n\nNotion\n\n\nZotero\n\n\n\nIn this tutorial we will learn how we can use Notion to make a literature database. With the help of the Notero plugin, our database will be synced with our Zotero‚Ä¶\n\n\n\nJewel Johnson\n\n\nJun 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Notion to keep a research diary\n\n\n\nPhD\n\n\nNotion\n\n\n\nThis a tutorial on how you can use Notion to maintain a research diary if you are a researcher or a PhD student\n\n\n\nJewel Johnson\n\n\nJun 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Notion story\n\n\n\nNotion\n\n\nZotero\n\n\n\nNotion is a powerful productivity software that implements database creation in its core function. In this article, I will share my story on what led me to use Notion and‚Ä¶\n\n\n\nJewel Johnson\n\n\nJun 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe World Happiness Report 2022\n\n\n\nmaps\n\n\nleaflet\n\n\n\nPlotting an interactive map using the {leaflet} package in R\n\n\n\nJewel Johnson\n\n\nMay 20, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SciQuest",
    "section": "",
    "text": "Hey there! My name is Jewel Johnson, and this is my blog showcasing my journey in exploring science. I am interested in studying animal behavior and am currently an incoming PhD student at Ball State University in Indiana, where I will be studying embryonic social learning in fathead minnow fish.\n\n[Past] I completed my Master‚Äôs in Biology from the Indian Institute of Science Education and Research, Thiruvananthapuram (IISER-TVM). My Master‚Äôs thesis was on understanding the visual ecology of the giant honeybee (Apis dorsata), under the guidance of Prof.¬†Hema Somanathan at IISER-TVM.\n[Future] I am now an incoming PhD student in Dr.¬†Jessica Ward‚Äôs lab at Ball State University and will be studying embryonic social learning using the fathead minnow as a model organism.\n\nI created this blog using the quarto package. My blog posts generally focus on science communication and R programming. I also believe that teaching refines one‚Äôs ability to learn and comprehend. Therefore, I document my learning experience by writing extensive tutorials on the topics I am learning. You can find them in the ‚ÄúTutorials‚Äù section in the navigation bar."
  },
  {
    "objectID": "posts/notion_intro/index.html",
    "href": "posts/notion_intro/index.html",
    "title": "My Notion story",
    "section": "",
    "text": "Notion is a productivity software that can be used to create databases, manage tasks, take notes etc.\nDownload Notion and using the links given below, ‚ÄòDuplicate‚Äô the templates to your workspace\nMade five templates using Notion which features a literature tracker, research diary, task manager, book list tracker, movie list tracker and a finance tracker\nThose who want to download all the templates in one go, click here"
  },
  {
    "objectID": "posts/notion_intro/index.html#tldr",
    "href": "posts/notion_intro/index.html#tldr",
    "title": "My Notion story",
    "section": "",
    "text": "Notion is a productivity software that can be used to create databases, manage tasks, take notes etc.\nDownload Notion and using the links given below, ‚ÄòDuplicate‚Äô the templates to your workspace\nMade five templates using Notion which features a literature tracker, research diary, task manager, book list tracker, movie list tracker and a finance tracker\nThose who want to download all the templates in one go, click here"
  },
  {
    "objectID": "posts/notion_intro/index.html#what-is-notion",
    "href": "posts/notion_intro/index.html#what-is-notion",
    "title": "My Notion story",
    "section": "2 What is Notion?",
    "text": "2 What is Notion?\nNotion is a productivity software which can be used to create databases, manage tasks, take notes etc. Its best known for its ability to make large collaborative databases where you can invite and work together with other people. In simple words, Notion can give you a personal Wiki experience by helping you manage your everyday activities.\n\n\n\n\nMy Notion dashboard\n\n\n\nHere is an introduction video from Notion themselves."
  },
  {
    "objectID": "posts/notion_intro/index.html#my-notion-story",
    "href": "posts/notion_intro/index.html#my-notion-story",
    "title": "My Notion story",
    "section": "3 My Notion story",
    "text": "3 My Notion story\nMy journey in discovering Notion came from my quest to find software that can track the research papers that I read. Naturally for this purpose, I began to use a reference manager software, Zotero, which is free to use and is open source. Zotero even comes in with a built-in pdf reader which you can use to highlight, comment and insert notes within a research paper This was very helpful as it enabled me to write my thoughts about the paper and summarize its results, which can be helpful if you are writing a research paper for yourself where you will be citing other papers. But soon I realized a big problem. The problem is that Zotero does not provide a clean user interface to show the summary notes I made on a research paper. What I was looking for was a tabular interface which had columns titled ‚Äòtitle of the paper‚Äô, which is followed by the ‚Äòsummary‚Äô column, where I list down a summary of the paper as bullet points. The rest of the columns can contain other relevant information about the paper.\n\n\nThe table interface I was looking for\n\n\n\n\n\n\n\n\n\n\nTitle\nAuthors\nDate added\nYear of publication\nSummary\nURL\n\n\n\n\nGlyphosate impairs collective thermoregulation in bumblebees\n(Weidenm√ºller et al., 2022)\n22-June-2022\n2022\n1. Glyphosate sugar water reduced life expectancy of worker bees2. Glyphosate affected bees invest less time in incubating the brood\nhttps://www.science.org/doi/10.1126/science.abf7482\n\n\n\n\nNow Zotero is not expected to have features like this simply because it is a reference manager and not a note-taking application. So I began looking for an application which can provide this exact feature. A simple Google search lead me to people suggesting Microsoft Excel and Google docs as their note-taking apps for summarizing the research papers they read. It sure works but is extremely cumbersome. You have to manually fill in all the columns and that is unnecessary work. Also, it‚Äôs not an elegant way of categorizing the data. I wanted something highly customizable and at the same time automated to an extent. Something similar to Zotero, where you can just simply add the pdf file of a research article and it automatically populates relevant details like Title, Authors, Year etc by grabbing information from the internet. And to my surprise, I came across a tweet on Twitter which had exactly what I was looking for. Thank you @thoughtsofaphd for that tweet.\n\n\nLiterature Tracker 2.0: Create a FREE customizable database in @airtable for keeping track of all the papers you download (and maybe readüòÇ). Please share if you find it helpful!‚ö°Ô∏èHere‚Äôs an easy step-by-step guide: https://t.co/jBQ0XeQahgcc @AcademicChatter pic.twitter.com/jLAkIlM2wQ\n\n‚Äî thoughtsofaphd (@thoughtsofaphd) January 7, 2022\n\n\nIn the tweet, I learned about a new software called Airtable and by following the guide given in the tweet, I made myself a literature tracker for managing the summaries of the papers I read.\n\n\n\n\nMy literature tracker template in Airtable\n\n\n\nEverything was going smoothly and I was in love with the new user interface Airtable provided. But I was still annoyed by the fact that I have to manually type in the basic information like ‚Äúname of the paper‚Äù, ‚Äúauthors name‚Äù etc. and quite frankly it was becoming quite tedious as the number of papers in my collection grew in number. Humans aren‚Äôt meant to write standard titles. If there was some way to connect Zotero to Airtable that would have made my life easy. So upon searching for a solution I came to know that there is an integration feature developed by Avana Vana. But upon reading about the feature on Github I quickly realized that it was beyond my current ability to comprehend what was happening and how to implement it. So I did what any honest person would have done after that. I gave up on Airtable and went to look for an alternative. And that‚Äôs how I came to meet Notion!\nAs I began learning about Notion I quickly realized the rich potential it offered. Initially, I just wanted a tabular database where I could efficiently write down the summaries of the papers that I read. But upon seeing how others use Notion in their daily life, I couldn‚Äôt help myself to try those features, expand on them and implement them in my day-to-day activities. So after putting in some hours in learning Notion, I was able to create a beautiful template which housed a database storing all the details of the papers I read. In addition to this template, I also made a few other ones focusing on various other day-to-day activities.\n\n\n\n\nMy literature tracker template in Notion\n\n\n\nThis new template is a significantly improved version of the Airtable template I had before. One of the best improvements was that this one is now automated. So no more wasting time by manually typing words in boring columns. With the help of the plugin Notero by David Vanoni, you can seamlessly integrate your Zotero literature collection with your workspace in Notion. This plugin was a game-changer for me. And thus began my Notion journey starting from this literature tracker template. In addition to this literature tracker template, I also made a few other ones focusing on various other day-to-day activities. If you are interested in how to use the template in Notion and how to get them, I have written separate articles, focusing on each of these templates. You can learn more about this at the end of this article."
  },
  {
    "objectID": "posts/notion_intro/index.html#do-you-need-notion",
    "href": "posts/notion_intro/index.html#do-you-need-notion",
    "title": "My Notion story",
    "section": "4 Do you need Notion?",
    "text": "4 Do you need Notion?\nConsider Notion as a LEGO set that you had when you were a child. You can build almost anything with Notion and it is tailored to give you a personal Wiki experience. You are mostly only limited by your creativity and needs. The learning curve for Notion is not a hard one and it‚Äôs pretty much self-intuitive. Whatever I was able to do until now required only a week of self-learning. With that said you can also get a wide variety of templates, mostly for free which can satisfy most of your needs. Most features of Notion are free for personal use but they also have different subscription-based plans for advanced power users. You can learn more about their plans here. Also, keep in mind that you need an active internet connection to use Notion.\nNow let us answer the elephant in the room. Is it secure? The answer is yes and no. Notion uses encryption to encrypt data when it is been sent and received between the user and Notion. But created databases that are stored in Notion‚Äôs cloud service are not encrypted. This means employees of Notion can access your data. Notion has a strict data security policy and claims that they will only access user data with prior permission from the users themselves. The reason why they don‚Äôt use end-to-end encryption is because it would make certain features like ‚Äòfull-text search‚Äô impossible to implement. As for Notion‚Äôs data security commitment, they are part of the Security First Initiative which pledges to put security first by sharing their security information proactively with their customers. So in my opinion I think it‚Äôs safe to use Notion unless you create tables containing your credit card info or account passwords. If you are a researcher, I would also recommend that you do not attach any sensitive research data to Notion. With over 30 million users and with strong ideals on user security and data protection, Notion is as good as any other productive software like Slack, Evernote etc. in terms of data security. So I don‚Äôt think you will face any problem when adopting Notion for your daily activities."
  },
  {
    "objectID": "posts/notion_intro/index.html#installing-notion",
    "href": "posts/notion_intro/index.html#installing-notion",
    "title": "My Notion story",
    "section": "5 Installing Notion",
    "text": "5 Installing Notion\nIf you are interested to try out Notion and seeing if it works for you, then please follow these steps.\n\nGo to Notion website and sign up for a free account. You can use your Gmail account for a quick sign-in.\nNotion can be used from your browser itself. But if you want to work from your desktop, please install the desktop app.\nNow you are ready to use Notion.\n\nIt can be pretty overwhelming at first, but as soon as you get familiar, you will learn to do a lot of cool new stuff with it. There are many YouTube videos and web articles that will walk you through the basics of Notion. A good place to start is by watching this YouTube video by Notion itself where they introduce the basics of Notion.\n\n\nNow my goal is to not make you watch multiple tutorial videos and bore yourself to death. Instead, I would like you to have a goal in mind. How I came to learn Notion is that, as I mentioned before, I wanted to have a ‚Äòliterature tracker‚Äô, so having a goal sets me up to see which features in Notion would help me build it. As Notion has a lot of features it can be too much on your plate in trying to learn about all these features. So proceed to go goal by goal. Rome wasn‚Äôt built in a single day! If you are learning any new skill it‚Äôs always helpful to visualize why you are learning this skill and what you want to use it for? A sense of purpose can make the learning process more enjoyable and satisfying in the end.\nNow I am aspiring to be a researcher, so naturally, that made me interested in using Notion for what would be my plausible PhD journey in the future. For this reason, I have made a couple of templates tailored to ease up my workflow when I start my PhD. At the time of writing this article, I am using Notion as a literature tracker, research diary (I will start using it once I get a PhD position), task manager, movie and book list tracker and as a simple finance tracker. If any of these templates interest you, then you are lucky as I have made all of them free for use! Furthermore, I have also written tutorials for some of these templates which you can read and get a better idea of how to use them."
  },
  {
    "objectID": "posts/notion_intro/index.html#tutorials-on-the-templates",
    "href": "posts/notion_intro/index.html#tutorials-on-the-templates",
    "title": "My Notion story",
    "section": "6 Tutorials on the templates",
    "text": "6 Tutorials on the templates\n\nUsing Notion as a literature tracker.\nUsing Notion to keep a research diary.\nUsing Notion as a task management software.\nUsing Notion as a reading list tracker and a movie list tracker.\nUsing Notion as a simple finance ledger. (I am still in the process of developing it, you can download the current version from the link)\n\nIf you want all these templates at once, then I have made a master template which has all of them together. You can get it here.\nI hope you find the templates useful and I wish you all the best in learning Notion. You can share your feedback and thoughts about the templates in the comment section below."
  },
  {
    "objectID": "posts/notion_intro/index.html#useful-references",
    "href": "posts/notion_intro/index.html#useful-references",
    "title": "My Notion story",
    "section": "Useful References",
    "text": "Useful References\n\n\nBeginners introduction to Notion. Source\nBasics of Notion. Source\nQuick tutorial on setting up Notion. Source\nNotion official website has detailed guides for very single feature in Notion. Source"
  },
  {
    "objectID": "posts/notion_literature/index.html",
    "href": "posts/notion_literature/index.html",
    "title": "Using Notion and Zotero to build a literature tracker",
    "section": "",
    "text": "An update (04-Oct-2022)\n\n\n\nAs Notion has been updated, they have moved the plugin integration to a new menu option called ‚ÄòAdd connections‚Äô. Therefore you won‚Äôt find the integration from the ‚ÄòShare‚Äô menu as before. So what does this mean to you?\n\nIf you are a returning user to this article, then please see the new ‚ÄòNotero in action‚Äô video which is given in step 6 under Configure Notero plugin section.\nIf you are a new user, reading this article for the first time, then don‚Äôt worry about anything and please follow the steps given below."
  },
  {
    "objectID": "posts/notion_literature/index.html#tldr",
    "href": "posts/notion_literature/index.html#tldr",
    "title": "Using Notion and Zotero to build a literature tracker",
    "section": "1 TL;DR",
    "text": "1 TL;DR\n\nUse Notion to build a literature database that can integrate with directories in Zotero with the help of the Notero plugin\nInstall Notion, Zotero and the Notero plugin\nDuplicate the literature template to your Notion database"
  },
  {
    "objectID": "posts/notion_literature/index.html#literature-tracker-using-notion-and-zotero",
    "href": "posts/notion_literature/index.html#literature-tracker-using-notion-and-zotero",
    "title": "Using Notion and Zotero to build a literature tracker",
    "section": "2 Literature tracker using Notion and Zotero",
    "text": "2 Literature tracker using Notion and Zotero\nThis is a follow-up article from my earlier post on My Notion story. If you don‚Äôt know what Notion is or how to install it, please refer to my earlier article for the background information.\nIn this article, we will aim to build a literature tracker that looks similar to the one below. It will be automated via the Notero plugin integrating Zotero with Notion.\n\n\n\n\nMy literature tracker template in Notion\n\n\n\n\n2.1 Install Zotero\n\nThe first thing to do is to download and install Zotero, a free-to-use reference manager that you can use to categorize and manage your research article collection. You should also create an account in Zotero and log in using that account in your Zotero app.\n\n\n\n2.2 Installing Notero plugin\nNow we are going to install a plugin for Zotero, called Notero by David Vanoni, which acts as a link between Zotero and Notion. This is what syncs your Zotero library to your Notion database.\n\nGo to the Github page of Notero and download the notero-0.3.5.xpi file under the assets section. You can right-click on the file and save it by clicking on ‚Äòsave link as‚Äô\n\n\n\n\n\n\n\nVideo: Downloading the plugin\n\n\n\nVideo\n\n\n\nGo to the Zotero app and click on ‚ÄòTools‚Äô and then click ‚ÄòAdd-ons‚Äô. Then in the new window, click on the Settings button and then click on ‚ÄòInstall add-on from file‚Äô. Then browse to where you downloaded your notero-0.3.5.xpi file (or any newer file than the given version) and install it.\n\n\n\n\n\n\n\nVideo: Installing the plugin\n\n\n\nVideo\n\n\n\n\n2.3 Configure Notero plugin\n\nAfter installing Notero, you can find a new option called ‚ÄòNotero Preferences‚Äô under the ‚ÄòTools‚Äô section. To have the plugin work, you need to provide an ‚Äòintegration token‚Äô and a ‚Äòdatabase ID‚Äô.\nTo get the integration token, go to Notion integrations and click on ‚ÄòNew integration‚Äô. Give a suitable name (something like ‚ÄòNotero integration‚Äô for the ease of finding it) and then select your workspace in Notion. You can keep the rest of the options in their default setting and then click save. You will then get your integration token.\n\n\n\n\n\n\n\nVideo: Getting the integration token\n\n\n\nVideo\n\n\n\nGo back to your Notion app and then select the database which you want to integrate with the Notero plugin. If you are creating a database from scratch then make sure that the table that you are creating has the following named columns.\n\n\n\n\nImage from Notero Github page\n\n\nIf you want to use my template then click here and click on Duplicate. It contains most of the relevant columns and will save you time from creating it from scratch.\n\n\n\nDuplicating a template in Notion\n\n\n\nOnce you have the database that you want to integrate with Notero, click on the ‚ÄòShare‚Äô button and click on the ‚Äòcopy link‚Äô. Paste the link in a text editor and copy the first 36 characters after your workspace name. This is your ‚Äòdatabase ID‚Äô.\n\n\n\n\n\n\n\nThe format of the copied link would in this form\n\n\n\nhttps://www.notion.so/{workspace_name}/{database_id}?v={view_id}\nCopy the {database_id} part\n\n\n\n\n\n\n\n\nVideo: Getting the database ID\n\n\n\nVideo\n\n\n\nThen finally paste your ‚Äòintegration token‚Äô and the ‚Äòdatabase ID‚Äô into the ‚ÄòNotero Preferences‚Äô window under the ‚ÄòTools‚Äô section in Zotero. Then select the directory in Zotero that you want to integrate with your Notion database. Now you have successfully configured the Notero plugin.\n\n\n\n\nNotero Preferences window\n\n\n\nNow go to the associated database and click on the ‚Äòthree dots‚Äô which will bring down a menu. In the menu, click on ‚ÄòAdd connections‚Äô and scroll down to find your Notero integration. Any research article that is present in the directory associated with the Notero plugin will now automatically be synced to your associated Notion database. Notero plugin allows one-time sync between Zotero to Notion but not the other way around. So any changes that you do in the Notion database won‚Äôt be reflected in your Zotero database. Please click on the green bar given below to see the video showcasing the steps.\n\n\n\n\n\n\n\nVideo: Notero in action\n\n\n\nVideo\n\n\nThere is a reason why I said it‚Äôs a one-time sync, any modifications that you do for already existing files won‚Äôt be reflected in Notion. But this can be easily fixed by assigning the existing papers to a tag, you can also delete it if you dont want it to appear in the database.\n\n\n\n\n\n\nVideo: Updating existing files with a new tag\n\n\n\nVideo\n\n\nThere are some limitations on what the plugin is capable of doing, but it‚Äôs still better than nothing. You can learn more about the plugin from its Github repo page.\nSo there you have it, your very own literature tracker made using Notion and integrated with Zotero using the Notero plugin. I am so proud of you üëç\nThank you David Vanoni and the developers who contributed to developing the Notero plugin. You can share your feedback and thoughts about the templates in the comment section below."
  },
  {
    "objectID": "posts/notion_movie_books/index.html",
    "href": "posts/notion_movie_books/index.html",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "",
    "text": "Use Notion to track your movie and book lists\nDuplicate the movie list tracker and the book list tracker templates to your Notion database and use them."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#tldr",
    "href": "posts/notion_movie_books/index.html#tldr",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "",
    "text": "Use Notion to track your movie and book lists\nDuplicate the movie list tracker and the book list tracker templates to your Notion database and use them."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#using-notion-to-keep-a-movie-list-tracker",
    "href": "posts/notion_movie_books/index.html#using-notion-to-keep-a-movie-list-tracker",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "2 Using notion to keep a movie list tracker",
    "text": "2 Using notion to keep a movie list tracker\nThis was a fun little project where I made two templates that helps in maintaining reading and a watch list tracker for books and movies. I heard from my friends who are doing PhD that they spent a hefty amount of time binge-watching movies and series and most of them also read a lot of new books. So I thought why not make a template that will help them manage this. The templates are very simple and visually pleasing.\nThis might be my shortest article ever on this blog but I thought this post can act as a source to receive feedback and comments about the templates. The links for the templates are given below.\n\nMovie list tracker\nBook list tracker"
  },
  {
    "objectID": "posts/notion_movie_books/index.html#movie-list-tracker",
    "href": "posts/notion_movie_books/index.html#movie-list-tracker",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "3 Movie list tracker",
    "text": "3 Movie list tracker\n\n\n\n\nMovie list tracker\n\n\n\nYou can categorise the list to their ‚Äòwatch status‚Äô and also indicate if it‚Äôs a movie or a series. Furthermore, if watch a lot of series, then you can also add in the extra columns indicating the season and so on. In the poster column, I have embedded an online image link showcasing the movie or series poster. This is done so that the media collection view (gallery view) can use these images and show visually pleasing cards. You can get the poster image links for most of the movies and series from here. The progress section shows the data in a ‚Äòkanban‚Äô view to quickly visualize the watch list according to their status."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#book-list-tracker",
    "href": "posts/notion_movie_books/index.html#book-list-tracker",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "4 Book list tracker",
    "text": "4 Book list tracker\n\n\n\n\nMovie list tracker\n\n\n\nThe book list tracker is almost identical to the movie list one. To get the book cover image links for the database, you can visit here. The way you add entries is also pretty self intuitive so I urge you to explore it yourself."
  },
  {
    "objectID": "posts/notion_movie_books/index.html#conclusion",
    "href": "posts/notion_movie_books/index.html#conclusion",
    "title": "Using Notion to keep a movie and a book list tracker",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nI hope you find these templates useful. You can give your feedback and thoughts about the template in the comment section below this article."
  },
  {
    "objectID": "posts/notion_research_diary/index.html",
    "href": "posts/notion_research_diary/index.html",
    "title": "Using Notion to keep a research diary",
    "section": "",
    "text": "Use Notion to write and maintain a research diary\nDuplicate the diary template and use it in Notion"
  },
  {
    "objectID": "posts/notion_research_diary/index.html#tldr",
    "href": "posts/notion_research_diary/index.html#tldr",
    "title": "Using Notion to keep a research diary",
    "section": "",
    "text": "Use Notion to write and maintain a research diary\nDuplicate the diary template and use it in Notion"
  },
  {
    "objectID": "posts/notion_research_diary/index.html#using-notion-to-keep-a-research-diary",
    "href": "posts/notion_research_diary/index.html#using-notion-to-keep-a-research-diary",
    "title": "Using Notion to keep a research diary",
    "section": "2 Using notion to keep a research diary",
    "text": "2 Using notion to keep a research diary\nIn this article, we will see how we can use Notion to keep a research diary. If you are a researcher or a PhD student, then a research diary would be a very important piece of intellectual work that will be staying with you for quite a few years. So it must be well written, well-formatted and efficiently organised. With the power of Notion, I will show you a template that I made that can be used in Notion and can act as an electronic research diary.\nYou can get the template by clicking here. Duplicate it into your Notion workspace.\n\n\n\n\nResearch diary database\n\n\n\nThe picture above shows the research diary database. Each row value created in the table will be each day‚Äôs entry. If you also have a literature tracker database in Notion workspace, then you can couple it with your diary entry and it will showcase any papers that you have read one that day.\nSo let us try writing an entry. First, make a blank row entry and then hover over the newly created empty row and click ‚ÄòOPEN‚Äô. This will open a new window. In that window click on ‚ÄòDiary entry template‚Äô. This will bring you a sample template. You can fill in relevant info or add in your subsection and modify the entry.\n\n\n\n\nResearch diary template\n\n\n\n\nThere are mainly five sections in the template. In the first section, you can outline what experiments or analyses or any other related work you did on that day. You can also attach any relevant pictures if it helps.\nIn the second section, you can outline the challenges you came across while implementing your work. It can be problems you encountered in your experiment or a bug in your code while doing data analysis etc.\nThe third section outlines your thoughts and ideas, it can either be possible solutions that you want to try out with regards to the challenges you faced or a new research question that you had while reading a paper.\nIn the fourth section of the template, I have attached my literature tracker database to the diary entry page. Now if I happened to read any research articles on that day, then by filtering the database to the diary entry date, I can have all the papers that I read on that date show up. This can be pretty convenient to track your reading goals. You can extend this idea and showcase any experiments that you have done on that day if you have an experiment database in your Notion database.\nThe fifth and last section of the template can have any other relevant work information which does not fall into the earlier section. It can be purchase reminders, suggestions by your supervisors or colleagues, deadlines for meetings or presentations etc.\n\nThis is a simple research diary template that I have created and I hope you find it useful. Suggestions and feedback to improve the diary template are most welcome. Please comment your thoughts below the article. Thanks!\n\n\n\n\n\n\nVideo: Making a diary entry (click here)\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "posts/notion_research_diary/index.html#useful-references",
    "href": "posts/notion_research_diary/index.html#useful-references",
    "title": "Using Notion to keep a research diary",
    "section": "Useful References",
    "text": "Useful References\n\n\nThe research diary image in the thumbnail for this post belongs to Dr.¬†Julia Everitt (@juilaeverittdr). You can read more about research diary here."
  },
  {
    "objectID": "posts/notion_task_manager/index.html",
    "href": "posts/notion_task_manager/index.html",
    "title": "Using Notion as your task manager",
    "section": "",
    "text": "Use Notion to manage your daily tasks\nFour templates to download; To-do-list, Completed task list, Task manager and Task dashboard\nThose who want to download all the templates in one go, click here\nDuplicate and use them in Notion"
  },
  {
    "objectID": "posts/notion_task_manager/index.html#tldr",
    "href": "posts/notion_task_manager/index.html#tldr",
    "title": "Using Notion as your task manager",
    "section": "",
    "text": "Use Notion to manage your daily tasks\nFour templates to download; To-do-list, Completed task list, Task manager and Task dashboard\nThose who want to download all the templates in one go, click here\nDuplicate and use them in Notion"
  },
  {
    "objectID": "posts/notion_task_manager/index.html#using-notion-as-a-task-management-dashboard",
    "href": "posts/notion_task_manager/index.html#using-notion-as-a-task-management-dashboard",
    "title": "Using Notion as your task manager",
    "section": "2 Using Notion as a task management dashboard",
    "text": "2 Using Notion as a task management dashboard\nWith the ability to make databases, Notion becomes an excellent tool to manage task lists. In this article, I am going to give you a quick tour of how you can use Notion as a task management dashboard with the templates I made.\nThere are four templates to add to your Notion database and you need all four of them to function properly. If you don‚Äôt want to individually download each of them, then I have a single-page master template which features all the templates I made in one place. You can find it here.\n\n\n\nThe templates of interest for this article\n\n\n\nTo-do-list: Acts as the master database where all tasks are stored. By default only shows incomplete tasks.\nCompleted task list: Same template as above, but only tasks that are completed are shown. Act as an archive for completed tasks.\nTask manager: Task manager interface where each task is arranged and displayed categorically. By default only shows incomplete tasks.\nTask dashboard: A dashboard which informs you which tasks are due and how long are they due to completion."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#to-do-list",
    "href": "posts/notion_task_manager/index.html#to-do-list",
    "title": "Using Notion as your task manager",
    "section": "3 To-do-list",
    "text": "3 To-do-list\nFirst, let us see the To-do-list. Specifically, we will be seeing the ‚ÄòMaster table‚Äô view which lists out all the tasks which are yet to be completed.\n\n\n\n\nTo-do-list\n\n\n\nLike any basic to-do list, this list has task names and due date columns, along with many other columns. I have also tagged the tasks into their respective category and their priority. The ‚ÄòDate added‚Äô and ‚ÄòDue status is automatically calculated once you input a row value in the table. The ‚ÄôDue status‚Äô column is dependent on a formula which takes in the ‚ÄòDue date‚Äô value. The possible values for the ‚ÄòDue status‚Äô column are; task finished, overdue, due today, due tomorrow, someday and later. Here ‚Äòsomeday‚Äô label means that the task has no due date mentioned and the ‚Äòlater‚Äô label means that the task has at least 7 days before the due. The ‚Äòtask finished‚Äô is only given to tasks which have the ‚ÄòDone‚Äô column check box checked. The rest of the labels are what it says.\nYou can change every single aspect of this table to suit your needs. What might be tricky to change is the ‚ÄòDue status‚Äô column as it is governed by a formula which may look complex but is very easy.\n\n\n\n\n\n\nDue status formula\n\n\n\nif(prop(\"Done\") == false, \nif(formatDate(prop(\"Due date\"),\"L\") == formatDate(now(), \"L\"), \"Due today\",\nif(prop(\"Due date\") &lt; now(), \"Overdue\",\nif(prop(\"Due date\") &lt;= dateAdd(now(), 1, \"days\"), \"Due tomorrow\",\nif(empty(prop(\"Due date\")), \"Someday\", \"Later\")))), \"Task finished\")\n\n\nThe formula is a series of if statements. The syntax for the if statement in Notion is as follows;\nboolean ? value : value\nif(boolean, value, value)\nFrom the syntax, you can see that the if statement is dependent on a Boolean value.\nSo let us try to understand what this formula is trying to calculate, by taking each section separately.\n\n\n\n\n\n\nif(prop(‚ÄúDone‚Äù) == false\n\n\n\nThis means that if the ‚ÄòDone‚Äô column is unchecked then run the rest of the statements, if not, then it returns the label ‚ÄòTask finished‚Äô which you can see in the last part of the formula. The checkboxes in Notion follow Boolean values. Checked means true and unchecked means false.\n\n\n\n\n\n\n\n\nif(formatDate(prop(‚ÄúDue date‚Äù),‚ÄúL‚Äù) == formatDate(now(), ‚ÄúL‚Äù), ‚ÄúDue today‚Äù\n\n\n\nThis is yet another ‚Äòif‚Äô statement but this time we are reformatting the date. The reason why we are reformatting the date is that, in Notion, any date is followed by time, so 12-12-2022 is Notion is denoted as 12-12-2022 12:00 AM. So there can be some issues where ‚ÄòDue today‚Äô won‚Äôt be properly shown. So reformatting the date to ‚Äòday‚Äô units is easier to work with. The now() function returns the current date. Therefore this formula simply returns the label ‚ÄòDue today‚Äô if the due date matches the current date.\n\n\n\n\n\n\n\n\nif(prop(‚ÄúDue date‚Äù) &lt; now(), ‚ÄúOverdue‚Äù\n\n\n\nThis formula returns an ‚ÄòOverdue‚Äô label if the due date is past the current date.\n\n\n\n\n\n\n\n\nif(prop(‚ÄúDue date‚Äù) &lt;= dateAdd(now(), 1, ‚Äúdays‚Äù), ‚ÄúDue tomorrow‚Äù,\n\n\n\nThe dateAdd() function takes in the first value, which is the current date returned from the now() function, and then adds 1 unit of ‚Äúdays‚Äù to it. Therefore the formula boils down to checking if the due date is less than or equal to tomorrow and returns the ‚ÄòDue tomorrow‚Äô label if it does. We give less than or equal to sign because we want the counter-statement to be strictly greater than so that, only then does the rest of the if statement runs. Also, the less than condition can never be fulfilled here as the first statement checks if the due date matches the current day.\n\n\n\n\n\n\n\n\nif(empty(prop(‚ÄúDue date‚Äù)), ‚ÄúSomeday‚Äù, ‚ÄúLater‚Äù\n\n\n\nNow sometimes we don‚Äôt want to give the due date or we forgot to give them to a task. This is notified by the ‚ÄòSomeday‚Äô label. Now the ‚ÄòLater‚Äô label is given when the ‚ÄòDue tomorrow‚Äô condition fails, which means that the due date is greater than tomorrow.\n\n\nNow the reason why I went to explain this formula so elaborately is for you to appreciate the formula feature in Notion. Using clever little formulas like this you can efficiently automate many boring tasks. If it wasn‚Äôt for this formula, we would have to manually check between the current date to the due date. So something to keep in mind while designing Notion databases.\nNow Notion also features some powerful ways of visualizing tabular data via ‚Äòviews‚Äô. You can see that there is a ‚ÄòGroup table‚Äô, ‚ÄòPriority table‚Äô and a ‚ÄòStatus table‚Äô views in the database. These are nothing but the ‚ÄòMaster table‚Äô data grouped into different categories for easy visualization.\n\n\n\n\n\n\nVideo: Different views of the to-do list (click here)\n\n\n\n\n\nVideo\n\n\n\nOkay, now we have a fair idea of what is there in this database and what each of the columns does. Now the funny part is that we won‚Äôt be adding any tasks to this table using this page. Instead, we will be exclusively using the ‚ÄòTask manager‚Äô for it."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#task-manager",
    "href": "posts/notion_task_manager/index.html#task-manager",
    "title": "Using Notion as your task manager",
    "section": "4 Task manager",
    "text": "4 Task manager\n\n\n\n\n\n\nVideo: Task manager\n\n\n\n\n\nVideo\n\n\n\nThe task manager page is your go-to page to list out incomplete tasks. By default, the page has categorized all tasks into their respective category as subsections. The inbox section is where you add your task to the to-do list and as soon as you add the associated category, it vanishes from the inbox and goes to its category subsection. Thus you don‚Äôt have to worry about post categorizing into the subsection parts after you have added a task. And once you check the ‚ÄòDone‚Äô column for a task, it is then archived on the Completed task list page to preserve the history of completed tasks.\n\n\n\n\n\n\nVideo: Adding a task in the task manager\n\n\n\n\n\nVideo"
  },
  {
    "objectID": "posts/notion_task_manager/index.html#task-dashboard",
    "href": "posts/notion_task_manager/index.html#task-dashboard",
    "title": "Using Notion as your task manager",
    "section": "5 Task dashboard",
    "text": "5 Task dashboard\n\n\n\n\nTask dashboard\n\n\n\nTreat the task dashboard page as your command centre for all the tasks. Remember all those ‚Äòdue status‚Äô labels that we had in the to-do list database. With the help of Notion‚Äôs grouping feature for databases, we can easily visualize all the tasks according to their deadlines. In the ‚Äòtask list subsection,‚Äô I have also added table views showing tasks associated with a particular priority. The purpose of this page is to easily get a glance at tasks that close the due date."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#completed-tasks",
    "href": "posts/notion_task_manager/index.html#completed-tasks",
    "title": "Using Notion as your task manager",
    "section": "6 Completed tasks",
    "text": "6 Completed tasks\nThe completed tasks page acts as an archive showcasing all the completed tasks. As someone used to say to me, it brings great joy to tick off all those tasks from the to-do list after spending your time and energy completing them. The primary function of this page is to preserve and record task completion history. The database is identical to that of the earlier shown to-do-list database, as I have filtered it to only show row values where the ‚ÄòDone‚Äô column is checked."
  },
  {
    "objectID": "posts/notion_task_manager/index.html#conclusion",
    "href": "posts/notion_task_manager/index.html#conclusion",
    "title": "Using Notion as your task manager",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nI hope you have fun using this template. I enjoyed making this template and I aimed to make it simple and effective to use. To summarise, the to-do list is your main database, actually your only database. Everything else is just a modified version of this database where the info is shown in a different form using filtering and grouping features. After duplicating this template, you can tweak each of the parameters to suit your liking. You only need to change the parameters in the to-do-list database as the changes in it will be reflected in the rest of the pages. The only other thing that you will be needing to modify after you have changed the key parameters in the database are the headings, which are pretty straightforward to change. With that, I hope you find this template useful. Any feedback and suggestions are most welcome. You can comment them below the article."
  },
  {
    "objectID": "posts/obsidian_zotero/index.html",
    "href": "posts/obsidian_zotero/index.html",
    "title": "Using Obsidian as a note-taking software for academic purposes",
    "section": "",
    "text": "It‚Äôs been a while since I have been trying to find various productivity software to prepare myself for my future Ph.D.¬†journey. One of those tasks which I wanted to automate was in organizing my scientific literature collection. The answer to organizing and annotating the literature was met by two software; Zotero and Notion. Zotero is a free-to-use reference manager that can organize your literature collection with ease. Notion, on the other hand, is a productivity software that can be used as a workspace manager, task manager, and many other things. I have written a guide on how one can use Zotero and Notion to build a literature tracker.\nNow that the organization part is taken care of, I was in search of software that can help me take summary notes on the literature that I am studying. Now I can write notes and embed them into the .pdf file using Zotero itself or use Notion to create notes using the literature database which is synced with the Zotero libraries. Both these methods work, but I wanted something versatile. And that‚Äôs what brought me to Obsidian, an open-source, free-to-use, note-taking software that uses the Markdown language at its core. In this blog post, I will show you how I use Obsidian to take literature notes."
  },
  {
    "objectID": "posts/obsidian_zotero/index.html#getting-started",
    "href": "posts/obsidian_zotero/index.html#getting-started",
    "title": "Using Obsidian as a note-taking software for academic purposes",
    "section": "",
    "text": "It‚Äôs been a while since I have been trying to find various productivity software to prepare myself for my future Ph.D.¬†journey. One of those tasks which I wanted to automate was in organizing my scientific literature collection. The answer to organizing and annotating the literature was met by two software; Zotero and Notion. Zotero is a free-to-use reference manager that can organize your literature collection with ease. Notion, on the other hand, is a productivity software that can be used as a workspace manager, task manager, and many other things. I have written a guide on how one can use Zotero and Notion to build a literature tracker.\nNow that the organization part is taken care of, I was in search of software that can help me take summary notes on the literature that I am studying. Now I can write notes and embed them into the .pdf file using Zotero itself or use Notion to create notes using the literature database which is synced with the Zotero libraries. Both these methods work, but I wanted something versatile. And that‚Äôs what brought me to Obsidian, an open-source, free-to-use, note-taking software that uses the Markdown language at its core. In this blog post, I will show you how I use Obsidian to take literature notes."
  },
  {
    "objectID": "posts/obsidian_zotero/index.html#using-obsidian-with-zotero",
    "href": "posts/obsidian_zotero/index.html#using-obsidian-with-zotero",
    "title": "Using Obsidian as a note-taking software for academic purposes",
    "section": "2 Using Obsidian with Zotero",
    "text": "2 Using Obsidian with Zotero\nBefore we begin, the picture given below shows my Obsidian interface. The interface is broken down into different panels, where each panel shows specific info.\n\n\n\n\n\nA glimpse of my Obsidian interface\n\n\n\n\n\nDifferent sections of the interface\n\n\n\n\nNow the primary goal for me in using Obsidian is to write summary notes of the papers that I am reading, but at the same time, I can also so use it as a research diary and as a general note-taking software to write my thoughts and ideas. One good thing about Obsidian is that you don‚Äôt need an active internet connection to use it and also the markdown files that you will be making will be saved locally. So let us get started!\n\n2.1 Setting up Obsidian\nWe will first see how to create the interface that we saw earlier. For that let us first install Obsidian.\n\nInstalling Obsidian is very easy. Just visit their official website and download the version suited for your operating system. I assume that you already have Zotero on your computer. If not, then please visit here and download the appropriate version for your system. Now that we have both Obsidian and Zotero, let us get started.\n\n\n\n\nInitial message after opening Obsidian for the first time\n\n\nWhen you first open Obsidian, it will ask you to create a vault as shown in the ?@fig-start below. The vault is a folder location that will contain all the markdown files you will be creating. Now instead of doing everything from scratch, for convenience, I will be sharing my vault template folder so that you can get started quickly. The folder will also contain the necessary plugins and configurations to jump-start your Obsidian journey.\n\nDownload the folder as a zip file from here.\nExtract the zip file and open the folder in Obsidian using ‚ÄòOpen folder as vault‚Äô as shown in ?@fig-start. If done properly, then you will get the following interface as shown in the image below.\n\n\n\n\nVault template interface\n\n\nI use the following plugins which are not owned or made by me. The credits go to the plugin developers. You can learn more about the plugins by following their GitHub repo links.\n\nObsidian Admonition\nAutocomplete Obsidian Plugin\nBetter Word Count\nobsidian-calendar-plugin\nobsidian-citation-plugin\ncMenu Plugin\nObsidian Dataview\nObsidian Icon Folder\nObsidian Pandoc Reference List\n\n\n\n2.2 Setting up Zotero\nNow before we proceed, let us configure Zotero. We will use Zotero to create a BibTeX file from a library. Now, this .bib file will contain all the metadata of the literature present in that library. But we won‚Äôt be making a normal .bib file, instead, we will use a plugin called ‚ÄòBetter BibTeX for Zotero‚Äô to make the .bib file, which features an auto-update feature. What it means is that, as you add more literature to your Zotero library, the .bib file will automatically update. Pretty cool right?\n\nDownload the ‚ÄòBetter BibTeX for Zotero‚Äô plugin from here. Hover your cursor to zotero-better-bibtex-6.7.36.xpi, then right-click and select ‚ÄòSave link as‚Äô to download the addon.\nThen open up Zotero and navigate to Tools -&gt; Add-ons. Then go to the setting options by clicking on the gear icon and then select ‚ÄòInstall add-on from file‚Äô. Browse to the location where your file was downloaded and then install the addon.\nLet us now configure the plugin. Select Edit -&gt; Preferences -&gt; Better BibTex. In the ‚ÄòCitation key format‚Äô section replace the existing text with the following: authEtal2+year.\n\n\n\n\nMy zotero BibTex citation key format\n\n\nThis will change the citation key to the ‚Äòauthor name + year‚Äô format which is much more useful and easy to understand than the default citation key format. You can visit here to learn more about how to customize your citation key format.\nFinally, select the folder which you want to make a .bib file for. If you want to access all the literature files from the Zotero library, then export the .bib file from ‚ÄòMy library‚Äô.\n\nRight-click on ‚ÄòMy library‚Äô and then click on ‚ÄòExport library‚Äô. A small window will pop up, select ‚ÄòBetter BibLaTeX‚Äô from the format option. Check the ‚ÄòKeep updated‚Äô option, this will auto-update the .bib file when you add new files to the Zotero library. You can also check the ‚ÄòExport notes‚Äô option if you want to export the highlights and annotations that you have made on the .pdf files using Zotero.\nSave your .bib file in a location of your choice. Then copy the full path (file location) to the .bib file. We will be needing this shortly.\n\n\n\nVideo\nExporting .bib file using Zotero\n\n\n\n\n2.3 Configuring the plugins\nAs you are using my template, you don‚Äôt have to individually download the plugins which I am using. But you do need to configure a few settings (only two configurations). Let us configure the ‚ÄòCitations‚Äô plugin first.\n\nGo to the setting page in Obsidian. Then under ‚ÄòCommunity plugins‚Äô select the ‚ÄòCitations‚Äô plugin. Then in the ‚ÄòCitation database path‚Äô paste your .bib file path.\n\nAs an example my .bib file location is the following; /home/jewel/Downloads/Example.bib\n\n\n\nCitations plugin option\n\n\nThe syntax of the citations template that I am using is the following;\n\n\n\n\n\n\nCitation template syntax (click here to see the code)\n\n\n\n\n\n---\ntitle: \"{{title}}\"\nauthors: {{authorString}}\nyear: {{year}}\ncitekey: {{citekey}}\n---\n\n&gt;[!title]\n{{title}}\n\n&gt;[!year]\n{{year}}\n\n&gt;[!author]\n{{authorString}}\n\n------------------------------------\n\nURL: {{URL}}\n\nZotero link:  [@{{citekey}}]({{zoteroSelectURI}})\n\ntags::\n\n------------------------------------\n\n### Research question\n1. \n\n### Findings\n1. \n\n### Discussion\n1. \n\n### Methodology\n1. \n\n### Remarks\n1. \n\n\n\n\nYou can visit the official GitHub repo of the Citations plugin or read the instructions shown in the plugin window in Obsidian to learn more about how to customize the template.\nYou also need to configure the ‚ÄòPandoc Reference List‚Äô plugin. To use this plugin you would need the pandoc package installed on your computer. Most operating systems have it pre-installed and if you don‚Äôt have it then please visit here for info on how to install the package. For Linux users, you can install ‚Äòpandoc‚Äô using your package manager by searching.\n\nHead on to the ‚ÄòPandoc Reference List‚Äô plugin under the ‚ÄòCommunity plugins‚Äô section. In ‚ÄòPath to Bibliography File‚Äô section, paste the same path location as before (i.e.¬†the location of your .bib file).\n\n\n\n\nPandoc plugin option\n\n\n\n\n2.4 Using the citation plugin\nTo access the plugin in the note window, press Ctrl+P to open up the commands menu. Type ‚ÄòCitations‚Äô and view the different options. For quick access, you can press Ctrl+Shift+E instead and browse through your library collection and search for the literature that you want to reference.\n\n\n\nCitation plugin hotkeys\n\n\nBy selecting a literature entry, a markdown file (.md file) will be automatically created in your ‚ÄòLiterature_collection‚Äô folder. You can open these markdown files and then make the necessary edits. I use them to write the summary and my understanding of that paper in the .md file.\n\n\nVideo\nCitations plugin in action\n\n\nI have also made a literature table using the ‚ÄòDataview‚Äô plugin. You can configure the source code of the table to suit your needs.\nThe source code is as follows;\n\n\n\n\n\n\nLiterature table syntax (click here to see the code)\n\n\n\n\n\n```dataview\ntable title, authors, year, tags\nfrom \"Literature_collection\"\nsort year desc\n```\n\n\n\nThe syntax picks up .yaml headers like title, authors, year and tags, all of which have been pre configured in the literature .md files using the Citations plugin template that we saw before. You can learn more about how to configure the table here.\n\n\n\nLiterature table"
  },
  {
    "objectID": "posts/obsidian_zotero/index.html#conlusion",
    "href": "posts/obsidian_zotero/index.html#conlusion",
    "title": "Using Obsidian as a note-taking software for academic purposes",
    "section": "3 Conlusion",
    "text": "3 Conlusion\nI have just started out using Obsidian and so far I have had an amazing experience. There are a lot of community plugins available and I get easily overwhelmed by them. You can browse the plugins collection and see what fits your interests. I hope you find this guide helpful and let me know in the comments about your experience with Obsidian."
  },
  {
    "objectID": "posts/obsidian_zotero/index.html#plugin-credits",
    "href": "posts/obsidian_zotero/index.html#plugin-credits",
    "title": "Using Obsidian as a note-taking software for academic purposes",
    "section": "4 Plugin credits",
    "text": "4 Plugin credits\nI use the following plugins in my workflow. You can learn more about these plugins in the links given below.\n\nObsidian Admonition\nAutocomplete Obsidian Plugin\nBetter Word Count\nobsidian-calendar-plugin\nobsidian-citation-plugin\ncMenu Plugin\nObsidian Dataview\nObsidian Icon Folder\nObsidian Pandoc Reference List"
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html",
    "href": "posts/research_article_biggest_bacterium/index_post.html",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "",
    "text": "Summary of the study in (1)\n\n\nResearch article that is summarised in this post: Volland, J.-M., Gonzalez-Rizzo, S., Gros, O., Tyml, T., Ivanova, N., Schulz, F., Goudeau, D., Elisabeth, N. H., Nath, N., Udwary, D., Malmstrom, R. R., Guidi-Rontani, C., Bolte-Kluge, S., Davies, K. M., Jean, M. R., Mansot, J.-L., Mouncey, N. J., Angert, E. R., Woyke, T., & Date, S. V. (2022). A centimeter-long bacterium with DNA contained in metabolically active, membrane-bound organelles. Science, 376(6600), 1453‚Äì1458. https://doi.org/10.1126/science.abb3634."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#tldr",
    "href": "posts/research_article_biggest_bacterium/index_post.html#tldr",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "",
    "text": "Summary of the study in (1)\n\n\nResearch article that is summarised in this post: Volland, J.-M., Gonzalez-Rizzo, S., Gros, O., Tyml, T., Ivanova, N., Schulz, F., Goudeau, D., Elisabeth, N. H., Nath, N., Udwary, D., Malmstrom, R. R., Guidi-Rontani, C., Bolte-Kluge, S., Davies, K. M., Jean, M. R., Mansot, J.-L., Mouncey, N. J., Angert, E. R., Woyke, T., & Date, S. V. (2022). A centimeter-long bacterium with DNA contained in metabolically active, membrane-bound organelles. Science, 376(6600), 1453‚Äì1458. https://doi.org/10.1126/science.abb3634."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#introduction",
    "href": "posts/research_article_biggest_bacterium/index_post.html#introduction",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "2 Introduction",
    "text": "2 Introduction\nWhen you think of bacteria, the first thing that might come to your mind is a microscope, given how tiny they are to see. Most of the bacteria and archaea are about ~ 2 \\mu m in size (1). The record for the smallest living organism is shared between the pathogenic bacteria, Mycoplasma pneumoniae and the Thermodiscus sp. belonging to Archae. Both can be as small as 0.2 \\mu m in size (2). The biggest bacteria was thought to be Thiomargarita namibiensis, which can be as big as 750 \\mu m in size (average size is 180 \\mu m) and can be seen with the naked eye (3). But the recent discovery of a new bacteria species has overtaken T. namibinesis to become the biggest bacteria ever. Moreover, it is about ~50 times bigger on average, compared to T. namibinesis and is seen in lengths of more than 9000 \\mu m, which is about 1cm! To put that in perspective, a housefly, which is a complex multi-cellular organism is on average only about 0.5 \\ to\\ 0.7cm long!\nMeet Thiomargarita magnifica, your newest species of bacteria, which as we know is the biggest bacteria to date. As reported by a group of researchers in a recent paper (1) in Science, there is more than just the size that makes this bacteria special. Thiomargarita magnifica is packed with so many complex features that it is blurring the lines between what we considered to be prokaryotes and eukaryotes."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#discovering-the-biggest-bacteria",
    "href": "posts/research_article_biggest_bacterium/index_post.html#discovering-the-biggest-bacteria",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "3 Discovering the biggest bacteria",
    "text": "3 Discovering the biggest bacteria\n\n\n\n\nFigure¬†1: Images (A) and (B) are taken from supplementary fig.¬†S1. in (1). (A) Shows Thiomargarita magnifica (white-filaments) attached to sunken leaves of Rhizophora mangle. (B) Buds forming on the apical pole the filament\n\n\n\nResearchers first discovered Thiomargarita magnifica attached to the sunken leaves of Rhizophora mangle (as seen in Figure¬†1) present in the shallow tropical marine mangroves from Guadeloupe, Lesser Antilles, a French overseas province. The bacteria appeared as a long-white filament and was first thought to be a fungus. It was only later through various techniques that the researchers concluded that this is not a fungus, but a new species of bacteria and rather a very long one that is. Since the bacteria is not yet culturable in laboratory conditions, under the nomenclature followed in microbiology, they are called Candidatus (Ca.) Thiomargarita magnifica."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#elucidating-the-biggest-bacteria",
    "href": "posts/research_article_biggest_bacterium/index_post.html#elucidating-the-biggest-bacteria",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "4 Elucidating the biggest bacteria",
    "text": "4 Elucidating the biggest bacteria\n\n4.1 The whole filament is a single bacterial cell\n\n\n\nFigure¬†2: Images (A), (B) and (C) are taken from supplementary fig.¬†S4. Image (D) is taken from Fig. 1. All images are from (1). In all the images the red outline represents the plasma membrane, arrowheads represent division septa and ‚ÄúV‚Äù in the image (D) represents a large central vacuole.(A) Cell membrane of Cyanobacterium Microcoleus vaginatus showing discontinuity throughout the filament because of the presence of division septa.(B) The membrane septum separating vacuolated cells of a Beggiatoa-like filament.(C) The membrane septum separating the large vacuolated cells of the Marithrix-like bacterial filaments.(D) Continuous cell membrane of with no division septa in (Ca.) T. magnifica.\n\n\n(Ca.) T. magnifica belongs to the family of large sulphur bacteria (LSB). LSBs are known to form long filaments of lengths up to 200 \\mu m (4‚Äì6). But these filaments are built up of individual LSB cells and therefore cannot be considered as a single living organism. Thus, the white filament was initially thought to be made of individual cells. To check this, researchers used dyes to visualize the entire plasma membrane of the filament. If it was made of individual cells, then the dye will show discontinuity in the plasma membrane structure indicating septa divisions. But to their surprise, they found no discontinuity and concluded that the whole filament is a single bacterial cell (as seen in Figure¬†2). Moreover, the filaments also had small buds forming from its apical pole and some of these filaments along with the buds were up to 2cm in size.\n\n\n4.2 Inside the biggest bacteria: Large central vacuole, sulphur granules and pepins\n\n\n\n\n\nFigure¬†3: Image (E) is taken from fig.¬†1. in (1). (E) Image shows a bud forming from the mother cell, where a Large central vacuole is present and is represented as ‚ÄúV‚Äù.\n\n\nInterestingly the whole filament was not filled with the cytoplasm rather, a large central vacuole was present in the middle (as seen in Figure¬†2 and Figure¬†3). This vacuole pushed the cellular cytoplasm to its periphery. The vacuole contributed to around 70% of the total cell volume. Therefore, although the cell is large, most of its volume is metabolically inactive. Apart from the vacuole, lucent vesicles were also found, which when further analyzed revealed to be filled with sulfur granules (see Figure¬†4). As Thiomargarita spp. are sulfur-oxidizing gammaproteobacteria (7), this was not a surprise.\nAdditionally, electron-rich membrane-bound compartments were found within the cytoplasm (see Figure¬†4), which was similar to structures previously reported in other LSBs. They were hypothesized to be compartments containing ribosomes and genetic material (8). To check if this holds, researchers used a stain to check where the genetic material was localized in (Ca.) T. magnifica. It was found that almost all of its DNA was concentrated inside these compartments. Further analysis also showed evidence of structures that were 10 to 20 nm in size, inside these compartments, similar to ribosomes. Researchers then used a technique to look for ribosomal RNAs and confirmed their presence inside these structures which indicated that those structures were ribosomes. Adding to this evidence, when translational activity was checked, they were consistently seen within most of the sites where ribosomal RNA was found.\n\n\n\nFigure¬†4: Image (G) is taken from fig.¬†1. in (1). (E) The arrowheads show two electron-rich membrane-bound compartments called pepins, the letter ‚ÄúS‚Äù represents sulphur granules and ‚ÄúV‚Äù represents a large central vacuole.\n\n\nThese membrane-bound structures containing DNA and ribosomes, which were not seen attached to the cell membrane are equivalent to the compartmentalization seen in eukaryotes, where DNA is present inside the nucleus and ribosomes are attached to the endoplasmic reticulum. Thus, this new membrane-bound organelle was named ‚Äòpepins‚Äô by the researchers, because of how they resemble pips or seeds in a watermelon. Overall, the cytoplasm of (Ca.) T. magnifica contained sulfur granules, pepins and various membrane structures forming a complex membrane network that spanned the whole cytoplasm.\n\n\n4.3 Localization of ATP synthase\nPeriplasm is a gel-like matrix present between the outer membrane and the inner membrane of a gram-negative bacteria\n\n\n\n\n\n\n\n(a) Image is taken from fig.¬†1. in: (9). Electron transport chain of E. coli\n\n\n\n\n\n\n\n\n\n(b) Image is taken from fig.¬†1. in: (10). Electron transport chain in a mitochondrial cell\n\n\n\n\nFigure¬†5: Electron transport chains of prokaryotes and eukaryotes\n\n\nAnother defining feature between eukaryotes and prokaryotes is the localization of ATP synthase. In eukaryotes, the ATP synthase is localized in the mitochondrial membrane where the electron transport chain (ETC) protein complexes are present (10). But for bacteria and archaea, mitochondria are absent and the ATP synthase is localized to the cell membrane along with the other protein complexes present in the ETC (9). Researchers checked within (Ca.) T. magnifica, where the ATP synthase is localized. Surprisingly ATP synthase was found in the membranes of pepins and the complex membrane structure, throughout the cytoplasm (see Figure¬†6). But ATP synthase was absent in the cellular membrane, this was similar to eukaryotes, where ATP synthase is only present in the mitochondria, which is present within the cytoplasm. These results together elevated pepins as not just a housing compartment for ribosomes and DNA, but also as a potential energy-producing organelle.\n\n\n\n\n\nFigure¬†6: Image (F) is taken from supplementary fig.¬†S20. in: (1). (E) The red colour indicates ATP synthase and blue colour dots highlight DNA which is used here as a proxy for where the pepins are situated. As you can see, the ATP synthase is expressed throughout the complex membrane network and the pepins present in the cytoplasm\n\n\nThe surprises do not stop there. Researchers further found that (Ca.) T. magnifica also showed extreme polyploidy, with around 740,000 genome copies for a 2 cm long cell. This is the highest number of genome copies ever reported for a single cell. The genome copies were very similar in their composition. A single genome copy was found to be 11.5 and 12.2 Mb in size, comparable to the popular Saccharomyces cerevisiae genome (12.1 Mb). (Ca.) T. magnifica genome had around 11,788 genes, more than three times the median gene count of prokaryotes (3935 genes) (11). Interestingly many of the common genes present within prokaryotes which coded for proteins that are responsible for cell division were absent here. But cell elongation protein genes were found to be intact and were found to be duplicated, forming multiple copies placed beside each other in the genome, which might suggest a possible reason why (Ca.) T. magnifica grow so long.\nThe researchers also found that, unlike most bacteria which reproduce via binary fission where cell volume doubles before cell division, in (Ca.) T. magnifica, they follow a dimorphic lifecycle; a bud-like cell stage and a filament-like cell stage. When they reproduce, the apical pole of the filament mother cell constricts and buds off to become a daughter cell (see Figure¬†1 and Figure¬†2). Interestingly, the genetic material is passed on to the daughter cell via pepins which is never seen before in bacteria."
  },
  {
    "objectID": "posts/research_article_biggest_bacterium/index_post.html#conclusion",
    "href": "posts/research_article_biggest_bacterium/index_post.html#conclusion",
    "title": "The world‚Äôs largest bacteria is bigger than a housefly",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nOverall (Ca.) T. magnifica turned out to be a very bizarre form of bacterial species that under the current understanding of cell morphology and energetics should not exist. So how come we find this bacteria in nature? Most bacteria are unable to grow to large sizes as they are met with various constraints related to energy and space. An overview of these constraints is given below.\n\nAs bacteria lack any mode of intracellular transporters to transport materials across the cytoplasm, they rely exclusively on diffusion. Chemical diffusion is extremely slow and is only effective in micro meter distances. So if the cell is too large, it will take a tremendous amount of time to disperse proteins and other materials effectively across the cell (2).\nWhen cell size increases, it reaches a point where the number of ribosomes needed to sustain the cell exceeds the available cell volume. This is called ‚Äòribosome catastrophe‚Äô and the upper limit of the cell volume for this catastrophe is 1.39¬±0.03 √ó 10^{‚àí15}m^3. The reported cytoplasmic volume of (Ca.) T. magnifica was around 5.91 √ó 10^{‚àí12} m^3, which is several orders greater than the ribosome catastrophe limit (12).\nPlasma membrane is the only location where ATP production can happen via the electron transport chain (ETC). And for bacteria and archaea, the only available membrane that they can use for ETC is their cellular membrane. Therefore as cell size increases, the surface area increase will quickly be outpaced by the volume increase owing to their differences in orders of increase. And as volume increase is followed by increased metabolic activity, energy production will become a bottleneck (12, 13).\n\nNow even if these constraints make perfect sense in the context of cell biology, there are always outliers in nature which defy them. One such outlies is this recently discovered bacterial species: (Ca.) T. magnifica.\nHow is (Ca.) T. magnifica overcoming these constraints? First of all, they have multiple copies of their genome compartmentalised along with ribosomes inside pepins. And these pepins are present across the cytoplasm. This might solve the diffusion limit problem because now the protein and other materials are almost always accessible across the cytoplasm at any time. Efficient compartmentalisation again might be the answer to surpassing the ribosome limit. In the case of the energy production limit attained due to surface area constraints, for (Ca.) T. magnifica, ATP synthase is expressed within the cytoplasm via pepin membranes and the complex membrane structure. Therefore, the available plasma membrane for energy production is much higher compared to other prokaryotes.\nIn the end, when we look at these features carefully, it is analogous to the various features present in eukaryotes. The polyploid genome of (Ca.) T. magnifica is equivalent to polypoid mtDNA present in eukaryotes. Compartmentalisation of DNA and ribosome via pepins is equivalent to compartmentalisation seen in the nucleus and the endoplasmic reticulum. And perhaps the icing on the cake is that (Ca.) T. magnifica transfers its genetic material via a membrane-bound organelle; pepins, something that is never seen before in prokaryotes. Therefore (Ca.) T. magnifica stands as one of the extreme cases of bacterial evolution which blurs our understanding of what constitutes a eukaryote and a prokaryote.\nFor more details about the study, please find the original paper (1)."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html",
    "href": "posts/research_article_embryonic_learning/index_post.html",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "",
    "text": "Summary of the study (1)\n\n\nResearch article that is summarised in this post: Crowder, C., & Ward, J. (2022). Embryonic antipredator defenses and behavioral carryover effects in the fathead minnow (Pimephales promelas). Behavioral Ecology and Sociobiology, 76(2), 27. https://doi.org/10.1007/s00265-022-03136-2"
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#tldr",
    "href": "posts/research_article_embryonic_learning/index_post.html#tldr",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "",
    "text": "Summary of the study (1)\n\n\nResearch article that is summarised in this post: Crowder, C., & Ward, J. (2022). Embryonic antipredator defenses and behavioral carryover effects in the fathead minnow (Pimephales promelas). Behavioral Ecology and Sociobiology, 76(2), 27. https://doi.org/10.1007/s00265-022-03136-2"
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#introduction",
    "href": "posts/research_article_embryonic_learning/index_post.html#introduction",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "2 Introduction",
    "text": "2 Introduction\nLearning is the process of acquiring information through experience. In nature, learning is ubiquitous in many different organisms and offers fitness advantages. Honeybees are known to learn floral features and utilize them during foraging (Review: 2), Wolf spiders (family Lycosidae) learn to avoid lesser quality prey when higher quality alternative prey is presented along with it (3), ants are known to learn and associate visual landmarks to their nest location (4). Learning is also particularly effective during different life stages. The experience acquired during the juvenile phase of an animal is known to affect their behaviour in adulthood. Baby Zebra finches (Taeniopygia guttata) are known to innately possess a ‚Äòtemplate song‚Äô which is used as a base to learn and mimic songs sung by adult conspecifics. If baby zebra finches are isolated and not allowed to mimic the song, then when they reach adulthood, they develop irregular and abnormally sound songs as compared to experienced adults (5). In the Tobacco hornworm moth, an aversive odour stimulus when learned during their larval stage persists when they metamorphose into a moth (6). In houseflies, adults develop a preference for olfactory cues that were exposed to them during their larval stage (7)."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#embryo-is-sensitive-to-external-cues",
    "href": "posts/research_article_embryonic_learning/index_post.html#embryo-is-sensitive-to-external-cues",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "3 Embryo is sensitive to external cues",
    "text": "3 Embryo is sensitive to external cues\nMost studies in the context of learning focused on fitness advantages to adults when learning was manifested during the juvenile phase. But some organisms, especially oviparous organisms undergo their embryonic stage outside of their mother‚Äôs body where the embryos are in direct contact with the surrounding environmental cues. They are also found to be highly sensitive to these cues. Perhaps the most famous evidence for this would be gender determination in reptiles, where the eggs respond to their surrounding ambient temperature. Here, gender is determined by temperature values after fertilization (Review: 8). In addition to sensing environmental cues, many studies have also reported evidence of ‚Äúembryonic learning‚Äù, where the embryos learn and form memories of surrounding environmental cues and utilize these memories in both the pre and post-embryonic period to modulate their behaviour. Some of the cases where the embryo is found to respond and/or learn external cues are given below;\n\nEggs of ringed salamanders (Ambystoma annulatum, Figure¬†1) when exposed to chemical cues from predators resulted in reduced activity and greater shelter-seeking behaviour in larvae (9).\nWood frog (Rana sylvatica, Figure¬†2) tadpoles learned to respond to chemical cues from unfamiliar predators when associated with conspecific alarm cues during the embryo stage (9).\nNaive cuttlefish (Sepia officinalis, Figure¬†3) prefer shrimps over crabs, but cuttlefish embryos which are visually conditioned with crabs preferred crabs over shrimps during the larval stage (10). Also, cuttlefish embryos that had experience with white crabs1 developed a preference for white crabs over black crabs2 during the larval stage (10). Moreover, cuttlefish larvae that received embryonic experience with white crabs exhibited prey generalisation where they prefer black crabs over shrimp (10).\nCuttlefish embryos (Sepia officinalis, Figure¬†3) are also seen to reduce their ventilation rate in the presence of predator indicative cues, possibly to remain cryptic. Furthermore, cuttlefish embryos are also found to associate non-predator cues with predator cues to invoke reduced ventilation behaviour which showcases evidence for associative learning in these embryos (11).\nRainbow trout embryos (Oncorhynchus mykiss, Figure¬†4) can learn both conspecific and heterospecific alarm cues and, rainbow trout fish with embryonic learning experiences had longer memory retention compared to the ones with no embryonic learning experience (12).\nBamboo sharks (Chiloscyllium punctatum, Figure¬†5), like all sharks, have electroreception which is normally used to locate prey. But unlike most other sharks where the embryo is developed within the mother‚Äôs body, in bamboo sharks egg sacks containing the embryo are oviposited on substrates which makes them vulnerable to predation (13). It is known that bamboo shark embryos can use their electrosensory system to innately detect electric fields indicative of predators and thereby cease their respiratory gill movements to become cryptic (14).\nPort Jackson shark (Heterodontus portusjacksoni, Figure¬†6) embryos are found to modulate their oxygen uptake rates against predator (Crested horn sharks; Heterodontus galeatus) and non-predator (Sand whiting; Sillago ciliata) olfactory cues with respect to their developmental stages. In earlier developmental stages, the embryos reduced oxygen uptake against non-predator odour cues but not against predator cues, indicating a cryptic response, possibly because of neophobia. At later developmental stages, the embryos increased oxygen uptake against predator cues and but not against non-predatory cues. This possibly indicates a fight-or-flight response where increased oxygen levels can contribute to enhanced capacity for aerobic or anaerobic activities that can aid in successful evasion by fleeing (15).\nCinnamon clownfish embryos (Amphiprion melanopus, Figure¬†7) are found to detect conspecific alarm cues and associate them with predator cues which resulted in increased heart rates (16). Increased heart rate is found to be positively correlated with anti-predator behaviours in fish.\n\nThese studies show that embryos can sense surrounding cues. The nature of surrounding cues also matters as some elicit innate behaviours and some don‚Äôt elicit any behaviour at all. For cues eliciting innate behaviours, some of the above-mentioned studies have shown that embryos can associate novel cues to these innately known cues and thereby change behaviour during the pre and/or post-embryonic stage.\n\n\n\n\n\n\n\nFigure¬†1: Image by Peter Paplanus from St.¬†Louis, Missouri - Ringed Salamander (Ambystoma annulatum)\n\n\n\n\n\n\n\nFigure¬†2: Image by Brian Gratwicke - Lithobates sylvaticus (Woodfrog). The wood frog is known as both Lithobates sylvaticus and Rana sylvatica\n\n\n\n\n\n\n\n\n\nFigure¬†3: Image by Diego Delso - Common cuttlefish (Sepia officinalis)\n\n\n\n\n\n\n\nFigure¬†4: Image by Mike Anderson - Female Rainbow trout (Oncorhynchus mykiss).\n\n\n\n\n\n\n\n\n\nFigure¬†5: Image by Zul M Rosle from Kuantan, Malaysia - Brownbanded bambooshark (Chiloscyllium punctatum) at the KLCC Aquaria.\n\n\n\n\n\n\n\nFigure¬†6: Image by Mark Norman/Museum Victoria - Port Jackson Shark (Heterodontus portusjacksoni) at Wilsons Promontory, Victoria.\n\n\n\n\n\n\n\n\n\nFigure¬†7: Image by Richard Ling - Red and Black Anemonefish (Amphiprion melanopus) in anemone (Entacmaea quadricolor). Steve‚Äôs Bommie, Ribbon Reefs, Great Barrier Reef 177094512."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#embryonic-response-to-predatory-cues",
    "href": "posts/research_article_embryonic_learning/index_post.html#embryonic-response-to-predatory-cues",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "4 Embryonic response to predatory cues",
    "text": "4 Embryonic response to predatory cues\nAs seen earlier, embryonic response to predators is evident and this might be because of strong predation pressures as most aquatic oviparous organisms are highly vulnerable to egg predators. While most studies have looked at the effect of embryonic response in juvenile and adult life stages, it might also have an immediate effect on the embryo itself [as seen in the Cuttlefish (11), in Bamboo shark (14), in Port Jackson shark (15) and Cinnamon clownfish (16)].\nSo meet fathead minnow fish (Pimephales promelas), who share the same fate of being embryonic prey as the above-mentioned ones. The Fathead minnow is a freshwater fish species native to North America. They live in a school of up to a hundred individuals (17) but during the onset of the breeding season. males depart from school and adopt a solitary lifestyle (18). Females of this species lay their eggs in sites guarded by a single male (19), which hatch into larvae after 5 days of post-fertilization. They are some of the well-studied model organisms which display prominent anti-predatory behaviours. They are perhaps best known for ‚ÄúSchreckstoff‚Äù (Paper is in German: 20), a chemical alarm signal, which is only produced when they suffer tissue damage, which occurs most often from predator attacks. Fathead minnow innately responds to these alarm cues and associates them with predator cues (21). With varying levels of these alarm cues, solitary individuals show a combination of anti-predatory behaviours that include, dashing, freezing, slowing and exploring (22). If chased by predators, they show an increased rate of shoaling3 and shooling4 and, also show increased shelter-seeking behaviour (23). Field studies have shown that they actively avoid areas containing heterospecific alarm cues (21). However, they are not reported to innately identify any predators (24).\nThis complex behaviour repertoire is only reported to be present in juveniles and adults (well no surprises there!). But fathead minnow starts their life cycle as eggs and faces high rates of predation (25). The embryonic stage is by far the worst period to get predated on as the embryos are heavily handicapped due to their immobility. Nevertheless, they are known to alter hatching times in response to predator cues (26). Moreover, when predator cues in combination with cues indicating embryo damage were introduced to embryos, the developed larvae from these embryos were small in size, indicating loss of developmental maturity (26). But in the presence of conspecific alarm cues, hatching times remain unchanged (27). Therefore these results suggest that fathead minnow embryos can sense environmental cues but clear-cut evidence was lacking. This is exactly what was explored by graduate student Christopher Crowder and Dr.¬†Jessica Ward at Ball State University, Indiana, USA. in their recent paper (1). The researchers asked whether fathead embryos can innately detect alarm cues just like their adult counterparts and if they do, then does that embryonic experience affect pre and/or post-embryonic behaviours?"
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#methods-and-results",
    "href": "posts/research_article_embryonic_learning/index_post.html#methods-and-results",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "5 Methods and results",
    "text": "5 Methods and results\nTo initiate embryonic experience, researchers grew the embryos in four different environments imparting ‚Äúpredator-risk‚Äù and ‚Äúnon-predator-risk‚Äù experiences.\n\n\nTable¬†1: Embryonic environment\n\n\n\n\n\n\nEnvironment\nCondition\n\n\n\n\nEnvironment 1\nEmbryos reared in a control condition (just water with no other cues)\n\n\nEnvironment 2\nEmbryos reared with predator cues\n\n\nEnvironment 3\nEmbryos reared with conspecific alarm cues\n\n\nEnvironment 4\nEmbryos reared with predator + conspecific alarm cues\n\n\n\n\nExcept for environments 1 and 2, all other environments indicated predator risk. Since the fatheaded minnow shows no innate behaviour against any predators, environment 2 (predator cue) should be similar to environment 1 (control). If associative learning occurs in fathead minnow embryos then the larvae which came from environment 4 should be able to associate the predator cue to the alarm cue, which can later be tested through behavioural assays. As adult fathead minnows are known to innately respond to conspecific alarm cues, environments 3 and 4 can be indicative of predation pressure and embryos should be able to detect them.\nAs fathead minnows show embryonic locomotor activity, it might make them conspicuous to egg predators. Interestingly, when locomotor activity was checked 5 days post fertilization, embryos from predator-risk environments (env. 3 and 4) showed lower motor activity, possibly indicating a cryptic response. This suggests that embryos can detect alarm cues like their adult counterparts.\nAfter testing for locomotor activity, the embryos were then grouped according to the environments they were raised in and were placed in separate control tanks devoid of alarm or predator cues. The embryos were allowed to hatch, and at 22 days post fertilization, the larvae were tested for predator avoidance behaviour and predator evasion behaviour. The idea was that larvae from predator-risk environments should show enhanced predator avoidance and evasion behaviours.\nTherefore, researchers tested the larvae individually on a test arena where swimming activity was checked before and after applying a stimulus. Four different stimuli were used which were synonymous with the environments. Here swimming activity was measured, which acted as a proxy to the ‚Äòfreezing‚Äô behaviour fathead minnow showed, which is a predator avoidance behaviour. Lower swimming activity corresponds to enhanced predator avoidance behaviour, as lower activity might make them more cryptic to predators.\nThe four stimuli that were used are;\n\n\nTable¬†2: Stimuli administered\n\n\nStimulus\nCondition\n\n\n\n\nStimulus 1\nControl (absence of predator or alarm cues)\n\n\nStimulus 2\nPredator cue\n\n\nStimulus 3\nConspecific alarm cue\n\n\nStimulus 4\nPredator + conspecific alarm cues\n\n\n\n\nLarvae from the predator risk environments (env. 3 and 4) should show enhanced freezing behaviour when predator risk stimuli (stimuli 3 and 4) are presented, compared to the control stimulus. Stimulus 2 should also elicit enhanced freezing behaviour if the embryo was able to associate the predator cue with the alarm cue (i.e.¬†associative learning occurred in embryos indicating evidence for embryonic learning)\nResearchers found that before stimulus use, larvae from predator-risk environments (env. 3 and 4) had decreased swimming activity. This is in conjunction with earlier mentioned studies, where it was shown that embryos which experience predator indicative cues showed an overall decrease in activity in post-embryonic stages. Upon stimulus usage, the same trend followed, larvae from predator-risk environments had an overall decreased swimming activity for all stimuli administration instances. Both these results indicate that the larvae from predator risk environments (env. 3 and 4) showed enhanced predator avoidance behaviour compared to larvae from non-predator risk environments (env. 1 and 2). In the case of larvae from environment 4 (predator + alarm cues), when the predator cue was administered, it showed reduced swimming activity compared to the control. But the difference was not statistically significant. The level of significance for this study was chosen to be \\alpha = 0.05 and the p-value for the difference between swimming activity when predator cue stimulus was used compared to control stimulus usage was 0.05 (same as the \\alpha value), as reported in the paper. But that does not rule out its biological significance and the researchers assert that associative learning has occurred. For larvae from environment 1 (control), except for the control stimulus, all other stimuli administration resulted in reduced swimming activity. This was a strange finding and the researchers suggested that it might be because of neophobia.\nThe next anti-predatory behaviour researchers looked at was predator evasion behaviour. As mentioned before, researchers hypothesized that larvae from predator-risk environments (env. 3 and 4) should show enhanced evasion behaviours. To quantify evasion behaviour, two parameters were measured; maximal body curvature and latency to respond to simulated predator attack. When a fish tries to escape from a predator, it bends its body into a ‚ÄúC‚Äù shape to generate momentum drag. The maximal body curvature is the angle formed when the ‚ÄúC‚Äù shape is formed in a fish. A shorter angle means that the body bend is greater indicating an enhanced escape behaviour. Overall, it was found that predator risk environments (env. 3 and 4) have resulted in reduced body curvature angle, indicating enhanced evasion behaviour in the larvae. But the evidence for associative learning was not present in this case, as for larvae from environment 4 (predator + alarm cues), upon predator cue administration, the maximal body curvature angle was not significantly different from the control. Latency to respond was found to be independent of embryonic environment and stimulus administration, as no change was observed between the groups. This indicates that post-embryonic behaviour can have varying levels of susceptibility to modification via embryonic learning."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#conclusion",
    "href": "posts/research_article_embryonic_learning/index_post.html#conclusion",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nThe researchers showed, for the first time in fathead minnow embryos (Pimephales promelas), that they can detect conspecific alarm cues, moreover, they also showed that;\n\nIn addition to detecting conspecific alarm cues, the embryos, in response to its detection also modulated their embryonic locomotor activity to become cryptic, indicating an immediate adaptive response within the embryos\nEmbryonic experience overall enhanced predator avoidance and evasion behaviours\nEmbryonic associative learning only modulated predator avoidance and not evasion behaviour in the larvae, indicating a varying level of susceptibility to modification via embryonic learning\n\nThe evidence for associative learning in this study did not achieve statistical significance and was in a borderline area (p = 0.05, same as \\alpha = 0.05) but that does not mean there is no biological significance. As sample sizes were adequate, we can explore some possible future directions.\n\nThe predator cue used in this study came from a known egg predator of fathead minnow, the blue gill sunfish (Lepomis macrochirus). Using another predator cue in association with the conspecific alarm cue can mitigate any possible intrinsic effect the previous predator cue had in learning.\nAs fathead minnows were trained to associate predator cues to conspecific alarm cues throughout 5 days post fertilization as embryos and were only tested 22 days post fertilization as larvae. There was a significant time gap between training and testing which might have corroded its memory. But fathead minnows are known to form long-lasting associations between odour cues and other cues. In an earlier study (28), fathead minnow was shown to distinguish between familiar shoal mates and unfamiliar conspecifics, and they were able to maintain this discriminability even after 2 months of separation suggesting evidence for a long-lasting memory. I did not find any other studies which have exclusively looked at learning and memory in fathead minnow. An interesting experiment to follow up would be, training fathead minnow embryos to associate a predator cue to the conspecific alarm and testing this association at different ages after hatching. This can again check if embryonic associative learning occurred? if it occurred then how does the memory fare with increasing time interval between last association and larval age.\n\nIn conclusion, this study (1) showcased a whole new aspect of learning in fathead minnow (Pimephales promelas) which opens up new avenues of research questions given their peculiar behaviour and life history."
  },
  {
    "objectID": "posts/research_article_embryonic_learning/index_post.html#footnotes",
    "href": "posts/research_article_embryonic_learning/index_post.html#footnotes",
    "title": "Learning starts in the embryo for fathead minnow fish",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCrabs with at least half the carapace area covered with white spots‚Ü©Ô∏é\nCrabs with dull brown carapace‚Ü©Ô∏é\nGroup of fish that stay together for social reasons‚Ü©Ô∏é\nGroup of fish moving together in a coordinated manner‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/welcome_to_blog/index.html",
    "href": "posts/welcome_to_blog/index.html",
    "title": "The story behind one carat blog",
    "section": "",
    "text": "Cover photo of my old blog made using distill\n\n\n\n\n\n\n\nCover photo of my new blog made using quarto"
  },
  {
    "objectID": "posts/welcome_to_blog/index.html#cover-photos-of-my-blogs",
    "href": "posts/welcome_to_blog/index.html#cover-photos-of-my-blogs",
    "title": "The story behind one carat blog",
    "section": "",
    "text": "Cover photo of my old blog made using distill\n\n\n\n\n\n\n\nCover photo of my new blog made using quarto"
  },
  {
    "objectID": "posts/welcome_to_blog/index.html#making-of-one-carat-blog",
    "href": "posts/welcome_to_blog/index.html#making-of-one-carat-blog",
    "title": "The story behind one carat blog",
    "section": "2 Making of one carat blog",
    "text": "2 Making of one carat blog\nMy name is Jewel Johnson and I am the creator of this blog. The story about this blog started with my wish to have a writing space, to showcase the things that I learned. I find that having a blog to write, instills self-discipline in learning new things. So I initially began looking for ways to build a blog. I read many online articles which talked about using WordPress, Blogger, Weebly, Wix and so on. But almost all of these services were paid and I was not able to afford them. I began looking for free alternatives and I came to know about Medium. Now Medium is not precisely a blog but a publishing platform and it restricts readers behind a paywall if they want to read your posts. But you do get paid if anyone reads your articles. So I tried writing a few articles on R programming. Medium supports code blocks but it was extremely difficult to use them and it was never tailored for writing R codes. As I was interested in sharing some of the things I learned about R, I wanted to see what all articles were written about R programming on Medium. So I read some of them in Towards data science, which is part of Medium but specialized in articles related to data science. There were some good articles but to my surprise, most articles were downright copy-pasting what was there in the package tutorials or their related source webpages. And the articles were made with very low effort. The only thing I found nice was the stock images. So I ditched Medium for these reasons and went on to see if I can make a blog using purely R. And that‚Äôs when I came to know about the {Rmarkdown} package in R and Github pages. So long story short, I learned how to use this package and how to host a webpage for free on Github. And this led to the creation of my first blog. It was very basic and visually simple, but I was proud of it. So I began writing tutorials on data visualization using R and wrote my first article about the {ggplot2} package in R. I shared it with my friends and received good support from them. Then I wanted to improve my blog and I began searching for new ways to make the blog better, which led me to learn about the {distill} package in R.\nI was flabbergasted when I saw the blogs and websites made using the distill package. They were so much better than the ones made from {Rmarkdown}. I first learned about {distill} from Dr.¬†Ella Kaye‚Äôs blog post and Dr.¬†Lisa Lendway‚Äôs blog post. And after reading Prof.¬†Andreas Handel blog post on how to build a website using distill I was convinced to rebuild my blog. The YouTube video by Dr.¬†Lisa Lendway‚Äôs and the official tutorials given on the distill website also helped me very much. And the end product was this beautiful blog that I created. I even named my blog; 'one carat blog', a name resembling me.\nAnd all was well and I began concentrating on my blog. I went on to write a series of articles encompassing a complete tutorial on learning data visualization and data manipulation using R. It was well received and I got a lot of helpful comments about it. One thing that kind of annoyed me about the {distill} package was that the table of contents won‚Äôt float for long posts. This made the ‚Äòtable of contents‚Äô obsolete. Nevertheless, the package fulfilled most of what I wanted to have in a blog. This period was also the first time I began learning about HTML and CSS languages. Using that knowledge I began tweaking my blog and customizing it. I also wrote an article highlighting a few quality of life modifications one can implement in their distill blog and to my surprise, the post was adopted to the official distill website, which showcased useful tutorials related to distill blogging.\n\n\nGreat to see you‚Äôve added your blog to the distillery. You should also consider adding your excellent ‚ÄúQuality of life modifications for your distill websites‚Äù post to the Tips & Tricks section of the site https://t.co/SMOtCU0eM1\n\n‚Äî Ella Kaye (@ellamkaye) December 20, 2021\n\n\nI also received back from Dr.¬†Ella Kaye‚Äôs in Twitter and I was added to the distill club! I was very happy at that time.\n\n\nThis is fab! Welcome to #DistillClub pic.twitter.com/4jpeBe7Kis\n\n‚Äî Ella Kaye (@ellamkaye) December 20, 2021\n\n\nThis was also the time when I changed the About me page. Everyone who was having a distill blog was using a default template provided by the {postcards} package in R. But I wanted to have a unique ‚Äòabout me‚Äô page, which acted like my CV. So I used the modern resume theme by James Grant and made this cool looking CV.\nThen for about four months, I had the worst time in my life. I was feeling depressed and sick. Many things had happened and I could not do any blogging or anything for that matter. And then finally after sorting out my affairs, I went back to blogging. I opened Rstudio one day and I got a blank window. This was the first time I was seeing this bug, so when I searched about it, I learned that a new package for R called {quarto} was causing some issues with Rstudio. To my knowledge, it only affected Linux users. Then I searched about what is {quarto} package is about and I struck a gold mine.\nIt was everything {distill} had, but better. The best thing was that it natively supported a floating table of contents, something which was missing in {distill}. My first introduction to quarto came from Dr.¬†Alison Hill‚Äôs blog post where she gave a brief intro on what the quarto package is about. After reading the post I was in a dilemma as to whether I should migrate my blog to quarto or not. Then I found that most of the blogs featured on the distill official website had migrated to quarto. It was the new trend in town, and everyone was adapting it. So I finally made up my mind and decided to migrate, again, for the second time and it led to a few sleepless nights that I am not very proud of. The post from Dr.¬†Danielle Navarro was also very helpful and made the transition process much easier.\nSo with help of all these articles and many glasses of homemade wine, I finally migrated from {distill} to {quarto} and made this blog which you are browsing now. I had to convert all my .Rmd files to the new .qmd file that quarto supports. The YAML headers were a pain in the beep to change. But in the end, it was worth it. I changed the previous visitor counter to a new flag counter for this blog. I also designed a new icon and a new cover image. And as of writing this blog post, I also wrote a series of articles focusing on beginner-level statistics in my blog. And recently I have been working on a software called Notion and have made some cool productivity templates focusing on PhD students. I also have written some blog posts showcasing those templates, so make sure to check them out.\nTo conclude I hitchhiked from Rmarkdown to distill and now to quarto, the end product is this beautifully made blog that you are browsing. I hope you enjoy this blog and find the posts and tutorials helpful. If you find them useful, make sure to share them. If you have any suggestions or feedback for this blog, then please comment them in the comment section at the end of the blog. Thanks for visiting my blog and I hope you have a great time."
  },
  {
    "objectID": "posts/welcome_to_blog/index.html#references",
    "href": "posts/welcome_to_blog/index.html#references",
    "title": "The story behind one carat blog",
    "section": "References",
    "text": "References\n\nUseful Distill articles\n\nKaye (2021, May 8). ELLA KAYE: Welcome to my {distill} website!. Retrieved from https://ellakaye.rbind.io/posts/2021-05-08-welcome-to-my-distill-website/\nLendway (2020, Dec.¬†18). Lisa Lendway: Building a {distill} website. Retrieved from https://lisalendway.netlify.app/posts/2020-12-09-buildingdistill/\nCreate a GitHub website with distill in less than 30 minutes by Prof.¬†Andreas Handel.\nOfficial guide on creating a website using distill.\nBuilding a website using R {distill}. Youtube video by Dr.¬†Lisa Lendway.\n\nUseful Quarto articles\n\nWe don‚Äôt talk about Quarto by Dr.¬†Alison Hill.\nDanielle Navarro. 2022. ‚ÄúPorting a Distill Blog to Quarto.‚Äù April 20, 2022. https://blog.djnavarro.net/posts/2022-04-20_porting-to-quarto.\nCreating a blog with Quarto in 10 steps by Beatriz Milz\nCool quarto tips and tricks. Tweet by Albert Rapp\nOfficial guide by developers of Quarto"
  },
  {
    "objectID": "posts/word_happy_2022/index.html",
    "href": "posts/word_happy_2022/index.html",
    "title": "The World Happiness Report 2022",
    "section": "",
    "text": "World Happiness Report 2022 shows which are the happiest countries for the year 2022. By statistically analysing six key parameters, each country is given a score (which is called happiness score within the dataset). The higher the score, the happier the country is and vice versa. The six key parameters which are taken into analysis for determining the score are;\n\nGross domestic product per capita\nSocial support\nHealthy life expectancy\nFreedom to make your own life choices\nGenerosity of the general population\nPerceptions of internal and external corruption levels.\n\nFinland is ranked first among 149 countries with an overall score of 7.82. Despite COVID 19 wrecking havoc around the world, citizens of Finland have persevered through it and they have been maintaining first rank since 2016. Afghanistan is at the lowest rank with a score of 2.40. With complications from COVID 19 pandemic and the Taliban take over, Afghanistan is going through one of the worst humanitarian crisis in human history and the ranking reflects it."
  },
  {
    "objectID": "posts/word_happy_2022/index.html#introduction",
    "href": "posts/word_happy_2022/index.html#introduction",
    "title": "The World Happiness Report 2022",
    "section": "",
    "text": "World Happiness Report 2022 shows which are the happiest countries for the year 2022. By statistically analysing six key parameters, each country is given a score (which is called happiness score within the dataset). The higher the score, the happier the country is and vice versa. The six key parameters which are taken into analysis for determining the score are;\n\nGross domestic product per capita\nSocial support\nHealthy life expectancy\nFreedom to make your own life choices\nGenerosity of the general population\nPerceptions of internal and external corruption levels.\n\nFinland is ranked first among 149 countries with an overall score of 7.82. Despite COVID 19 wrecking havoc around the world, citizens of Finland have persevered through it and they have been maintaining first rank since 2016. Afghanistan is at the lowest rank with a score of 2.40. With complications from COVID 19 pandemic and the Taliban take over, Afghanistan is going through one of the worst humanitarian crisis in human history and the ranking reflects it."
  },
  {
    "objectID": "posts/word_happy_2022/index.html#getting-the-data",
    "href": "posts/word_happy_2022/index.html#getting-the-data",
    "title": "The World Happiness Report 2022",
    "section": "\n2 Getting the data",
    "text": "2 Getting the data\nIn this post we will do some exploratory data visualizations using data from The World Happiness Report 2022. You can download the .csv file from here."
  },
  {
    "objectID": "posts/word_happy_2022/index.html#plotting-a-world-map",
    "href": "posts/word_happy_2022/index.html#plotting-a-world-map",
    "title": "The World Happiness Report 2022",
    "section": "\n3 Plotting a world map",
    "text": "3 Plotting a world map\nWe will plot a world map with a scalable colour palette based on the ladder score where greater scores indicated happier countries and vice versa.\nIn short what we we will be doing is, we are going to join the World Happiness Report 2021 dataset with the map data and plot it using the ggplot2 package. The map_data() function helps us easily turn data from the {maps} package in to a data frame suitable for plotting with ggplot2.\n\n# required packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(viridis)\n\n# load the dataset which you have downloaded\n# please change the location to where your downloaded file is kept\nhap_pre &lt;- read.csv(\"datasets/2022.csv\")\n\n# renaming column names of ease of use\ncolnames(hap_pre)[2] &lt;- \"country\"\ncolnames(hap_pre)[3] &lt;- \"score\"\nhap_pre &lt;- hap_pre[-147,]\n\n# the score values are separated by commas\n# let us change that to dots\nhap_pre$score &lt;- scan(text=hap_pre$score, dec=\",\", sep=\".\")\n\n# selecting country and score columns\nhap &lt;- hap_pre %&gt;% select(country,score)\n\n# removing special characters in df\nhap$country &lt;- gsub(\"[[:punct:]]\",\"\",as.character(hap$country))\n\n# loading map\nmap_world &lt;- map_data('world')\n# remove Antarctica\nmap_world &lt;- map_world[!map_world$region ==\"Antarctica\",]\n\n# checking which country names are a mismatch between map data and the downloaded dataset\nanti_join(hap, map_world,  by = c(\"country\" = \"region\"))\n\n\n\n  \n\n\n\nYou can see that some country names are a mismatch with the map data. So we will attempt to fix it.\n\n# display all country names in the dataset\n# useful to locate correct country names\n#map_world %&gt;% group_by(region) %&gt;% summarise() %&gt;% print(n = Inf)\n\n# correcting country names\n# here we are matching the country names of downloaded dataset with the map data\ncorrect_names &lt;- c(\"United States\" = \"USA\",\n                   \"United Kingdom\" = \"UK\",\n                   \"Czechia\" = \"Czech Republic\",\n                   \"Taiwan Province of China\"  = \"Taiwan\",\n                   \"North Cyprus\"= \"Cyprus\",\n                   \"Congo\"= \"Republic of Congo\",\n                   \"Palestinian Territories\" = \"Palestine\",\n                   \"Eswatini Kingdom of\" = \"Swaziland\")\n\n# recoding country names \nhap2 &lt;- hap %&gt;% mutate(country = recode(country, !!!correct_names))\n\n# joining map and the data\nworld_hap &lt;- left_join(map_world, hap2, by = c(\"region\" = \"country\"))\n\n# creating a function to add line in text, for the caption\naddline_format &lt;- function(x,...){\n  gsub(',','\\n',x)}\n\n# plotting the world map\nggplot(world_hap, aes(long, lat)) + geom_polygon(aes(fill = score, group = group)) +\n  scale_fill_viridis(option = \"viridis\") + theme_void() +\n  theme(plot.background = element_rect(fill = \"aliceblue\"),\n        legend.position=\"bottom\") + \n  labs(title = \"Happiness scores of countries in 2022\",\n       subtitle = addline_format(\"Higher scores indicate happier countries and vice versa,Grey colour represents countries with no data\"),\n       fill = \"Score\",\n       caption = addline_format(\"Source: World Happiness Report 2022,Visualization by Jewel Johnson\"))"
  },
  {
    "objectID": "posts/word_happy_2022/index.html#plotting-an-interactive-world-map",
    "href": "posts/word_happy_2022/index.html#plotting-an-interactive-world-map",
    "title": "The World Happiness Report 2022",
    "section": "\n4 Plotting an interactive world map",
    "text": "4 Plotting an interactive world map\nLets plot an interactive world map with happiness score as a variable, where greater scores indicates happier countries and vice versa. We will be using the leaflet package in R for plotting the world map.\nWe have to download the world map data which comes as a .shp file.\nRun the codes given below to download the .shp file and load the .csv file required to plot the map. For plotting the interactive map, we will be using the sf package for reading the .shp file and the leaflet package for plotting the map.\n\n# loading the .csv file which was downloaded\n# please change the location to where your .csv file is kept\nhap_pre &lt;- read.csv(\"datasets/2022.csv\")\n\n# renaming column names of ease of use\ncolnames(hap_pre)[1] &lt;- \"rank\"\ncolnames(hap_pre)[2] &lt;- \"country\"\ncolnames(hap_pre)[3] &lt;- \"score\"\nhap_pre &lt;- hap_pre[-147,]\n\n# the score values are separated by commas\n# let us change that to dots\nhap_pre$score &lt;- scan(text=hap_pre$score, dec=\",\", sep=\".\")\n\n# selecting country and score columns\nhap &lt;- hap_pre %&gt;% select(rank,country,score)\n\n# removing special characters in df\nhap$country &lt;- gsub(\"[[:punct:]]\",\"\",as.character(hap$country))\n\nNow that we have the dataset ready, let us download the .shp file which contains the world map.\n\n# downloading and loading the .shp file\n# please change the 'destfile' location to where your zip file is located\ndownload.file(\"http://thematicmapping.org/downloads/TM_WORLD_BORDERS_SIMPL-0.3.zip\",\n              destfile=\"shp/world_shape_file.zip\")\n\n# unzip the file into a directory. You can do it with R (as below).\n# the directory of my choice was folder named 'shp'\nunzip(\"shp/world_shape_file.zip\", exdir = \"shp/\")\n\nNow let us load the .shp file and plot the map.\n\n# Read the shape file with 'sf'\n#install.packages(\"sf\")\nlibrary(sf)\n\nworld_spdf &lt;- st_read(paste0(getwd(),\"/shp/TM_WORLD_BORDERS_SIMPL-0.3.shp\"), stringsAsFactors = FALSE)\n\nReading layer `TM_WORLD_BORDERS_SIMPL-0.3' from data source \n  `D:\\Work\\GitHub\\one-carat-blog\\posts\\word_happy_2022\\shp\\TM_WORLD_BORDERS_SIMPL-0.3.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 246 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.57027\nGeodetic CRS:  WGS 84\n\n# checking which country names are a mismatch between map data and the downloaded dataset\n# this is an important check as we have to join the happiness dataset and .shp file with country names\nanti_join(hap, world_spdf,  by = c(\"country\" = \"NAME\"))\n\n\n\n  \n\n\n# correcting country names, note that some countries are not available in the .shp file\n\ncorrect_names &lt;- c(\"Czechia\" = \"Czech Republic\",\n                   \"Taiwan Province of China\" = \"Taiwan\",\n                   \"South Korea\" = \"Korea, Republic of\",\n                   \"Moldova\"  = \"Republic of Moldova\",\n                   \"Belarus*\" = \"Belarus\",\n                   \"Vietnam\" = \"Viet Nam\",\n                   \"Hong Kong SAR of China\"= \"Hong Kong\",\n                   \"Libya\" = \"Libyan Arab Jamahiriya\",\n                   \"Ivory Coast\" = \"Cote d'Ivoire\",\n                   \"North Macedonia\" = \"The former Yugoslav Republic of Macedonia\",\n                   \"Laos\" = \"Lao People's Democratic Republic\",\n                   \"Iran\" = \"Iran (Islamic Republic of)\",\n                   \"Palestinian Territories*\" = \"Palestine\",\n                   \"Eswatini, Kingdom of*\" = \"Swaziland\",\n                   \"Myanmar\" = \"Burma\",\n                   \"Tanzania\" = \"United Republic of Tanzania\")\n\n# recoding country names \nhap2 &lt;- hap %&gt;% mutate(country = recode(country, !!!correct_names))\n\n# joining .shp file and the happiness data\nworld_hap &lt;-  left_join(world_spdf, hap2, by = c(\"NAME\" = \"country\"))\n\n#install.packages(\"leaflet\")\nlibrary(leaflet)\nlibrary(viridis)\n\n# making colour palette for filling\nfill_col &lt;- colorNumeric(palette=\"magma\", domain=world_hap$score, na.color=\"transparent\")\n\n# Prepare the text for tooltips:\ntext &lt;- paste(\n  \"Country: \", world_hap$NAME,\"&lt;br/&gt;\", \n  \"Score: \", world_hap$score, \"&lt;br/&gt;\", \n  \"Rank: \", world_hap$rank, \n  sep=\"\") %&gt;%\n  lapply(htmltools::HTML)\n\n# plotting interactive map\nleaflet(world_hap) %&gt;% \n  addTiles()  %&gt;% \n  setView( lat=10, lng=0 , zoom=2) %&gt;%\n  addPolygons( \n    fillColor = ~fill_col(score), \n    stroke=TRUE, \n    fillOpacity = 0.9, \n    color= \"grey\", \n    weight=0.3,\n    label = text,\n    labelOptions = labelOptions( \n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"), \n      textsize = \"13px\", \n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend( pal=fill_col, values=~score, opacity=0.7, title = \"Score\", position = \"bottomleft\" )"
  },
  {
    "objectID": "posts/word_happy_2022/index.html#summary",
    "href": "posts/word_happy_2022/index.html#summary",
    "title": "The World Happiness Report 2022",
    "section": "\n5 Summary",
    "text": "5 Summary\nI hope this post was helpful to you in understanding how to plot world maps in R. In short using ggplot2 we have first plot a static world map using the data from The World Happiness Report 2022, then similarly using the leaflet package we plotted an interactive world map."
  },
  {
    "objectID": "posts/word_happy_2022/index.html#references",
    "href": "posts/word_happy_2022/index.html#references",
    "title": "The World Happiness Report 2022",
    "section": "\n6 References",
    "text": "6 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\nJoe Cheng, Bhaskar Karambelkar and Yihui Xie (2021). leaflet: Create Interactive Web Maps with the JavaScript ‚ÄòLeaflet‚Äô Library. R package version 2.0.4.1. https://CRAN.R-project.org/package=leaflet\nPebesma, E., 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1), 439-446, https://doi.org/10.32614/RJ-2018-009\nTutorial on plotting interactive maps in R.\nThe World Happiness Report\nSource for .csv file of World Happiness Score of countries 2022. Compiled by Mathurin Ach√© in Kaggle.com"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html",
    "href": "tutorials/data_man/dplyr_1.html",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "",
    "text": "The dplyr package is a grammar of data manipulation just like how ggplot2 is the grammar of data visualization. It helps us to apply a wide variety of functions such as;\n\nSummarising the dataset\nApplying selections and orderings as a function of a variable\nCreating new variables as a function of existing variables\n\nWe will see in-depth how to manipulate our data like a boss!"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#introduction-to-dplyr-package",
    "href": "tutorials/data_man/dplyr_1.html#introduction-to-dplyr-package",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "",
    "text": "The dplyr package is a grammar of data manipulation just like how ggplot2 is the grammar of data visualization. It helps us to apply a wide variety of functions such as;\n\nSummarising the dataset\nApplying selections and orderings as a function of a variable\nCreating new variables as a function of existing variables\n\nWe will see in-depth how to manipulate our data like a boss!"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#the-pipe-operator",
    "href": "tutorials/data_man/dplyr_1.html#the-pipe-operator",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n2 The pipe operator %>%",
    "text": "2 The pipe operator %&gt;%\nPerhaps the most amazing thing in making codes short and efficient is the pipe operator which is originally from the magrittr package which is made available for the dplyr package. The pipe operator helps you skip the intermediate steps of saving an object before you can use them in command. It does so by ‚Äòpiping‚Äô together results from the first object to the function ahead of the pipe operator. The command x %&gt;% y %&gt;% z can be read as ‚Äòtake the result of x and use it with function y and take that result and use it with function z‚Äô. This is the gist of what the pipe operator does. Allow me to demonstrate.\n\nlibrary(ggplot2)\n\n# dummy data\na &lt;- c(sample(1:100, size = 50))\nb &lt;- c(sample(1:100, size = 50))\ndata &lt;- as.data.frame(cbind(a,b))\n\n# without %&gt;%\ndata &lt;- mutate(data, ab = a*b, twice_a = 2*a)\ndata_new &lt;- filter(data, ab &lt; 300, twice_a &lt; 200)\nggplot(data_new, aes(ab, twice_a)) + geom_point()\n\n# with %&gt;%\ndata %&gt;% mutate(ab = a*b, twice_a = 2*a) %&gt;% \n  filter(ab &lt; 300, twice_a &lt; 200) %&gt;%\n  ggplot(aes(ab, twice_a)) + geom_point()\n\nAs you can see, with pipe operator %&gt;%, we did not have to save any objects in the intermediate steps and also it improved the overall clarity of the code. I have used a few commands from the dplyr package in the example given above. So without further ado let us delve into the dplyr package. For this chapter, I will be using the penguin dataset from the popular {palmerpenguin} package as an example.\n\n# install palmerpenguins package\ninstall.packages(\"palmerpenguins\")\nlibrary(dplyr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#grouping-the-data",
    "href": "tutorials/data_man/dplyr_1.html#grouping-the-data",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n3 Grouping the data",
    "text": "3 Grouping the data\n\n3.1 group_by()\nThe command group_by() allows us to group the data via existing variables. It allows for a ‚Äòsplit-apply-combine‚Äô way of getting output. First, it will split the data or group the data with the levels in the variable, then apply the function of our choice and then finally combine the results to give us a tabular output. On its own the command doesn‚Äôt do anything, we use it in conjunction with other commands to get results based on the grouping we specify. The command ungroup() is used to ungroup the data."
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#summarising-the-data",
    "href": "tutorials/data_man/dplyr_1.html#summarising-the-data",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n4 Summarising the data",
    "text": "4 Summarising the data\n\n4.1 summarise()\nThe summarise() command allows you to get the summary statistics of a variable or a column in the dataset. The result is given as tabular data. Many types of summary statistics can be obtained using the summarise() function. Some of them are given below. To calculate average values it is necessary to drop NA values from the dataset. Use drop_na() command from the tidyr package. The comments denote what each summary statistic is.\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\nsummary_data &lt;- penguins %&gt;% drop_na() %&gt;%\n  group_by(species) %&gt;% # we are grouping/splitting the data according to species\n  summarise(avg_mass = mean(body_mass_g), # mean mass\n            median_mass = median(body_mass_g), # median mass\n            max_mass = max(body_mass_g), # max value of mass, can also use min()\n            standard_deviation_bill_length = sd(bill_length_mm), # standard deviation of bill_length\n            sum_mass = sum(flipper_length_mm), # sum\n            distinct_years = n_distinct(year), # distinct values in column year\n            no_of_non_NAs = sum(!is.na(year)), # gives no of non NAs, \n            length_rows = n(), # length of the rows\n            iqr_mass = IQR(body_mass_g), # inter quartile range of mass\n            median_absolute_deviation_mass = mad(body_mass_g), # median absolute deviation of mass\n            variance_mass = var(body_mass_g)) # variance\n# viewing summary as a table\nsummary_data\n\n\n\n  \n\n\n\nThe number of non NA values will be the same as that of n() result as we have used drop_na()command in the beginning.\nThe base function summary() in R also gives the whole summary statistics of a dataset.\n\nlibrary(palmerpenguins)\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\n4.2 when to use group_by()\nIt can be confusing to decide when to use the group_by() function. In short, you should use it whenever you want any function to act separately on different groups present in the dataset. Here is a graphical representation of how the summarise() function is used to calculate the mean values of a dataset. When used with group_by() it calculates mean values for the respective groups in the data, but when group_by() is not used, it will calculate the mean value of the entire dataset irrespective of the different groups present and outputs a single column.\n\n\n\n\n\n\n4.3 count()\nThe count() command is used to count the number of rows of a variable. Has the same function as that of n()\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\ncount &lt;- penguins %&gt;% group_by(species) %&gt;%\n  count(island)\n\n# viewing count as a table\ncount"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#manipulating-cases-or-observations",
    "href": "tutorials/data_man/dplyr_1.html#manipulating-cases-or-observations",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n5 Manipulating cases or observations",
    "text": "5 Manipulating cases or observations\nThe following functions affect rows to give a subset of rows in a new table as output.\n\n5.1 filter()\nUse filter() to filter rows corresponding to a given logical criteria\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% filter(body_mass_g &lt; 3000)\n\n\n\n  \n\n\n\n\n5.2 distinct()\nUse distinct() to remove rows with duplicate or same values.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% group_by(species) %&gt;% distinct(body_mass_g)\n\n\n\n  \n\n\n\n\n5.3 slice()\nUse slice() to select rows by position.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# slice from first row to fifth row\npenguins %&gt;% slice(1:5) \n\n\n\n  \n\n\n\n\n5.4 slice_sample()\nUse slice_sample() to randomly select rows from the dataset. Instead of (n = ) you can also provide the proportion value (between 0 and 1) using (prop = ). For e.g.¬†for a dataset with 10 rows, giving (prop = 0.5) will randomly sample 5 rows. Other related functions include;\n\n\npreserve : Values include TRUE to preserve grouping in a grouped dataset and FALSE to not preserve grouping while sampling.\n\nweight_by : Gives priority to a particular variable during sampling. An example is given below.\n\nreplace : Values include TRUE if you want sampling with replacement which can result in duplicate values, FALSE if you want sampling without replacement.\n\n\n\n(n = 4)\nweight_by\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# samples 4 rows randomly\npenguins %&gt;% slice_sample(n = 4)\n\n\n\n  \n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# sampling will favour rows with higher values of 'body_mass_g'\npenguins %&gt;% drop_na() %&gt;% slice_sample(n = 4, weight_by = body_mass_g)\n\n\n\n  \n\n\n\n\n\n\n\n5.5 slice_min() and slice_max()\nUse slice_min() to extract rows containing least values and use slice_max() to extract rows with greatest values. The function with_ties = FALSE is included to avoid tie values.\n\n\nslice_min()\nslice_max()\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% slice_min(body_mass_g, n = 4, with_ties = FALSE)\n\n\n\n  \n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% slice_max(body_mass_g, n = 4,with_ties = FALSE)\n\n\n\n  \n\n\n\n\n\n\n\n5.6 slice_head() and slice_tail()\nUse slice_head() to extract first set of rows and use slice_tail() to extract last set of rows.\n\n\nslice_head()\nslice_tail()\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% slice_head(n = 4)\n\n\n\n  \n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% slice_tail(n = 4)\n\n\n\n  \n\n\n\n\n\n\n\n5.7 arrange()\nUse arrange() to arrange rows in a particular order.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# arranging rows in descending order of bill length\n# by default it arranges data by ascending order when no specifications are given\npenguins %&gt;% arrange(desc(bill_length_mm))\n\n\n\n  \n\n\n\n\n5.8 add_row()\nUse add_row() to add rows to the dataset.\n\nlibrary(dplyr)\n\nName &lt;- c(\"a\", \"b\")\nAge &lt;- c(12,13)\ndata.frame(Name, Age) %&gt;% add_row(Name = \"c\", Age = 15)"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#manipulating-variables-or-columns",
    "href": "tutorials/data_man/dplyr_1.html#manipulating-variables-or-columns",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n6 Manipulating variables or columns",
    "text": "6 Manipulating variables or columns\nThe following functions affect columns to give a subset of columns in a new table as output.\n\n6.1 pull()\nUse pull() to extract columns as a vector, by name or index. Only the first 10 results are shown for easy viewing.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% pull(body_mass_g)\n\n\n6.2 select()\nUse select() to extract columns as tables, by name or index.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% select(species, body_mass_g)\n\n\n\n  \n\n\n\n\n6.3 relocate()\nUse relocate() to move columns to new position. Results are not shown as these are trivial results.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# relocates 'species' column to last position\npenguins %&gt;% relocate(species, .after = last_col())\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# relocates 'species' column before column 'year' and renames the column as 'penguins'\npenguins %&gt;% relocate(penguins = species, .before = year)\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# you can also relocate columns based on their class\n# relocates all columns with 'character' class to last position\npenguins %&gt;% relocate(where(is.character), .after = last_col())\n\n\n6.4 rename()\nUse rename() function to rename column names in the dataset.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# renames the column sex to gender\npenguins %&gt;% rename(gender = sex)\n\n\n6.5 mutate()\nUse mutate() function to create new columns or variables.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% drop_na() %&gt;% \n  group_by(species) %&gt;%\n  mutate(mean_mass = mean(body_mass_g))\n\n\n\n  \n\n\n\n\n6.6 transmute()\nDoes the same function as mutate() but in the process will drop any other columns and give you a table with only the newly created columns.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% drop_na() %&gt;% \n  group_by(species) %&gt;%\n  transmute(mean_mass = mean(body_mass_g))\n\n\n\n  \n\n\n\n\n6.7 across()\nUse across() to summarise or mutate columns in the same way. First example shows across() used with summarise() function.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# summarise across columns body mass, bill length and bill depth\n# and calculate the mean values\n# since we are calculating mean values,\n# NAs are dropped using 'drop_na() function from 'tidyr' package\n\npenguins %&gt;% drop_na() %&gt;%\n  group_by(species) %&gt;%\n  summarise(across(c(body_mass_g, bill_length_mm, bill_depth_mm), mean))\n\n\n\n  \n\n\n\nSecond example showing across() used with mutate() function. We can efficiently create new columns using mutate() and across() together. Suppose we want to multiply all numerical values in a dataset with 2 and create new columns of those values. This can be done using the code below.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# define the function\ntwo_times &lt;- function(x) {\n  2*x\n} \n\n# .name will rename the new columns with 'twice` prefix combined with existing col names\npenguins %&gt;% group_by(species) %&gt;%\n  mutate(across(where(is.numeric), two_times, .names = \"two_times_{col}\"))\n\n\n\n  \n\n\n\nThe same code when used just with mutate() function will look like this\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# define the function\ntwo_times &lt;- function(x) {\n  2*x\n}\n\n# using only 'mutate()' function\npenguins %&gt;% group_by(species) %&gt;%\n  mutate(twice_bill_lenght = two_times(bill_length_mm),\n         twice_body_mass = two_times(body_mass_g),\n         .....)\n\nSo in this code, I will have to manually type all the col names and apply the operation individually which is too much of a hassle. Now we can better appreciate how efficient it is in using mutate() and across() functions together.\n\n6.8 c_across()\nThe function c_across() is similar to the earlier mentioned across() function. But instead of doing a column-wise function, it applies function across columns in a row-wise manner. Now, most functions in R by default computes across columns, so to specify row-wise computation, we have to explicitly use the function rowwise() in conjunction with other functions. In the example below we will sum both bill and flipper lengths of the penguins in the penguins dataset and create a new column called ‚Äòsum_of_lengths‚Äô.\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\npenguins %&gt;% drop_na() %&gt;%\n  group_by(species) %&gt;%\n  rowwise() %&gt;%\n  transmute(sum_of_length = sum(c_across(c(bill_length_mm,flipper_length_mm))))"
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#summary",
    "href": "tutorials/data_man/dplyr_1.html#summary",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n7 Summary",
    "text": "7 Summary\nThe dplyr package is the grammar of the data manipulation in R. It features well-made functions to help us summarise the data, group data by variables and manipulate columns and rows in the dataset. In this chapter, we learned in detail the different functions that help us manipulate data efficiently and have seen case examples also. In the next chapter, we will see the remaining set of functions in the dplyr package."
  },
  {
    "objectID": "tutorials/data_man/dplyr_1.html#references",
    "href": "tutorials/data_man/dplyr_1.html#references",
    "title": "Chapter 2: Data manipulation using dplyr (part 1)",
    "section": "\n8 References",
    "text": "8 References\n\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr Here is the link to the cheat sheet explaining each function in dplyr.\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nHadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html",
    "href": "tutorials/data_man/dplyr_2.html",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "",
    "text": "In the previous chapter we have seen quite a lot of functions from the dplyr package. In this chapter, we will see the rest of the functions where we learn how to handle row names, how to join columns and rows and different set operations in the dplyr package.\n\n# loading necessary packages\nlibrary(dplyr)\n\n\nTidy data does not use row names. So use rownames_to_column() command to convert row names to a new column to the data. The function column_to_rownames() does the exact opposite of rownames_to_column() as it converts a column into rownames but make sure that the column you are converting into rownames does not contain NA values.\n\n# mtcars dataset contains rownames\n# creates new column called car_names which contains row names\nmtcars %&gt;% rownames_to_column(var = \"car_names\")\n\n# returns the original mtcars dataset\nmtcars %&gt;% rownames_to_column(var = \"car_names\") %&gt;%\n  column_to_rownames(var = \"car_names\")"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#continuation-from-the-previous-chapter",
    "href": "tutorials/data_man/dplyr_2.html#continuation-from-the-previous-chapter",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "",
    "text": "In the previous chapter we have seen quite a lot of functions from the dplyr package. In this chapter, we will see the rest of the functions where we learn how to handle row names, how to join columns and rows and different set operations in the dplyr package.\n\n# loading necessary packages\nlibrary(dplyr)\n\n\nTidy data does not use row names. So use rownames_to_column() command to convert row names to a new column to the data. The function column_to_rownames() does the exact opposite of rownames_to_column() as it converts a column into rownames but make sure that the column you are converting into rownames does not contain NA values.\n\n# mtcars dataset contains rownames\n# creates new column called car_names which contains row names\nmtcars %&gt;% rownames_to_column(var = \"car_names\")\n\n# returns the original mtcars dataset\nmtcars %&gt;% rownames_to_column(var = \"car_names\") %&gt;%\n  column_to_rownames(var = \"car_names\")"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#combine-tablescolumns",
    "href": "tutorials/data_man/dplyr_2.html#combine-tablescolumns",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n2 Combine tables/columns",
    "text": "2 Combine tables/columns\n\n2.1 bind_cols()\nJoins columns with other columns. Similar function as that of cbind() from base R.\n\ndf1 &lt;- tidytable::data.table(x = letters[1:5], y = c(1:5))\ndf2 &lt;- tidytable::data.table(x = letters[3:7], y = c(6:10))\nbind_cols(df1,df2)\n\n#similar functionality\ncbind(df1,df2)\n\n\n2.2 bind_rows()\nJoins rows with other rows. Similar function as that of rbind() from base R.\n\ndf1 &lt;- tidytable::data.table(x = letters[1:5], y = c(1:5))\ndf2 &lt;- tidytable::data.table(x = letters[3:7], y = c(6:10))\nbind_rows(df1,df2)\n\n#similar functionality\nrbind(df1,df2)\n\nThe functions that are described below have the same functionality as that of bind_cols() but give you control over how the columns are joined."
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#mutating-joins-and-filtering-joins",
    "href": "tutorials/data_man/dplyr_2.html#mutating-joins-and-filtering-joins",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n3 Mutating joins and filtering joins",
    "text": "3 Mutating joins and filtering joins\nMutating joins include left_join(), right_join(), inner_join() and full_join() and filtering joins include semi_join() and anti_join().\n\n\nleft_join()\nright_join()\ninner_join()\nfull_join()\nanti_join()\nsemi_join()\n\n\n\nIn the code below, matching variables of df2 are joined with df1. In the final data, you can see that only kevin and sam from df2 are matched with df1, and only those row values are joined with df1. For those variables which didn‚Äôt get a match, the row values for those are filled with NA. You can interpret the variables with NA values as; both john and chris are not present in df2.\nIf you are familiar with set theory in mathematics, what we are doing essentially is similar to (df1 \\cap df2) \\cup df1.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %&gt;% left_join(df2)\n\n\n\n  \n\n\n\n\n\nSimilar to left_join() but here, you will be joining matching values from df1 to df2, the opposite of what we did earlier. As you can see only kevin and sam from the df1 is matched with df2, and only those row values are joined with df2. For the variables which didn‚Äôt get a match, the row values for those are filled with NA. You can interpret the variables with NA values as; bob is not present in df1.\nThis function, in the manner used here, is similar to (df1 \\cap df2) \\cup df2.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %&gt;% right_join(df2)\n\n\n\n  \n\n\n\n\n\nThe function inner_join() compares both df1 and df2 variables and only joins rows with the same variables. Here only kevin and sam are common in both the dataframes so the row values of only those columns are joined and others are omitted.\nThis function is similar to df1 \\cap df2.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %&gt;% inner_join(df2)\n\n\n\n  \n\n\n\n\n\nThe function full_join() compares both df1 and df2 variables and joins all possible matches while retaining both mistakes in df1 and df2 with NA values.\nThis function is similar to df1 \\cup df2.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %&gt;% full_join(df2)\n\n\n\n  \n\n\n\n\n\nThis is an example of filtering join. The function anti_join() compares df1 variables to and df2 variables and only outputs those variables of df1 which didn‚Äôt get a match with df2.\nThis function, in the manner used here, is similar to df1 \\cap df2^c.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\ndf1 %&gt;% anti_join(df2)\n\n\n\n  \n\n\n\n\n\nThis is an example of filtering join. The function semi_join() is similar to inner_join() but it only gives variables of df1 which has a match with df2.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\n\ndf1 %&gt;% semi_join(df2)\n\n\n\n  \n\n\n\n\n\n\nHere is a nice graphical representation of the functions we just described now. Image source.\n\n\n\n\n\n(a) Mutating joins\n\n\n\n\n\n\n(b) Filtering joins\n\n\n\n\nFigure¬†1: Graphical abstract for joins. Image source: RPubs.com"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#additional-commands-for-joins",
    "href": "tutorials/data_man/dplyr_2.html#additional-commands-for-joins",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n4 Additional commands for joins",
    "text": "4 Additional commands for joins\nAdditionally, you can specify which common columns to match.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\n\n# match with column 'x'\ndf1 %&gt;% left_join(df2, by = \"x\")\n\n\n\n  \n\n\ndf3 &lt;- tidytable::data.table(a = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"), y = 1:5)\ndf4 &lt;- tidytable::data.table(b = c(\"kevin\",\"sam\", \"bob\"), z = 10:12)\n\n# matching with column having different names, a and b in this case\ndf3 %&gt;% left_join(df4, by = c(\"a\" = \"b\"))"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#set-operations",
    "href": "tutorials/data_man/dplyr_2.html#set-operations",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n5 Set operations",
    "text": "5 Set operations\nSimilar to the mutating join functions that we had seen, there are different functions related to set theory operations.\n\n5.1 intersect()\nOutputs common rows in the dataset.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nintersect(df1, df2)\n\n\n\n  \n\n\n\n\n5.2 setdiff()\nOutputs rows in first data frame but not in second data frame.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nsetdiff(df1, df2)\n\n\n\n  \n\n\n\n\n5.3 union()\nOutputs all the rows in both dataframes\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nunion(df1, df2)\n\n\n\n  \n\n\n\n\n5.4 setequal()\nChecks whether two datasets have same number of rows.\n\nlibrary(dplyr)\n\ndf1 &lt;- tidytable::data.table(x = c(\"john\",\"kevin\",\"chris\",\"sam\",\"sam\"))\ndf2 &lt;- tidytable::data.table(x = c(\"kevin\",\"sam\", \"bob\"))\n\nsetequal(df1, df2)\n\n[1] FALSE"
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#summary",
    "href": "tutorials/data_man/dplyr_2.html#summary",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n6 Summary",
    "text": "6 Summary\nIn this chapter, we have seen;\n\n\nHow to handle row names\nHow to combine columns and rows\nWhat are mutating and filtering joins and various set operations\n\n\nThus to conclude this chapter, we have now learned almost all functions in the dplyr package and have seen how to manipulate data efficiently. With the knowledge of the pipe operator that we have seen in chapter 1, we are now equipped to write codes compactly and more clearly. I hope this chapter was useful for you and I will see you next time."
  },
  {
    "objectID": "tutorials/data_man/dplyr_2.html#references",
    "href": "tutorials/data_man/dplyr_2.html#references",
    "title": "Chapter 3: Data manipulation using dplyr (part 2)",
    "section": "\n7 References",
    "text": "7 References\n\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr. Here is the link to the cheat sheet explaining each and every function in dplyr."
  },
  {
    "objectID": "tutorials/data_man/tidyr.html",
    "href": "tutorials/data_man/tidyr.html",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "",
    "text": "Raw data might not be always in a usable form for any form of analysis or visualization process. The tidyr package aims to help you in reshaping your data in a usable form. In short, it helps you to ‚Äòtidy‚Äô up your data using various tools. In this chapter, we will see how you can use the tidyr package to make your data tidy."
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#introduction-to-tidyr-package",
    "href": "tutorials/data_man/tidyr.html#introduction-to-tidyr-package",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "",
    "text": "Raw data might not be always in a usable form for any form of analysis or visualization process. The tidyr package aims to help you in reshaping your data in a usable form. In short, it helps you to ‚Äòtidy‚Äô up your data using various tools. In this chapter, we will see how you can use the tidyr package to make your data tidy."
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#what-is-tidy-data",
    "href": "tutorials/data_man/tidyr.html#what-is-tidy-data",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n2 What is tidy data?",
    "text": "2 What is tidy data?\nFirst, we need to understand what tidy data looks like. For that let us imagine a scenario where you are a doctor who is trying to find the best treatment for a disease. Now your colleagues have short-listed five different treatment methods and have reported their efficacy values when tested with five different patients. Now you are tasked with finding which of the five treatments is the best against the disease. You open your computer and you find the following data of the experiment.\n\n\n\n\n  \n\n\n\nThis is how often data is stored because it is easy to write it this way. In the first column, you can see the different treatments from one to five. And in the second column, you have the efficacy values of the treatments for patient 1 and it goes on for the other patients. Now, this is a good example of how a dataset should not look like! Surprised? Let us see what makes this dataset ‚Äòdirty‚Äô.\nYou can quickly notice that there is no mentioning of what these numerical values mean. Of course, we know that they are efficacy values for the different treatments. But for someone who only has this data as a reference, that person would not have a clue as to what these numbers mean. Also, note that each of the rows contains multiple observation values which is not a feature of tidy data. This kind of data format is called ‚Äòwide data‚Äô which we will talk more about later.\nWith that being said, tidy data will have;\n\nEach of its variables represented in its own column\nEach observation or a case in its own row.\nEach of the rows will contain only a single value.\n\nSo let us see how the ‚Äòtidier‚Äô version of this data would look like.\n\n\n\n\n  \n\n\n\nYou can see each of the columns represent only one type of variable. In the first column, you have the types of treatments, followed by patient IDs and their efficacy values for each treatment. Also, note that each row represents only one observation. So this kind of data format is what we strive to achieve by using the tidyr package and they are called as ‚Äòlong data‚Äô. So let us begin!"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#reshaping-dataset",
    "href": "tutorials/data_man/tidyr.html#reshaping-dataset",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n3 Reshaping dataset",
    "text": "3 Reshaping dataset\nThere are different sets of commands which you can utilize to reshape your data and make it tidy. Let us each of these commands in action. But first, make sure you have the tidyr package loaded.\n\nCode# load tidyr package\nlibrary(tidyr)\n\n\n\n3.1 pivot_longer()\nThe pivot_longer() command converts a ‚Äòwide data‚Äô to a ‚Äòlong data‚Äô. It does so by converting row names to a new column under a new variable name with its corresponding values moved into another column with another variable name. So let us see how it goes. We will take the earlier mentioned example and will see how to make it tidy. Now you don‚Äôt have to be concerned with the codes I have used to make the dummy data. Just have your focus on the pivot_longer() syntax.\n\nCodelibrary(tidyr)\n# making a dummy data\n# using sample function to pick random numbers in a sequence\npatient1 &lt;- c(seq(1,5,1))\npatient2 &lt;- c(seq(6,10,1))\npatient3 &lt;- c(seq(11,15,1))\npatient4 &lt;- c(seq(16,20,1))\npatient5 &lt;- c(seq(21,25,1))\n\n# cbind simple combines the columns of same size\ntreatment_data &lt;- cbind(patient1,patient2,patient3,patient4,patient5) \n\ntrt &lt;- c(\"treatment1\", \"treatment2\",\"treatment3\",\"treatment4\",\"treatment5\")\n\ntrt_data &lt;- cbind(trt, treatment_data)\ntrt_data &lt;- as.data.frame(trt_data) # making it a data frame\n\ntrt_data_tidy &lt;- pivot_longer(trt_data,\n                              c(patient1,patient2,patient3,patient4,patient5), \n                              names_to = \"patient_ID\", values_to = \"efficacy\")\ntrt_data_tidy\n\n\n\n  \n\n\n\nFurthermore, you don‚Äôt have to manually type in the column names as you can use colnames() to call the column names of the dataset. Another way of doing the same is by excluding the first column from the process. By doing so the command will automatically pivot all columns except the excluded ones, so in this way, we don‚Äôt need to manually specify the column names. The codes given below will give you the same result as before.\n\nShow the codelibrary(tidyr)\npatient1 &lt;- c(seq(1,5,1))\npatient2 &lt;- c(seq(6,10,1))\npatient3 &lt;- c(seq(11,15,1))\npatient4 &lt;- c(seq(16,20,1))\npatient5 &lt;- c(seq(21,25,1))\ntreatment_data &lt;- cbind(patient1,patient2,patient3,patient4,patient5) \ntreatment &lt;- c(\"treatment1\", \"treatment2\",\"treatment3\",\"treatment4\",\"treatment5\")\ntrt_data &lt;- cbind(treatment, treatment_data)\ntrt_data &lt;- as.data.frame(trt_data)\n# using colnames, [-1] is included to exclude the name of first column from the process\ntrt_data_tidy1 &lt;- pivot_longer(trt_data,\n                              colnames(trt_data)[-1], \n                              names_to = \"patient_ID\", values_to = \"efficacy\")\n\n# the same can be done by manually specifying which columns to exclude\n# this can be done by denoting the column name ('treatment' in this case) with '-' sign\ntrt_data_tidy2 &lt;- pivot_longer(trt_data, names_to = \"patient_ID\",\n                               values_to = \"efficacy\", -treatment)\n# checking if both the tidy datasets are one and the same\ntrt_data_tidy1 == trt_data_tidy2\n\n\nThe syntax for pivot_longer() is given below with description\n\nCodepivot_longer(\"data\", c(\"colname1, colname2,.....\"), \n  names_to = \"name of the column where your row names are present\",\n  values_to = \"name of the column where your corresponding row values are present\")\n\n\nHere is a graphical representation\n\n\n3.2 pivot_wider()\nThe pivot_wider() does the exact opposite of what pivot_longer() does, which is to convert long data into wide data. We will use the previously given dummy data.\n\nCodelibrary(tidyr)\n# making a dummy data\n# using sample function to pick random numbers in a sequence\npatient1 &lt;- c(seq(1,5,1))\npatient2 &lt;- c(seq(6,10,1))\npatient3 &lt;- c(seq(11,15,1))\npatient4 &lt;- c(seq(16,20,1))\npatient5 &lt;- c(seq(21,25,1))\n\n# cbind simple combines the columns of same size\ntreatment_data &lt;- cbind(patient1,patient2,patient3,patient4,patient5) \n\ntrt &lt;- c(\"treatment1\", \"treatment2\",\"treatment3\",\"treatment4\",\"treatment5\")\n\ntrt_data &lt;- cbind(trt, treatment_data)\ntrt_data &lt;- as.data.frame(trt_data) # making it a data frame\n\ntrt_data_tidy &lt;- pivot_longer(trt_data,\n                              c(patient1,patient2,patient3,patient4,patient5), \n                              names_to = \"patient_ID\", values_to = \"efficacy\")\n\n# making the data wide\ntrt_data_wider &lt;- pivot_wider(trt_data_tidy, names_from = \"patient_ID\",\n                              values_from = \"efficacy\")\n\n# paged_Table() for viewing the dataset as a table, \n# you can see that the dataset is same as before\ntrt_data_wider\n\n\n\n  \n\n\n\nThe syntax for pivot_wider() is given below with description\n\nCodepivot_longer(\"data\", \n  names_from = \"name of the column which contains your wide data columns\",\n  values_from = \"name of the column where your corresponding wide data column values are\")\n\n\nHere is a graphical representation"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#splitting-and-uniting-cells",
    "href": "tutorials/data_man/tidyr.html#splitting-and-uniting-cells",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n4 Splitting and uniting cells",
    "text": "4 Splitting and uniting cells\nThere can be an instance where you want to split or untie cells within your dataset. Let us look at some examples.\n\n4.1 unite()\nIn the data given below, let say we want to unite the century column and the year column together. This can be done using the unite() command. You can view the before and after instances in the tabs below.\n\n\nBefore\nAfter\n\n\n\n\nShow the codeevent &lt;- c(letters[1:4])\ncentury &lt;- c(rep(19:20, each = 2))\nyear &lt;- c(seq(10,16,2))\ndata &lt;- as.data.frame(cbind(event,century,year))\n\ndata\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nevent &lt;- c(letters[1:4])\ncentury &lt;- c(rep(19:20, each = 2))\nyear &lt;- c(seq(10,16,2))\ndata &lt;- as.data.frame(cbind(event,century,year))\n\n# uniting columns century and year\ndata_new &lt;- unite(data, century, year, col = \"event_year\", sep = \"\")\n# viewing data as a table\ndata_new\n\n\n\n  \n\n\n\n\n\n\nThe syntax of unite() is as follows.\n\nCodeunite(\"dataset name\",\n      \"name of first column to unite, name of second column to unite,.......\",\n      col = \"name of the new column to which all the other column will unite together\",\n      sep = \"input any element as a separator between the joining column values\")\n# in this case we are not putting a sep value\n\n\n\n4.2 separate()\nIn the data given below, let say we want to split the ‚Äòarea_perimeter‚Äô column into two separate columns. This can be done using the separate() command. You can view the before and after instances in the tabs below. As always I will be making dummy data to work with.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nshapes &lt;- c(letters[1:4])\narea &lt;- c(paste0(10:13, \"m^2\"))\nperimetre &lt;- c(paste0(30:33, \"m\"))\nratio &lt;-as.data.frame(cbind(shapes,area,perimetre))\ndata &lt;- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n# viewing data as a table\ndata\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nshapes &lt;- c(letters[1:4])\narea &lt;- c(paste0(10:13, \"m^2\"))\nperimetre &lt;- c(paste0(30:33, \"m\"))\nratio &lt;-as.data.frame(cbind(shapes,area,perimetre))\ndata &lt;- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n\n# separating column values into two separate columns named area and perimeter respectively\ndata_new &lt;- separate(data, area_perimetre, sep = \"_\",\n                     into = c(\"area\", \"perimetre\"))\n# viewing data as a table\ndata_new\n\n\n\n  \n\n\n\n\n\n\nThe syntax of separate() is as follows.\n\nCodeseparate(\"data name\",\n         \"column to separate into\",\n         sep = \"the separator element\",\n         into = c(\"col1\", \"col2\", \"........\")) # column names for the separated values\n\n\n\n4.3 separate_rows()\nSimilar to the above case, you can also separate column values into several rows.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nshapes &lt;- c(letters[1:4])\narea &lt;- c(paste0(10:13, \"m^2\"))\nperimetre &lt;- c(paste0(30:33, \"m\"))\nratio &lt;-as.data.frame(cbind(shapes,area,perimetre))\ndata &lt;- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n# viewing data as a table\ndata\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nshapes &lt;- c(letters[1:4])\narea &lt;- c(paste0(10:13, \"m^2\"))\nperimetre &lt;- c(paste0(30:33, \"m\"))\nratio &lt;-as.data.frame(cbind(shapes,area,perimetre))\ndata &lt;- unite(ratio, area, perimetre, col = \"area_perimetre\", sep = \"_\")\n\n# separating column values into two several rows\ndata_new &lt;- separate_rows(data, area_perimetre, sep = \"_\")\n# viewing data as a table\ndata_new\n\n\n\n  \n\n\n\n\n\n\nThe syntax of separate_rows() is as follows.\n\nCodeseparate_rows(\"data name\",\n         \"column to separate\",\n         sep = \"the separator element\")"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#expanding-and-completing-dataset",
    "href": "tutorials/data_man/tidyr.html#expanding-and-completing-dataset",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n5 Expanding and completing dataset",
    "text": "5 Expanding and completing dataset\nYou can expand your data to include all possible combinations of values of variables listed or complete the dataset with NA values for all possible combinations.\n\n5.1 expand()\nUsing the expand() command we can expand our data with missing combinations for the variables we specify.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nbrand &lt;- c(letters[1:4])\ndress &lt;- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize &lt;- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data &lt;- as.data.frame(cbind(brand,dress,size))\n\n# viewing data as a table\ndress_data\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand &lt;- c(letters[1:4])\ndress &lt;- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize &lt;- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data &lt;- as.data.frame(cbind(brand,dress,size))\n\n# expanding dataset with brand and dress as variables\ndress_data_expand &lt;- expand(dress_data, brand, dress)\n\n# viewing data as a table\ndress_data_expand\n\n\n\n  \n\n\n\n\n\n\nThe syntax of expand() is as follows.\n\nCodeexpand(\"data name\", \"column names which you want to expand separated by commas\")\n\n\n\n5.2 complete()\nThe complete() command functions similar to the expand() command, but it also fills in NA values for columns which we didn‚Äôt specify, The main reason to use this command would be to convert implicit NA values hidden in the dataset to explicit NA values which are expressed in the dataset. Given below is a comparison between the complete() and expand() commands.\n\n\nexpand()\ncomplete()\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand &lt;- c(letters[1:4])\ndress &lt;- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize &lt;- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data &lt;- as.data.frame(cbind(brand,dress,size))\n\n# expanding dataset with brand and dress as variables\ndress_data_expand &lt;- expand(dress_data, brand, dress)\n\n# viewing data as a table\ndress_data_expand\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand &lt;- c(letters[1:4])\ndress &lt;- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize &lt;- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data &lt;- as.data.frame(cbind(brand,dress,size))\n\n# completing dataset with brand and dress as variables\n# the variable 'size' will be filled with NAs as we did not specify it\ndress_data_complete &lt;- complete(dress_data,brand,dress)\n\n# viewing data as a table\ndress_data_complete\n\n\n\n  \n\n\n\n\n\n\nThe syntax of complete() is as follows.\n\nCodecomplete(\"data name\", \"column names which you want to complete separated by commas\")"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#handling-nas-or-missing-values",
    "href": "tutorials/data_man/tidyr.html#handling-nas-or-missing-values",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n6 Handling NAs or missing values",
    "text": "6 Handling NAs or missing values\nMost data collection would often result in possible NA values. The tidyr package allows us to drop or convert NA values. We will reuse the earlier example. Below tabs show before and removing NA values.\n\n6.1 drop_na()\nUse drop_na() to remove NA value containing rows from the dataset.\n\n\nBefore\nAfter\n\n\n\n\nShow the code# dummy data\nbrand &lt;- c(letters[1:4])\ndress &lt;- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize &lt;- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data &lt;- as.data.frame(cbind(brand,dress,size))\n\ndress_data_complete &lt;- complete(dress_data,brand,dress)\n\n# viewing data as a table\ndress_data_complete\n\n\n\n  \n\n\n\n\n\n\nShow the codelibrary(tidyr)\n# dummy data\nbrand &lt;- c(letters[1:4])\ndress &lt;- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize &lt;- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data &lt;- as.data.frame(cbind(brand,dress,size))\n\ndress_data_complete &lt;- complete(dress_data,brand,dress)\n\n# dropping NA values\n\ndress_data_noNA &lt;- drop_na(dress_data_complete)\n\n# viewing data as a table\ndress_data_noNA\n\n\n\n  \n\n\n\n\n\n\n\n6.2 fill()\nUse fill() to replace NA values by taking values from nearby cells. By default the NA values as replaced by whatever value that is above the cell containing the NA value. This can be changed by specifying the .direction value within fill()\n\nCodelibrary(tidyr)\n# dummy data\nbrand &lt;- c(letters[1:4])\ndress &lt;- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize &lt;- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data &lt;- as.data.frame(cbind(brand,dress,size))\ndress_data_complete &lt;- complete(dress_data,brand,dress)\n\n# direction 'downup' simultaneously fill both upwards and downwards NA containing cells\ndress_data_fill &lt;- fill(dress_data_complete, size, .direction = \"downup\")\n\n# viewing data as a table\ndress_data_fill\n\n\n\n  \n\n\n\n\n6.3 replace_na()\nUse replace_na() command to replace NA values to whatever value specified.\n\nCodelibrary(tidyr)\n# dummy data\nbrand &lt;- c(letters[1:4])\ndress &lt;- c(\"shirt\", \"pant\", \"jeans\", \"trousers\")\nsize &lt;- c(\"s\", \"m\", \"l\", \"xl\")\ndress_data &lt;- as.data.frame(cbind(brand,dress,size))\ndress_data_complete &lt;- complete(dress_data,brand,dress)\n\n# replace NA to unknown\n# specify the column which have NA inside the list()\n# then equate the value which would replace NAs\ndress_data_zero &lt;- replace_na(dress_data_complete, list(size = \"unknown\"))\n\n# viewing data as a table\ndress_data_zero"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#summary",
    "href": "tutorials/data_man/tidyr.html#summary",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n7 Summary",
    "text": "7 Summary\nSo in this chapter, we learned what is tidy data and how we can make our data into tidy data. Making our data tidy is very important as it helps us to analyse and visualise the data in a very efficient manner. We also learned how to reshape our data, how to split or unite cells, how to complete and expand data and how to handle NA values. Hope this chapter was fruitful for you!"
  },
  {
    "objectID": "tutorials/data_man/tidyr.html#references",
    "href": "tutorials/data_man/tidyr.html#references",
    "title": "Chapter 1: Data tidying using tidyr",
    "section": "\n8 References",
    "text": "8 References\n\nHadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html",
    "href": "tutorials/data_viz/ggplot_1.html",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "",
    "text": "This series of chapters are focused on people who have a basic understanding of R programming. You are expected to know how the basic syntaxes of R language work like how to assign values, how to load your data, the different operations of R, and so on. If you just started out on your R journey then this chapter and the rest will become a roadblock for you. But fear not, there are some amazing websites that teach you the very basics of R, and that too for free! For an interactive way of learning, I recommend DataCamp. It is an online platform for learning programming languages. They have both paid and free classes. Luckily for us, they are providing the introductory classes on R programming for free. This would be a great way to start your R journey. After you completed the course on data camp you can come back to this blog and you will find it very easy to comprehend and learn the various chapters that are available here. Coming from my own experience, it would be really helpful if you have your own data to work with. Rather than religiously following the steps in these chapters, I would recommend you have a goal in your mind before diving into the chapters. The goal should be to try incorporating the things you learned here into your own data. That would be the best way to learn anything in R. Hope you have a great time learning!"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#before-we-begin",
    "href": "tutorials/data_viz/ggplot_1.html#before-we-begin",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "",
    "text": "This series of chapters are focused on people who have a basic understanding of R programming. You are expected to know how the basic syntaxes of R language work like how to assign values, how to load your data, the different operations of R, and so on. If you just started out on your R journey then this chapter and the rest will become a roadblock for you. But fear not, there are some amazing websites that teach you the very basics of R, and that too for free! For an interactive way of learning, I recommend DataCamp. It is an online platform for learning programming languages. They have both paid and free classes. Luckily for us, they are providing the introductory classes on R programming for free. This would be a great way to start your R journey. After you completed the course on data camp you can come back to this blog and you will find it very easy to comprehend and learn the various chapters that are available here. Coming from my own experience, it would be really helpful if you have your own data to work with. Rather than religiously following the steps in these chapters, I would recommend you have a goal in your mind before diving into the chapters. The goal should be to try incorporating the things you learned here into your own data. That would be the best way to learn anything in R. Hope you have a great time learning!"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#introduction-to-ggplot2-package",
    "href": "tutorials/data_viz/ggplot_1.html#introduction-to-ggplot2-package",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n2 Introduction to ggplot2 package",
    "text": "2 Introduction to ggplot2 package\nIn this chapter we will be plotting different types of graphs using a package called ggplot2 in R. The ggplot2 package is based on ‚Äògrammar of graphics plot‚Äô which provides a systematic way of doing data visualizations in R. With a few lines of code you can plot a simple graph and by adding more layers onto it you can create complex yet elegant data visualizations.\nA ggplot2 graph is made up of three components.\n\n\nData: Data of your choice that you want to visually summarise.\n\nGeometry or geoms: Geometry dictates the type of graph that you want to plot and this information is conveyed to ggplot2 through the geom() command code. For e.g.¬†using the geom_boxplot() command, you can plot a box plot with your data. Likewise, there are many types of geometry that you can plot using the ggplot2 package.\n\nAesthetic mappings: Aesthetics define the different kinds of information that you want to include in the plot. One fo the most important aesthetic is in choosing which data values to plot on the x-axis and the y-axis. Another example is changing the colour of the data points, which can be used to differentiate two different categories in the data. The use of aesthetics depends on the geometry that you are using. We use the command aes() for adding different types of aesthetics to the plot. We will learn more about aes() in Chapter 2. For now, we will only see what kind of plots can be made using the ggplot2 package. We will learn how to tweak them in Chapter 2.\n\nThis tutorial is primarily focused on students who are beginners in R programming and wants to quickly plot their data without much of a hassle. So without further ado let‚Äôs plot some graphs!"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#setting-up-the-prerequisites",
    "href": "tutorials/data_viz/ggplot_1.html#setting-up-the-prerequisites",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n3 Setting up the prerequisites",
    "text": "3 Setting up the prerequisites\nFirst, we need to install the ggplot2 package in R as it does not come in the standard distribution of R.\n\nTo install packages in R we use the command install.packages() and to load packages we use the command library(). Therefore to install and load ggplot2 package we use the following lines of command.\n\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nAll right we have the ggplot2 package loaded, now we just need some data to plot. Most R programming tutorials use the iris dataset as an example. But this tutorial won‚Äôt be like most tutorials. So let me introduce you to some lovely penguins from Palmer Station in Antarctica!\nFor this tutorial, we will be installing the palmerpenguins package which showcases body measurements taken from three different species of penguins from Antarctica. This package was made possible by the efforts of Dr.¬†Allison Horst. The penguin data was collected and made available by Dr.¬†Kristen Gorman and the Palmer Station, Antarctica LTER.\n\nInstall the palmerpenguins package and load it in R.\n\n\ninstall.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\nNow there are two datasets in this package. We will be using the penguins dataset which is a simplified version of the raw data present in the package.\n\nUse the command head() to display the first few values of penguins dataset to see how it looks like\n\n\nlibrary(palmerpenguins)\nhead(penguins)\n\n\n\n  \n\n\n\nWe can see that are 8 columns in the dataset representing different values. Now let us try plotting some graphs with this data.\n\n3.1 Bar graph\nSo we will try to plot a simple bar graph first. Bar graphs are used to represent categorical data where the height of the rectangular bar represents the value for that category. We will plot a bargraph representing frequency data for all three species of penguins.\n\nWe will be using the geom_bar() command to plot the bar graph. Let us also use the command theme_bw() for a nice looking theme.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, fill = species)) + \n  xlab(\"Species\") + ylab(\"Frequency\") + \n  ggtitle(\"Frequency of individuals for each species\") + \n  geom_bar() + theme_bw()\n\n\n\n\n\n3.2 Histogram\nHistograms are similar to bar graphs visually and are used to represent continuous data. Histograms splits data into ‚Äòbins‚Äô and tells us the number of observations for each of the bin.\n\nWe can plot a histogram using the command geom_histogram() and change the bin width using the binwidth = argument.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = body_mass_g, fill = species)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Frequency\") + \n  ggtitle(\"Frequency of individuals for respective body mass\") + \n  geom_histogram(binwidth = 1000) + theme_bw()\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nThe warning message indicates that for two rows in the dataset, they have NA values or that they did not have any values present. This is true for real-life cases, as during data collection sometimes you will be unable to collect data due to various reasons. So this is perfectly fine.\n\n3.3 Line graph\nLine graph simply joins together data points to show overall distribution.\n\nUse the command geom_line() for plotting a line graph.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = bill_length_mm, \n                            y = bill_depth_mm, colour = species)) + \n  xlab(\"Bill length (mm)\") + ylab(\"Bill depth (mm)\") + \n  ggtitle(\"Bill length vs Bill depth\") + geom_line() + theme_bw()\n\n\n\n\n\n3.4 Scatter plot\nThe scatter plot simply denotes the data points in the dataset.\n\nUse the command geom_point() to plot a scatter plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm, \n                            shape = species, colour = species)) + \n  xlab(\"Body mass (g)\") + ylab(\"Flipper length (mm)\") + \n  ggtitle(\"Body mass vs Filpper length\") + geom_point(size = 2) + theme_bw()\n\n\n\n\n\n3.5 Density Plot\nDensity plots are similar to histograms but show it shows the overall distribution of the data in a finer way. This way we will get a bell-shaped curve if our data follows a normal distribution.\n\nUse the command geom_density() to a density plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = body_mass_g, fill = species)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density() + theme_bw()\n\n\n\n\nSince we plotted for all three species the graph looks clustered. Let us try plotting the same graph for only gentoo penguins. We will use the dplyr package to filter() data for gentoo penguins alone. The dplyr package comes in-built with R so just load the dplyr package using the command library().\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\npenguins_gentoo &lt;- penguins %&gt;% filter(species == \"Gentoo\")\n\nggplot(data = penguins_gentoo, aes(x = body_mass_g)) + \n  xlab(\"Body Mass of Gentoo penguins (g)\") + ylab(\"Density\") + \n  ggtitle(\"Body mass distribution of Gentoo penguins\") + \n  geom_density(fill = \"red\") + theme_bw()\n\n\n\n\n\n3.6 Dot-plot\nDot-plot is similar to a density plot but it shows discretely each data point in the distribution.\n\nUse the command geom_dotplot() to plot a dot-plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + \n  geom_dotplot(binaxis = \"y\", stackdir = \"center\", binwidth = 100) + theme_bw()\n\n\n\n\n\n3.7 Rug-plot\nRug-plot is a simple way to visualize the distribution of data along the axis lines. It is often used in conjunction with other graphical representations.\n\nUse the command geom_rug() to plot a rug-plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\npenguins_gentoo &lt;- penguins %&gt;% filter(species == \"Gentoo\")\n\nggplot(data = penguins_gentoo, aes(x = body_mass_g, y = flipper_length_mm)) + \n  xlab(\"Body Mass of Gentoo penguins (g)\") + ylab(\"Density\") + \n  ggtitle(\"Body mass distribution of Gentoo penguins\") + \n  geom_point(colour = \"darkred\") + geom_rug() + theme_bw()\n\n\n\n\n\n3.8 Box plot\nBox-plot is one of the better ways of showing data via quartiles. You can learn more about box plots here.\n\nUse the command geom_boxplot() to plot a box-plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, colour = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + geom_boxplot() + \n  theme_bw()\n\n\n\n\n\n3.9 Violin plot\nViolin plot can be considered as the best of both a box-plot and a density plot. It shows the quartile values, like in a box-plot and also shows the distribution of the data, like in a density plot.\n\nUse the command geom_violin() in conjunction with geom_boxplot() to plot a violin plot.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + \n  geom_violin(aes(colour = species), trim = TRUE) + geom_boxplot(width = 0.2) +\n  theme_bw()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#saving-your-ggplot2-graphs",
    "href": "tutorials/data_viz/ggplot_1.html#saving-your-ggplot2-graphs",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n4 Saving your ggplot2 graphs",
    "text": "4 Saving your ggplot2 graphs\n\nUse the command ggsave() to save the graph locally. In the code below, ‚Äòmy_graph‚Äô is the ggplot element containing your graph. The plot will be saved in your working directory.\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\nmy_graph &lt;- ggplot(data = penguins, aes(x = species, y = body_mass_g,\n                                    fill = species)) + \n  xlab(\"Species\") + ylab(\"Body mass (g)\") + \n  ggtitle(\"Body mass in three diferent species of penguins\") + \n  geom_violin(aes(colour = species), trim = TRUE) + \n  geom_boxplot(width = 0.2) +\n  theme_bw()\n\n#to save the plot\nggsave(my_graph, filename = \"your_graph_name.png\", width = 20, height = 20,\n       units = \"cm\")"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#summary",
    "href": "tutorials/data_viz/ggplot_1.html#summary",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n5 Summary",
    "text": "5 Summary\nI hope this tutorial helped you to get familiarize with the ggplot2 commands. The best way to learn R is by actually doing it yourself. So try to recreate the examples given in this tutorial and then try to apply what you learned using the different datasets available in R. In chapter 2, we will learn how to customize the plots by tweaking the aesthetic mappings."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_1.html#references",
    "href": "tutorials/data_viz/ggplot_1.html#references",
    "title": "Chapter 1: Data visualization using ggplot2",
    "section": "\n6 References",
    "text": "6 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Read more about ggplot2 here. You can also look at the cheat sheet for all the syntax used in ggplot2. Also check this out.\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html",
    "href": "tutorials/data_viz/ggplot_2.html",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "",
    "text": "After going through Chapter 1 you would be now familiar with the different types of graphs that you can plot using ggplot2. So for this tutorial, we will be learning how to customize those ggplot graphs to our liking. We will learn how to tweak the aesthetics, how to change labels and how to modify and change the axes in a graph.\nSo let us plot a graph from scratch and learn how to use different aesthetics available."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#setting-up-the-prerequisites",
    "href": "tutorials/data_viz/ggplot_2.html#setting-up-the-prerequisites",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n1 Setting up the prerequisites",
    "text": "1 Setting up the prerequisites\nFirst, we need to install the ggplot2 package in R as it does not come in the standard distribution of R. For the dataset, we will first download the Stat2Data package which houses a lot of cool datasets. For this tutorial let us use the Hawks dataset which showcases body measurements from three different species of Hawks. This data was collected by students and faculty at Cornell College in Mount Vernon and the dataset was made available by late Prof.¬†Bob Black at Cornell College.\n\nTo install packages in R we use the command install.packages() and to load packages we use the command library(). Therefore to install and load ggplot2 and {Stats2Data} packages we use the following lines of command. Call the Hawks data using the data() command.\n\n\n# Installng packages\ninstall.packages(\"ggplot2\")\ndevtools::install_github(\"statmanrobin/Stat2Data\")\n\n# Loading required packages\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\n# Loading the Hawks dataset\ndata(\"Hawks\")\n\n\nLet us look at how the dataset is structured. Use str() command\n\n\nlibrary(Stat2Data)\ndata(\"Hawks\")\n\nstr(Hawks)\n\n'data.frame':   908 obs. of  19 variables:\n $ Month       : int  9 9 9 9 9 9 9 9 9 9 ...\n $ Day         : int  19 22 23 23 27 28 28 29 29 30 ...\n $ Year        : int  1992 1992 1992 1992 1992 1992 1992 1992 1992 1992 ...\n $ CaptureTime : Factor w/ 308 levels \" \",\"1:15\",\"1:31\",..: 181 25 138 42 62 71 181 88 261 192 ...\n $ ReleaseTime : Factor w/ 60 levels \"\",\" \",\"10:20\",..: 1 2 2 2 2 2 2 2 2 2 ...\n $ BandNumber  : Factor w/ 907 levels \" \",\"1142-09240\",..: 856 857 858 809 437 280 859 860 861 281 ...\n $ Species     : Factor w/ 3 levels \"CH\",\"RT\",\"SS\": 2 2 2 1 3 2 2 2 2 2 ...\n $ Age         : Factor w/ 2 levels \"A\",\"I\": 2 2 2 2 2 2 2 1 1 2 ...\n $ Sex         : Factor w/ 3 levels \"\",\"F\",\"M\": 1 1 1 2 2 1 1 1 1 1 ...\n $ Wing        : num  385 376 381 265 205 412 370 375 412 405 ...\n $ Weight      : int  920 930 990 470 170 1090 960 855 1210 1120 ...\n $ Culmen      : num  25.7 NA 26.7 18.7 12.5 28.5 25.3 27.2 29.3 26 ...\n $ Hallux      : num  30.1 NA 31.3 23.5 14.3 32.2 30.1 30 31.3 30.2 ...\n $ Tail        : int  219 221 235 220 157 230 212 243 210 238 ...\n $ StandardTail: int  NA NA NA NA NA NA NA NA NA NA ...\n $ Tarsus      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ WingPitFat  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ KeelFat     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Crop        : num  NA NA NA NA NA NA NA NA NA NA ...\n\n\nSo there is a lot of information in the dataset which we can use for plotting. So let us try plotting them."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#building-a-plot",
    "href": "tutorials/data_viz/ggplot_2.html#building-a-plot",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n2 Building a plot",
    "text": "2 Building a plot\nOne thing to remember here is that how ggplot2 builds a graph is by adding layers. Let us start by plotting the basic layer first where the x-axis shows ‚Äòweight of the hawks‚Äô and the y-axis shows ‚Äòwingspan of the hawks‚Äô.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n\nggplot(data = Hawks, aes(x = Weight, y = Wing))\n\n\n\n\nWait a sec! Where are my data points? So right now if we look at the syntax of the ggplot code we can see that we have not told ggplot2 which geometry we want. Do we want a scatter plot or a histogram or any other type of graph? So let us plot a scatter plot first. Use geom_point() command. By adding geom_point() to the ggplot() command is equivalent to adding an extra layer to the already existing layer that we got previously. Let us also use theme_bw() for a nice looking theme.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + geom_point() + \n  theme_bw()\n\n\n\n\nWe got the graph! but we also got a warning message. The warning message tells us that the dataset which we had used to plot the graph had 11 rows of NA values and which could not be plotted into the graph. In real-life cases, we can have datasets with NA values due to various reasons, so this is fine.\nNow, this graph even though shows us data points we are not sure which point belongs to which species, as this dataset contains data for three species of Hawks. So let us try giving different colours to the points concerning the different species so that we are able to differentiate them.\n\n2.1 Changing colour\n\nTo change colour of the ‚Äòelement‚Äô as a function species, we have to add colour = Species within the aes() of the ggplot command. I use the general term ‚Äòelement‚Äô here to emphasize that the same change in aesthetics will work for most of other types of geometries in ggplot2 (something which you have seen extensively in Chapter 1. Like for a line graph, the ‚Äòelement‚Äô would be lines. Here we have a scatter plot, so the ‚Äòelement‚Äô would be points.\n\nAlso note that, in addition to colour, R also recognizes color and col wordings and they function the same as colour.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw()\n\n\n\n\nThe species abbreviations are the following: CH=Cooper‚Äôs, RT=Red-tailed, SS=Sharp-Shinned.\nNow, this graph is way better than the previous one.\n\n2.2 Changing point shape.\n\nNow instead of the colour let us change the shape of the point. Use shape() command in aes()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, shape = Species)) + # instead of colour use shape.\n  geom_point() + theme_bw() \n\n\n\n\nNow we did change the shape of points but it is still hard to make out the difference. Let us try specifying colour along with the shape\n\nAdding both colour and shape in aesthetics\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, \n  colour = Species, shape = Species)) + geom_point() + theme_bw()\n\n\n\n\nThis plot is much better than the previous one.\nNow let us try specifying colour within the aes() of the geom()\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, shape = Species)) + \n  geom_point(aes(colour = Species)) + theme_bw()\n\n\n\n\nWe got the same graph as before! So what is the difference in specifying colour within aes() of ggplot() compared to the same but within geom_point(). Here Let us look at another example.\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = \"red\")) + \n  geom_point() + theme_bw()\n\n\n\n\nI manually changed the colour of the points to red colour. Please not that you can also use hex codes to specify the colour attribute. Now let try specifying colour to the aes() within the geom()\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = \"red\")) + \n  geom_point(aes(colour = Species)) + theme_bw()\n\n\n\n\nYou can see that the red colour is overridden by other colours. So the aes() mapping (in this case colour) within geom_point() will override any aes() mapping within ggplot(). And whatever aes() mapping we give within ggplot() will be inherited by all other geom layers that are specified.\nLet us see another case.\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_point(colour = \"darkred\") + theme_bw()\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_point(aes(colour = \"darkred\")) + theme_bw()\n\n\n\n\nIf you compare both the codes, the only difference is that the colour = \"darkred\" command was outside aes() in the first code and inside aes() in the second code. So why didn‚Äôt the second graph have the same dark-red coloured points as the first one? The reason is that in the first code we are explicitly told to have all data points to be coloured dark-red but that is not the case with the second code. In the second code, since we have specified it inside aes(), ggplot is trying to look for a variable called ‚Äúdarkred‚Äù inside the dataset and colour it accordingly. This is why the legend that appears in the second graph has listed ‚Äúdarkred‚Äù as a category. And ggplot fails to find the variable called ‚Äúdarkred‚Äù but it still recognizes the colour command line and colour all the points in red. So the bottom line is that R has a pre-determined way of reading a code, so we users should well-understand what each line is expected to do and should not expect R to just fill it in accordingly to what we write.\nNow let us try a few other examples;\n\n2.3 Changing size\n\nUse size() in aes(). The shape aesthetic works best if the input variable is categorical.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\nggplot(data = Hawks, aes(x = Species, y = Hallux, size = Culmen)) + \n  geom_point() + theme_bw()\n\n\n\n\n\n2.4 Changing colour, shape and size manually\n\nUse scale_shape_manual() for changing shape, similarly scale_color_manual() for changing colour and scale_size_manual() for changing size of the element.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Hallux, colour = Species,\n                         shape = Species, size = Species)) + \n  geom_point() +\n  scale_shape_manual(values=c(1, 2, 3)) +\n  scale_color_manual(values=c('red','blue', 'green')) +\n  scale_size_manual(values=c(1,5,10)) + theme_bw()\n\n\n\n\n\n\n\n\n\n\n2.5 Changing the opcaity of the elements\n\nUse alpha() within the geom() with a numeric value to change the opacity of the elements. This is useful for visualizing large datasets such as this.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point(alpha = 1/5) + theme_bw()\n\n\n\n\nThe same commands also work for most of the other types of geom(). Now let us see a few other aesthetics in other types of geoms.\n\n2.6 Changing fill colour\n\nUse fill() in aes()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, fill = Species)) +\n  geom_histogram(bins = 25) + theme_bw()\n\n\n\n\n\nUse scale_fill_manual() to manually change the colours.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, fill = Species)) +\n  geom_histogram(bins = 25) + theme_bw() + \n  scale_fill_manual(values = c(\"darkred\", \"darkblue\", \"darkgreen\"))\n\n\n\n\n\n2.7 Changing line type\n\nUse linetype in aes()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, linetype = Species)) + \n  geom_line() + theme_bw()\n\n\n\n\n\nYou can manually change line types using scale_linetype_manual()\n\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, linetype = Species)) + \n  geom_line() + \n  scale_linetype_manual(values= c(\"twodash\", \"longdash\", \"dotdash\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNow let us also see how to change the labels in a graph.\n\n2.8 Viewing datapoints as labels\n\nYou can plot data points as their values or as their labels using the geom_text() function.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n# Plotting the values\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_text(aes(label = Wing)) + theme_bw()\n\n\n\n# Plotting the values according to species text label\nggplot(data = Hawks, aes(x = Weight, y = Wing)) + \n  geom_text(aes(label = Species)) + theme_bw()\n\n\n\n\n\n2.9 Changing labels in the axes\n\nUse xlab() to change x-axis title, ylab() to change y-axis title, ggtitle() with label and subtitle to add title and subtitle respectively.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + xlab(\"Weight (gm)\") + ylab(\"Wing (mm)\") +\n  ggtitle(label = \"Weight vs Wing span in three different species of Hawks\", \n          subtitle = \"CH=Cooper's, RT=Red-tailed, SS=Sharp-Shinned\")\n\n\n\n\n\nThe same result can be obtained by using labs() to specify each label in the graph. For renaming the legend title, the command will depend on what is there within the aes() or in other words what is the legend based on.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + labs(x = \"Weight (gm)\", y = \"Wing (mm)\", \n  title= \"Weight vs Wing span in three different species of Hawks\", \n  subtitle = \"CH=Cooper's, RT=Red-tailed, SS=Sharp-Shinned\",\n  caption = \"Source: Hawk dataset from Stat2Data r-package\", #caption for the graph\n  colour = \"Hawk Species\", # rename legend title\n  tag = \"A\") #figure tag\n\n\n\n\n\n2.10 Tweaking the axes\n\nUse xlim() and ylim() for limiting x and y axes respectively.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + xlim(c(0,1000)) + ylim(c(200,350))\n\n\n\n\n\nUse coord_cartesian() to zoom in on a particular area in the graph\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + coord_cartesian(xlim = c(0,1000),\n                                                   ylim = c(200,350))\n\n\n\n\n\nUse coord_flip() to flip the x and y axes.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + coord_flip()\n\n\n\n\n\nUse scale_x_continuous() for tweaking the x-axis. The same command work for the y-axis also. You can include label() inside the command to manually label the breaks of the axes.\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + scale_x_continuous(breaks = c(0,1000,2000))\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + \n  scale_x_continuous(breaks = c(0,1000,2000),label = c(\"low\", \"medium\", \"high\"))\n\n\n\n\n\nUse scale_y_reverse() to display the y values in the descending order. Same command applies to x-axis also.\n\n\nlibrary(ggplot2)\nlibrary(Stat2Data)\n\nggplot(data = Hawks, aes(x = Weight, y = Wing, colour = Species)) +\n  geom_point() + theme_bw() + scale_y_reverse()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#typical-aesthetic-mappings",
    "href": "tutorials/data_viz/ggplot_2.html#typical-aesthetic-mappings",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n3 Typical aesthetic mappings",
    "text": "3 Typical aesthetic mappings\n\nTable of aesthetic mappings\n\nAesthetic\nDescription\n\n\n\nx\nX axis position\n\n\ny\nY axis position\n\n\nfill\nFill colour\n\n\ncolor\nColour points, outlines of other geoms\n\n\nsize\nArea or radius of points, thickness of the lines\n\n\nalpha\nTransparency\n\n\nlinetype\nLine dash pattern\n\n\nlabels\nText on a plot or axes\n\n\nshape\nShape\n\n\n\nWe are now familiar with all these different aesthetic mappings."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#summary",
    "href": "tutorials/data_viz/ggplot_2.html#summary",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n4 Summary",
    "text": "4 Summary\nIn this tutorial, we learned how to modify aesthetic present for different geoms in ggplot2 Then we learned how to modify labels in a graph and finally, we learned how to modify and change the axes elements. This tutorial is in no way exhaustive of the different ways you can modify a graph as there many more methods which are not discussed here. Instead of trying to include everything, this tutorial tries to be a stepping stone to help students of R to learn the basics of tweaking a graph. Try to practice what is covered here using other datasets available in the r-package Stat2Data."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_2.html#references",
    "href": "tutorials/data_viz/ggplot_2.html#references",
    "title": "Chapter 2: Customizing graphs in ggplot2",
    "section": "\n5 References",
    "text": "5 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Read more about ggplot2 here. You can also look at the cheat sheet for all the syntax used in ggplot2. Also check this out.\nAnn Cannon, George Cobb, Bradley Hartlaub, Julie Legler, Robin Lock, Thomas Moore, Allan Rossman and Jeffrey Witmer (2019). Stat2Data: Datasets for Stat2. R package version 2.0.0. https://CRAN.R-project.org/package=Stat2Data"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html",
    "href": "tutorials/data_viz/ggplot_3.html",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "",
    "text": "In this chapter, we will learn how to change the theme settings of a graph in ggplot2 The theme of a graph consists of non-data components present in your graph. This includes the different labels of the graph, fonts used, colour of axes, the background of the graph etc. By changing the theme we would not be changing or transforming how the data will look in the graph. Instead, we would only change the visual appearances in the graph and by doing so we can make it more aesthetically pleasing. Furthermore, we will see a few popular packages featuring ready to use themes. We will also learn about colour palettes and will see different packages associated with them."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#complete-themes",
    "href": "tutorials/data_viz/ggplot_3.html#complete-themes",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n1 Complete themes",
    "text": "1 Complete themes\nThe ggplot2 package features ready to use themes called ‚Äòcomplete themes‚Äô. So before we begin customizing themes, let us plot a few graphs and see how these themes look like. For the plots, I have used the BirdNest dataset from the Stat2Data package in R. The BirdNest dataset contains nest and species characteristics of North American passerines. The data was collected by Amy R. Moore, as a student at Grinnell College in 1999.\n\nGetting the BirdNest dataset and viewing how the dataset is structured.\n\n\ninstall.packages(\"Stat2Data\") # for installing Stat2Data package\ninstall.packages(\"ggplot2\") # for installing ggplot2 package\n\n# load the packages\nlibrary(Stat2Data)\nlibrary(ggplot2)\n\ndata(\"BirdNest\") # loading the BirdNest dataset\nstr(BirdNest) # for viewing structure of the dataset\n\nSo we have plenty of variables to play with. The tabs shown below are named according to the theme used in the plots.\n\n\ntheme_gray()\ntheme_bw()\ntheme_linedraw()\ntheme_light()\ntheme_dark()\ntheme_minimal()\ntheme_classic()\ntheme_void()\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_gray()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_linedraw()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_light()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_dark()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_minimal()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_classic()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggplot2)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_void()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#themes-from-ggthemes-package",
    "href": "tutorials/data_viz/ggplot_3.html#themes-from-ggthemes-package",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n2 Themes from ggthemes package",
    "text": "2 Themes from ggthemes package\nIf you want even more pre-built themes then you can try the ggthemes package. This package was developed by Dr.¬†Jeffrey B. Arnold.\n\n# install and load ggthemes package\ninstall.packages(\"ggthemes\")\nlibrary(ggthemes)\n\nThe tabs shown below are named after the themes present in ggthemes package.\n\n\ntheme_base()\ntheme_calc()\ntheme_clean()\ntheme_economist()\ntheme_excel()\ntheme_excel_new()\ntheme_few()\ntheme_fivethirtyeight()\ntheme_foundation()\ntheme_gdocs()\ntheme_hc()\ntheme_igray()\ntheme_map()\ntheme_pander()\ntheme_par()\ntheme_solarized()\ntheme_solid()\ntheme_stata()\ntheme_tufte()\ntheme_wsj()\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_base()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_calc()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_clean()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_economist()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_excel()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_excel_new()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_few()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_fivethirtyeight()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_foundation()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_gdocs()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_hc()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_igray()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_map()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_pander()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_par()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_solarized()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_solid()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_stata()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_tufte()\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(ggthemes)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_wsj()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#changing-colour-palettes-in-ggplot2",
    "href": "tutorials/data_viz/ggplot_3.html#changing-colour-palettes-in-ggplot2",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n3 Changing colour palettes in ggplot2",
    "text": "3 Changing colour palettes in ggplot2\nApart from ready to use themes, there are also ready to use colour palettes which we can use. A colour palette contains a set of pre-defined colours which will be applied to the different geometries present in a graph.\nChoosing a good colour palette is important as it helps us to represent data in a better way and at the same time, it also makes the graph easier to read for people with colour blindness. Let us see a few popular colour palette packages used in R.\n\n3.1 viridis package\nviridis package is a popularly used colour palette in R. It is aesthetically pleasing and well designed to improve readability for colour blind people. The {virids} package was developed by Bob Rudis, Noam Ross and Simon Garnier. There are eight different colour scales present in this package. The name of the tab denotes the colour scale present in this package.\n\nlibrary(viridis)\n\n\n\nviridis\nmagma\nplasma\ninferno\ncividis\nmako\nrocket\nturbo\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"viridis\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"magma\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"plasma\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"inferno\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"cividis\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"mako\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"rocket\")\n\n\n\n\n\n\n\nShow the codelibrary(Stat2Data)\nlibrary(viridis)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_viridis(discrete = TRUE, option = \"turbo\")\n\n\n\n\n\n\n\n\n3.2 wesanderson package\nIf you like your colours distinctive and narrative, just like how American film-maker Mr.¬†Wes Anderson would like it, then try the wesanderson package. Relive the The Grand Budapest Hotel moments through your graphs. The wesanderson package was developed by Karthik Ram. There are a total of 19 colour palettes present in this package. We will see a subset of them. All colour scales in this package are available here. The name of the tab denotes the colour scale used. The data used in this plot is the penguin dataset present in the package palmerpenguins.\n\n#install and load wesanderson and palmerpenguins package\ninstall.packages(\"wesanderson\")\ninstall.packages(\"palmerpenguins\")\nlibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\") #load the penguins dataset\n\n\n\nGrandBudapest1\nBottleRocket2\nRushmore1\nRoyal1\nZissou1\nDarjeeling2\nIsleofDogs1\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"GrandBudapest1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"BottleRocket2\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Rushmore1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Royal1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Zissou1\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"Darjeeling2\", n = 3))\n\n\n\n\n\n\n\nShow the codelibrary(wesanderson)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nggplot(data = penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  labs(x= \"Species\", y= \"Body mass (g)\",\n       fill= \"Nest type\", title= \"Body mass in three diferent species of penguins\",\n       subtitle = \"Penguins observed on islands near Palmer Station, Antarctica\",\n       caption= \"DataSource: penguins dataset in palmerpenguins r-package\") + geom_boxplot() + \n  theme_bw() + scale_fill_manual(values = wes_palette(\"IsleofDogs1\", n = 3))\n\n\n\n\n\n\n\n\n3.3 ggsci package\nIf you want high-quality colour palettes reflecting scientific journal styles then you can try the ggsci package. The ggsci package was developed by Dr.¬†Nan Xiao and Dr.¬†Miaozhu Li. All colour scales in this package are available in package webpage. The name of the tab denotes the colour scale used.\n\n# load ggsci package\ninstall.packages(\"ggsci\")\nlibrary(ggsci)\n\n\n\nnpg\naaas\nnejm\nlancet\njama\njco\nucscgb\nd3\nlocuszoom\nigv\nuchicago\nstartrek\ntron\nfuturama\nrickandmorty\nsimpsons\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_npg()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_aaas()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_nejm()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_lancet()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_jama()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_jco()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_ucscgb()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_d3(palette = \"category10\")\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_locuszoom()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_igv()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_uchicago()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_startrek()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_tron()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_futurama()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_rickandmorty()\n\n\n\n\n\n\n\nShow the codelibrary(ggsci)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\nggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\") +\n  theme_bw() + scale_fill_simpsons()"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#customizing-the-theme",
    "href": "tutorials/data_viz/ggplot_3.html#customizing-the-theme",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n4 Customizing the theme()",
    "text": "4 Customizing the theme()\nA ggplot theme is made up of different elements and it‚Äôs functions. For e.g.¬†plot.title() element allows you to modify the title of the graph using the element function element_text(). In this way, we can change the font size, font family, text colour etc. of the title of the plot. So let us begin customising our graph. We will be reusing the BirdNest dataset for the graphs.\n\n4.1 Customizing text elements using element_text()\nAll text elements can be customized using the element function element_text(). The syntax for element_text() is as follows\n\nelement_text(\n  family = NULL, #insert family font name, e.g. \"Times\"\n  face = NULL,  #font face (\"plain\", \"italic\", \"bold\", \"bold.italic\")\n  colour = NULL, #either from colours() or hex code inside \"\"\n  size = NULL, #text size (in pts)\n  hjust = NULL, #horizontal justification values 0 or 1\n  vjust = NULL, #vertical justification values 0 or 1\n  angle = NULL, #angle in degrees\n  lineheight = NULL, #distance between text and axis line\n  color = NULL, #same function as colour\n  margin = NULL,\n  debug = NULL,\n  inherit.blank = FALSE\n)\n\nWe can modify each of these parameters to improve our plots as shown below in ‚Äòbefore‚Äô and ‚Äòafter‚Äô the changes are made.\n\n\nBefore\nAfter\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np &lt;- ggplot(BirdNest, aes(No.eggs, Totcare, colour = Nesttype)) + geom_point() +\n    labs(x= \"Number of eggs\", y= \"Total care time (days)\",\n       title= \"Relationship between number of eggs and total care time\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\",\n       colour = \"Nest type\")\np\n\n\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np &lt;- ggplot(BirdNest, aes(No.eggs, Totcare, colour = Nesttype)) + geom_point() +\n    labs(x= \"Number of eggs\", y= \"Total care time (days)\",\n       title= \"Relationship between number of eggs and total care time\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\",\n       colour = \"Nest type\")\n\n#customizing text elements\np + theme(plot.title=element_text(size = 15,family = \"Comic Sans MS\",colour = \"darkred\",face = \"bold\"),\n          plot.subtitle=element_text(size = 10,family = \"Courier\",colour= \"blue\",face= \"italic\"),\n          plot.caption = element_text(size = 8,family = \"Times\",colour= \"green\",face=\"bold.italic\", hjust=0),\n          axis.text.x= element_text(size = 6,colour = \"magenta\", angle=20),\n          axis.text.y= element_text(size = 6,colour = \"darkblue\", angle=30),\n          axis.title.x = element_text(colour = \"orchid\"),\n          axis.title.y = element_text(colour = \"sienna\"),\n          legend.text = element_text(size = 8,colour = \"darkgreen\"),\n          legend.title = element_text(size = 10,colour = \"lightblue\",face = \"bold\"))\n\n\n\n\n\n\n\n\n4.2 Customizing line elements using element_line()\nLine elements include axes, grid lines, borders of the graph etc. All line elements can be customized using the element function element_line(). The syntax for element_line() is as follows\n\nelement_line(\n  colour = NULL, #either from colours() or hex code inside \"\"\n  size = NULL, #line size in mm units\n  linetype = NULL, # eg: dashed, dotted etc \n  lineend = NULL, #line end style (round, butt, square)\n  color = NULL, #same function as colour\n  arrow = NULL, #arrow specification\n  inherit.blank = FALSE\n)\n\n\n\nBefore\nAfter\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np &lt;- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\np\n\n\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np &lt;- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\n\n#customizing line elements\np + theme(panel.grid.major = element_line(colour = \"red\", size = 0.8, linetype = \"dashed\"),\n          panel.grid.minor = element_line(colour = \"blue\",linetype = \"twodash\"),\n          axis.line.x = element_line(colour = \"darkred\", arrow = arrow()),\n          axis.line.y = element_line(colour = \"darkblue\"),\n          axis.ticks = element_line(size = 5, colour = \"yellow\"),\n          axis.ticks.length.y=unit(0.5, \"cm\")) #ticks positioned 0.5cm away from y axis\n\n\n\n\n\n\n\n\n4.3 Customizing background elements using element_rect()\nBackground elements include plot, panel and legend backgrounds and their margins. All background elements can be customized using the element function element_rect(). The syntax for element_rect() is as follows\n\nelement_rect(\n  fill = NULL, #fills colour\n  colour = NULL, #colours the border\n  size = NULL, #changes border size in mm units\n  linetype = NULL, #changes border linetype\n  color = NULL,\n  inherit.blank = FALSE\n)\n\n\n\nBefore\nAfter\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np &lt;- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\np\n\n\n\n\n\n\n\nShow the codelibrary(ggplot2)\nlibrary(Stat2Data)\ndata(\"BirdNest\")\n\np &lt;- ggplot(BirdNest, aes(Nesttype, Length, fill = Nesttype)) + geom_boxplot() + \n  labs(x= \"Type of nest\", y= \"Mean body length for the species (in cm)\",\n       fill= \"Nest type\", title= \"Relationship between body length and nest types\",\n       subtitle = \"Data shown for 84 different species of North American passerines\",\n       caption= \"DataSource: BridNest dataset in Stat2Data r-package\")\n\n#customizing line elements\np + theme(plot.background = element_rect(size = 5, colour = \"red\", fill = \"lightblue\"),\n          panel.background = element_rect(size = 3, colour = \"blue\", fill = \"lightyellow\", linetype = \"dotted\"),\n          legend.key = element_rect(fill = \"lightgreen\"),\n          legend.background = element_rect(fill = \"grey\"),\n          legend.key.size = unit(0.75, \"cm\"))"
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#summary",
    "href": "tutorials/data_viz/ggplot_3.html#summary",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n5 Summary",
    "text": "5 Summary\nI hope you are now able to customize the theme of a graph with ease. In this chapter, we learned about different theme elements and how to customize them. We also saw different packages in R which featured ready to use themes. We learned about colour palettes and got introduced to the popular colour packages available R. With that being said, always make sure that your graphs are colour-blind friendly."
  },
  {
    "objectID": "tutorials/data_viz/ggplot_3.html#references",
    "href": "tutorials/data_viz/ggplot_3.html#references",
    "title": "Chapter 3: Even more customizations in ggplot2",
    "section": "\n6 References",
    "text": "6 References\n\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Read more about ggplot2 here. You can also look at the cheat sheet for all the syntax used in ggplot2. Also check this out.\nAnn Cannon, George Cobb, Bradley Hartlaub, Julie Legler, Robin Lock, Thomas Moore, Allan Rossman and Jeffrey Witmer (2019). Stat2Data: Datasets for Stat2. R package version 2.0.0. https://CRAN.R-project.org/package=Stat2Data\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nJeffrey B. Arnold (2021). ggthemes: Extra Themes, Scales and Geoms for ‚Äòggplot2‚Äô. R package version 4.2.4. https://CRAN.R-project.org/package=ggthemes\nSimon Garnier, Noam Ross, Robert Rudis, Ant√¥nio P. Camargo, Marco Sciaini, and C√©dric Scherer (2021). Rvision - Colorblind-Friendly Color Maps for R. R package version 0.6.2. You can read more here.\nKarthik Ram and Hadley Wickham (2018). wesanderson: A Wes Anderson Palette Generator. R package version 0.3.6. https://CRAN.R-project.org/package=wesanderson\nNan Xiao (2018). ggsci: Scientific Journal and Sci-Fi Themed Color Palettes for ‚Äòggplot2‚Äô. R package version 2.9. https://CRAN.R-project.org/package=ggsci"
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html",
    "href": "tutorials/data_viz/ggpubr.html",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "",
    "text": "If you are a researcher who wants to have publication-ready plots but does not want to get hassled by the ggplot2 package, then let me introduce you to the ggpubr package. Using this package you can make publication grade plots without spending too much time modifying things. Even if you are a beginner in R programming and does not know how to use theggplot2 package, you will still be able to plot graphs using the ggpubr package because of how easy the syntax is. But having prior knowledge of the ggplot2 package will surely make things easier, and an experienced person will know that any plot which can be plotted using ggpubr can also be plotted using ggplot2. So let us start!\nFirst things first, install the ggpubr package and load it in the library.\n\ninstall.packages(\"ggpubr\")\nlibrary(ggpubr)\n\nLet us see what all plots can be plotted."
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#introduction-to-ggpubr-package",
    "href": "tutorials/data_viz/ggpubr.html#introduction-to-ggpubr-package",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "",
    "text": "If you are a researcher who wants to have publication-ready plots but does not want to get hassled by the ggplot2 package, then let me introduce you to the ggpubr package. Using this package you can make publication grade plots without spending too much time modifying things. Even if you are a beginner in R programming and does not know how to use theggplot2 package, you will still be able to plot graphs using the ggpubr package because of how easy the syntax is. But having prior knowledge of the ggplot2 package will surely make things easier, and an experienced person will know that any plot which can be plotted using ggpubr can also be plotted using ggplot2. So let us start!\nFirst things first, install the ggpubr package and load it in the library.\n\ninstall.packages(\"ggpubr\")\nlibrary(ggpubr)\n\nLet us see what all plots can be plotted."
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#plots-in-ggpubr-package",
    "href": "tutorials/data_viz/ggpubr.html#plots-in-ggpubr-package",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n2 Plots in ggpubr package",
    "text": "2 Plots in ggpubr package\n\n2.1 Balloon plot\nThe balloon plot is similar to bar plots as it is used to represent a large categorical dataset. The size and colour of the dot can be attributed to different values in the dataset.\n\nlibrary(viridis)\nlibrary(ggpubr)\n\nggballoonplot(mtcars, fill = \"value\") + \n  scale_fill_viridis(option = \"turbo\")\n\n\n\n\n\n2.2 Bar plot\nA simple bar graph which is used for representing categorical data. By using the add function inside the main plot function, you can easily display summary statistics like mean, median etc. and various types of errors like standard error, standard deviation and various others. You can view the whole list of features here.\n\n# install.packages(\"palmerpenguins\")\nlibrary(ggpubr)\nlibrary(palmerpenguins)\n\nggbarplot(penguins,\n          x = \"species\",\n          y = \"bill_length_mm\",\n          add = c(\"mean_sd\"),\n          fill = \"species\",\n          label = TRUE,\n          lab.nb.digits = 2,\n          lab.vjust = -2.2,\n          lab.col = \"red\",\n          title = \"Mean bill length of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill length (mm)\",\n          ylim = c(0,60),\n          palette = \"npg\")\n\n\n\n\n\n2.3 Box plot\nStandard box plot graph. Like in the previous graph you can specify colour palettes from the scientific journal palettes featured in the ggsci R package.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\")\n\n\n\n\n\n2.4 Violin plot\nA simple violin plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggviolin(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          fill = \"species\",\n          palette = \"npg\",\n          add = \"boxplot\",\n          shape = \"species\")\n\n\n\n\n\n2.5 Density plot\nStandard density plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggdensity(penguins,\n          x = \"body_mass_g\",\n          color = \"species\",\n          rug = TRUE,\n          fill = \"species\",\n          add = \"mean\",\n          title = \"Mean body mass of penguins\",\n          xlab = \"Body mass (g)\",\n          palette = \"lancet\")\n\n\n\n\n\n2.6 Donut chart\nSimilar to a pie diagram. Also please note that you don‚Äôt have to explicitly mention x and y parameters in the command. You can simply just type the column names, the first column name will be shown on the x-axis and the second on the y axis.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\nlibrary(dplyr)\nlibrary(tidyr)\n\npenguins_freq &lt;- penguins %&gt;% drop_na() %&gt;%\n  group_by(species) %&gt;%\n  summarise(frequency = length(species))\n\nlabs &lt;- paste0(penguins_freq$species, \" (\", round((penguins_freq$frequency/sum(penguins_freq$frequency))*100, digits = 0), \"%)\")\n\nggdonutchart(penguins_freq,\n             \"frequency\",\n             label = labs,\n             fill = \"species\",\n             palette = \"ucscgb\",\n             lab.pos = \"in\",\n             title = \"Frequency of penguins\")\n\n\n\n\n\n2.7 Pie chart\nSimple pie chart.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\nlibrary(dplyr)\nlibrary(tidyr)\n\npenguins_freq &lt;- penguins %&gt;% drop_na() %&gt;%\n  group_by(species) %&gt;%\n  summarise(frequency = length(species))\n\nlabs &lt;- paste0(penguins_freq$species, \" (\", round((penguins_freq$frequency/sum(penguins_freq$frequency))*100, digits = 0), \"%)\")\n\nggpie(penguins_freq,\n             \"frequency\",\n             label = labs,\n             fill = \"species\",\n             palette = \"futurama\",\n             lab.pos = \"in\",\n             title = \"Frequency of penguins\")\n\n\n\n\n\n2.8 Dot chart\nThis is an upgrade from bar charts where the data is displayed with minimum clutter in the form of dots. This allows the readers to not get bothered about things like the slope of a line in case of line plots, or width of bars in case of bar charts or any other confusing aesthetics of a plot. You can read more about this graph here. It is also called ‚ÄúCleveland dot plots‚Äù named after the founder of this plot.\n\nlibrary(tibble)\nlibrary(ggpubr)\nlibrary(tidyr)\n\nmtcars %&gt;% rownames_to_column(var = \"car_names\") %&gt;% \n  mutate(cyl = as.factor(cyl)) %&gt;%\n  ggdotchart(\"car_names\",\n             \"mpg\",\n             color = \"cyl\",\n             palette = \"aaas\",\n             sorting = \"ascending\",\n             rotate = TRUE,\n             y.text.col = TRUE,\n             dot.size = 2,\n             ylab = \"Miles per gallon of fuel\",\n             title = \"Mileage of different cars\",\n             ggtheme = theme_pubr()) + theme_cleveland() \n\n\n\n\n\n2.9 Dot plot\nSimple dot plot. Similar to a box plot. You can also overlay a box plot or a violin plot over the dot plot using the add function inside the main function.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggdotplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          fill = \"species\",\n          add = \"mean_sd\",\n          palette = \"locuszoom\")\n\n\n\n\n\n2.10 Histogram plot\nThe same function as that of a density plot but the data is represented in bars.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\ngghistogram(penguins,\n            x = \"body_mass_g\",\n            add = \"mean\",\n            fill = \"species\",\n            rug = TRUE,\n            title = \"Body mass of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Frequency\",\n            palette = \"startrek\")\n\n\n\n\n\n2.11 Line plot\nA simple line plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nggline(penguins,\n      x = \"body_mass_g\",\n      y = \"bill_depth_mm\",\n      linetype = \"species\",\n      shape = \"species\",\n      color = \"species\",\n      title = \"Body mass vs Bill depth\",\n      xlab = \"Body mass (g)\",\n      ylab = \"Bill depth (mm)\",\n      palette = \"startrek\")\n\n\n\n\n\n2.12 Plotting paired data\nThis is essentially a box plot but for paired data. Widely used to represent treatment groups showing before and after results of the same sample. We will be using the Anorexia dataset from the {PairedData} package in R. It features weights of girls before and after treatment for Anorexia.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\nlibrary(ggpubr)\ndata(\"Anorexia\")\n\nAnorexia %&gt;% \n  ggpaired(cond1 = \"Prior\",\n           cond2 = \"Post\",\n           title = \"Weights of girls before and after treatment for anorexia\",\n           xlab = \"Condition\",\n           ylab = \"Weight (lbs)\",\n           fill = \"condition\",\n           line.color = \"darkgreen\",\n           line.size = 0.2,\n           palette = \"simpsons\")\n\n\n\n\n\n2.13 Quantile-Quantile plot\nQuantile-Quantile plot or QQ plot is useful in assessing the distribution of a data. A data having normal distribution will be shown as a straight line of the formula ‚Äòy=x‚Äô in the QQ plot. Points outside the confidence interval are outliers in the data.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %&gt;% ggqqplot(\"body_mass_g\",\n                      color = \"species\",\n                      palette = \"aaas\",\n                      title = \"Quantile-Quantile plot\")\n\n\n\n\n\n2.14 Scatter plot\nA simple scatter plot.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %&gt;% filter(species == \"Chinstrap\") %&gt;%\n  ggscatter(\"body_mass_g\",\n            \"bill_length_mm\",\n            add = \"reg.line\",\n            add.params = list(color = \"darkred\", fill = \"yellow\"),\n            cor.coef = TRUE,\n            cor.method = \"pearson\",\n            conf.int = TRUE,\n            title = \"Body mass distribution of Chinstrap penguins\",\n            subtitle = \"Correlation method used was Pearson\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill lenght (mm)\")\n\n\n\n\nYou can also use scatter plot for data having different categories. Using ellipse=TRUE you can group data to its category.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %&gt;%\n  ggscatter(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5,\n            palette = \"d3\",\n            ellipse = TRUE, #adds an ellipse to group data of different category\n            title = \"Body mass vs Bill length\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\")\n\n\n\n\nYou can also label points in the scatter plot using the label function.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nmtcars %&gt;% rownames_to_column(var = \"car_names\") %&gt;% \n  mutate(cyl = as.factor(cyl)) %&gt;%\n  ggscatter(\"wt\",\n             \"mpg\",\n             color = \"cyl\",\n             palette = \"nejm\",\n             xlab = \"Weight (1000 lbs)\",\n             ylab = \"Miles per gallon of fuel\",\n             title = \"Mileage vs Weight of different cars\",\n             label = \"car_names\",\n             repel = TRUE,\n             ggtheme = theme_pubr()) + theme_cleveland() \n\n\n\n\n\n2.15 Scatter plot with marginal histograms\nThis is plot is a combination of scatter plot and histograms.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %&gt;%\n  ggscatterhist(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5, size = 2,\n            palette = \"futurama\",\n            margin.params = list(fill = \"species\", color = \"black\", size = 0.2),\n            title = \"Body mass distribution of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\")\n\n\n\n\nYou can also choose to show box plots.\n\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %&gt;%\n  ggscatterhist(\"body_mass_g\",\n            \"bill_depth_mm\",\n            color = \"species\",\n            alpha = 0.5, size = 2,\n            palette = \"futurama\",\n            margin.plot = \"boxplot\",\n            title = \"Body mass vs Bill depth\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill depth (mm)\",\n            ggtheme = theme_bw())"
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#other-functions-in-ggpubr-package",
    "href": "tutorials/data_viz/ggpubr.html#other-functions-in-ggpubr-package",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n3 Other functions in ggpubr package",
    "text": "3 Other functions in ggpubr package\n\n3.1 Statistical tests\nYou can do various statistical tests using the functions in the ggpubr package. We will be using the Anorexia dataset in the {PairedData} package in R. In the code given below, we are doing a Wilcoxon test to compare the mean weights of girls before treatment to the mean weights of girls post-treatment. Since the data is paired we will indicate it by the paired = TRUE function. A word of caution! Before starting to do statistical tests please ensure whether you can fulfil conditions for using parametric tests or not using or data. You can check whether your data is normally distributed using a QQ plot or by using any normality tests.\nPS: I use knitr::kable() just for illustrative purpose only. You can run the command inside the kable() argument and you will be fine.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\nlibrary(dplyr)\nlibrary(tidyr)\n\ndata(\"Anorexia\")\n\n# tidying the data\nAnorexia_new &lt;- Anorexia %&gt;% \n  pivot_longer(c(Prior, Post), names_to = \"condition\", values_to = \"weight\")\nknitr::kable(compare_means(weight ~ condition, Anorexia_new, paired = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\np\np.adj\np.format\np.signif\nmethod\n\n\nweight\nPrior\nPost\n0.0008392\n0.00084\n0.00084\n***\nWilcoxon\n\n\n\n\nYou can also do parametric tests like ANOVA and its non-parametric version; the Kruskal-Wallis test, which can be followed by multiple pairwise comparisons.\n\nknitr::kable(compare_means(body_mass_g ~ species, penguins, method = \"anova\"))\n\n\n\n.y.\np\np.adj\np.format\np.signif\nmethod\n\n\nbody_mass_g\n0\n0\n&lt;2e-16\n****\nAnova\n\n\n\nknitr::kable(compare_means(body_mass_g ~ species, penguins, method = \"kruskal.test\"))\n\n\n\n.y.\np\np.adj\np.format\np.signif\nmethod\n\n\nbody_mass_g\n0\n0\n&lt;2e-16\n****\nKruskal-Wallis\n\n\n\n\nNow doing pairwise comparisons\n\n# multiple pairwise comparisons\n# when there is more than two levels, the function automatically does pairwise comparisons\nknitr::kable(compare_means(body_mass_g ~ species, penguins))\n\n\n\n\n\n\n\n\n\n\n\n\n\n.y.\ngroup1\ngroup2\np\np.adj\np.format\np.signif\nmethod\n\n\n\nbody_mass_g\nAdelie\nChinstrap\n0.4854773\n0.49\n0.49\nns\nWilcoxon\n\n\nbody_mass_g\nAdelie\nGentoo\n0.0000000\n0.00\n&lt;2e-16\n****\nWilcoxon\n\n\nbody_mass_g\nChinstrap\nGentoo\n0.0000000\n0.00\n&lt;2e-16\n****\nWilcoxon\n\n\n\n\n\n\n3.2 Descriptive statistics by groups\nUsing the function desc_statby() we can get the summary statistics of a dataset in the form of a data frame. Similar to the summary() function in base R.\n\nknitr::kable(desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nlength\nmin\nmax\nmedian\nmean\niqr\nmad\nsd\nse\nci\nrange\ncv\nvar\n\n\n\nAdelie\n151\n2850\n4775\n3700\n3700.662\n650.0\n444.780\n458.5661\n37.31758\n73.73601\n1925\n0.1239146\n210282.9\n\n\nChinstrap\n68\n2700\n4800\n3700\n3733.088\n462.5\n370.650\n384.3351\n46.60747\n93.02891\n2100\n0.1029537\n147713.5\n\n\nGentoo\n123\n3950\n6300\n5000\n5076.016\n800.0\n555.975\n504.1162\n45.45463\n89.98198\n2350\n0.0993134\n254133.2\n\n\n\n\n\nYou can also show the data as a table using the ggtexttable() function.\n\nsummary &lt;- desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\")\nsummary_short &lt;- summary %&gt;% dplyr::select(species, mean, median, se, sd)\nsummary_tbl &lt;- ggtexttable(summary_short, rows = NULL, theme = ttheme(\"mRed\")) # use ?ttheme to see more themes\nsummary_tbl\n\n\n\n\n\n3.3 Showing p-values and statistical results within plots\nUsing various functions you can show statistical outputs within the plots.\nUsing Wilcoxon test for paired data.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\nlibrary(ggpubr)\n\ndata(\"Anorexia\")\nAnorexia %&gt;% \n  ggpaired(cond1 = \"Prior\",\n           cond2 = \"Post\",\n           title = \"Weights of girls before and after treatment for anorexia\",\n           xlab = \"Condition\",\n           ylab = \"Weight (lbs)\",\n           fill = \"condition\",\n           line.color = \"darkgreen\",\n           line.size = 0.2,\n           palette = \"simpsons\") + stat_compare_means(paired = TRUE)\n\n\n\n\nUsing t-test\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %&gt;% filter(species == \"Adelie\" & island == c(\"Biscoe\",\"Torgersen\")) %&gt;%\nggboxplot(x = \"island\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Island\",\n          ylab = \"Bill depth (mm)\",\n          color = \"island\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"island\") + stat_compare_means(method = \"t.test\")\n\n\n\n\nUsing ANOVA test and t-test as post hoc test. For pairwise comparison, we have to manually list out the pairwise comparisons that we want.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\n# listing out pairwise comparisons \ncompare &lt;- list(c(\"Adelie\", \"Chinstrap\"), c(\"Adelie\", \"Gentoo\"), c(\"Chinstrap\", \"Gentoo\"))\n\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\") + \n  stat_compare_means(method = \"anova\", label.y = 25) + #anova test\n  stat_compare_means(comparisons = compare, method = \"t.test\") # post hoc test using t-test\n\n\n\n\nUsing Kruskal-Wallis test and Wilcoxon test as post hoc test.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\ncompare &lt;- list(c(\"Adelie\", \"Chinstrap\"), c(\"Adelie\", \"Gentoo\"), c(\"Chinstrap\", \"Gentoo\"))\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\") + \n  stat_compare_means(label.y = 25) + #anova test\n  stat_compare_means(comparisons = compare) # post hoc test using t-test\n\n\n\n\nYou can also choose to show only asterisks as significance levels\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\ncompare &lt;- list(c(\"Adelie\", \"Chinstrap\"), c(\"Adelie\", \"Gentoo\"), c(\"Chinstrap\", \"Gentoo\"))\nggboxplot(penguins ,\n          x = \"species\",\n          y = \"bill_depth_mm\",\n          title = \"Mean bill depth of penguins\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill depth (mm)\",\n          color = \"species\",\n          palette = \"futurama\",\n          add = \"jitter\",\n          shape = \"species\") + \n  stat_compare_means(label.y = 25) +\n  stat_compare_means(comparisons = compare, label = \"p.signif\")\n\n\n\n\nFor illustrative purposes, I have used box plots for showing p-values and statistical test results, but you can do the same with most of the other types of graphs shown in this chapter.\n\n3.4 Faceting plots into grids\nYou can also facet different plots into grids using the function facet.by.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %&gt;% drop_na() %&gt;%\n  ggscatter(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5,\n            palette = \"d3\",\n            facet.by = c(\"island\", \"sex\"), # faceting graphs via island and sex categories\n            title = \"Body mass vs Bill length\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\")\n\n\n\n\n\n3.5 Adding paragraph\nYou can also add a paragraph beneath the plot of your interest using the ggparagraph() and ggarrange() functions.\n\n# install.packages(\"PairedData\")\nlibrary(PairedData)\ndata(\"Anorexia\")\nlibrary(ggpubr)\n\ntext &lt;- paste(\"The above dataset shows the weight (in lbs) of 17 girls before\",\n              \"and after they got treatment for anorexia.\", sep = \" \")\n\ntext_plot &lt;- ggparagraph(text, face = \"bold\", size = 12)\n\nplot &lt;- Anorexia %&gt;% \n  ggpaired(cond1 = \"Prior\",\n           cond2 = \"Post\",\n           title = \"Weights of girls before and after treatment for anorexia\",\n           xlab = \"Condition\",\n           ylab = \"Weight (lbs)\",\n           fill = \"condition\",\n           line.color = \"darkgreen\",\n           line.size = 0.2,\n           palette = \"simpsons\")\n\nggarrange(plot, text_plot,\n         ncol = 1, nrow = 2,\n         heights = c(1, 0.3))\n\n\n\n\n\n3.6 Having plots placed adjacent to each other\nYou can use the ggarrange() function to place different plots together.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nbar_plot &lt;- ggbarplot(penguins,\n          x = \"species\",\n          y = \"bill_length_mm\",\n          add = c(\"mean_sd\"),\n          fill = \"species\",\n          label = TRUE,\n          lab.nb.digits = 2,\n          lab.vjust = -2.2,\n          lab.col = \"red\",\n          title = \"Mean bill length of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill length (mm)\",\n          ylim = c(0,60),\n          palette = \"npg\")\n\nhistogram &lt;- gghistogram(penguins,\n            x = \"body_mass_g\",\n            add = \"mean\",\n            fill = \"species\",\n            rug = TRUE,\n            title = \"Body mass of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Frequency\",\n            palette = \"startrek\")\n\nsummary &lt;- desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\")\nsummary_short &lt;- summary %&gt;% dplyr::select(species, mean, median, se, sd)\nsummary_tbl &lt;- ggtexttable(summary_short, rows = NULL, theme = ttheme(\"mRed\")) # use ?ttheme to see more themes\n\n# arranging plots together\nggarrange(bar_plot, histogram,\n         ncol = 2, nrow = 2, labels = c(\"A\", \"B\"),\n         heights = c(1, 0.3))\n\n\n\n\nIf you are arranging three graphs it is better to use the grid.arrange() function from the {gridExtra} package in R.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\nbar_plot &lt;- ggbarplot(penguins,\n          x = \"species\",\n          y = \"bill_length_mm\",\n          add = c(\"mean_sd\"),\n          fill = \"species\",\n          label = TRUE,\n          lab.nb.digits = 2,\n          lab.vjust = -2.2,\n          lab.col = \"red\",\n          title = \"Mean bill length of penguins\",\n          subtitle = \"Error bars shows standard deviation\",\n          xlab = \"Species of penguins\",\n          ylab = \"Bill length (mm)\",\n          ylim = c(0,60),\n          palette = \"npg\")\n\nhistogram &lt;- gghistogram(penguins,\n            x = \"body_mass_g\",\n            add = \"mean\",\n            fill = \"species\",\n            rug = TRUE,\n            title = \"Body mass of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Frequency\",\n            palette = \"startrek\")\n\nsummary &lt;- desc_statby(penguins, measure.var = \"body_mass_g\", grps = \"species\")\nsummary_short &lt;- summary %&gt;% dplyr::select(species, mean, median, se, sd)\nsummary_tbl &lt;- ggtexttable(summary_short, rows = NULL, theme = ttheme(\"mRed\")) # use ?ttheme to see more themes\n\n# arranging three plots together\nlayout_matrix &lt;- matrix(c(1, 1, 2, 2, 4, 3, 3, 4), nrow = 2, byrow = TRUE)\nlibrary(gridExtra)\ngrid.arrange(bar_plot, histogram, summary_tbl, layout_matrix = layout_matrix)"
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#saving-your-plot",
    "href": "tutorials/data_viz/ggpubr.html#saving-your-plot",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n4 Saving your plot",
    "text": "4 Saving your plot\nUsing the function ggexport() you can save your plot. Tweak width and height accordingly and also change the resolution to fit your needs.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggpubr)\n\npenguins %&gt;%\n  ggscatterhist(\"body_mass_g\",\n            \"bill_length_mm\",\n            color = \"species\",\n            alpha = 0.5, size = 2,\n            palette = \"futurama\",\n            margin.params = list(fill = \"species\", color = \"black\", size = 0.2),\n            title = \"Body mass distribution of penguins\",\n            xlab = \"Body mass (g)\",\n            ylab = \"Bill length (mm)\") %&gt;%\n  ggexport(filename = \"my_plot.png\", width = 800, height = 600, res = 150)"
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#summary",
    "href": "tutorials/data_viz/ggpubr.html#summary",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n5 Summary",
    "text": "5 Summary\nIn this chapter we learned how to plot publication standard graphs using the ggpubr package in R. Even with little to no experience in using the ggplot2 package in R, one can plot graphs with ease using the ggpubr package. To quickly recap, from this chapter we saw;\n\nHow to plot around 15 different types of graphs\nHow to facet plots\nHow to do basic statistical tests and visualize them within graphs\nHow to add paragraph text under the graphs\nHow to group different graphs into one single file\n\nI hope this chapter was useful to you. Check out the other chapter for more beginner content."
  },
  {
    "objectID": "tutorials/data_viz/ggpubr.html#references",
    "href": "tutorials/data_viz/ggpubr.html#references",
    "title": "Chapter 4: Publication ready plots using ggpubr",
    "section": "\n6 References",
    "text": "6 References\n\nAlboukadel Kassambara (2020). ggpubr: ‚Äòggplot2‚Äô Based Publication Ready Plots. R package version 0.4.0. https://CRAN.R-project.org/package=ggpubr\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/\nStephane Champely (2018). PairedData: Paired Data Analysis. R package version 1.1.1. https://CRAN.R-project.org/package=PairedData\nWilliam S. Cleveland & Robert McGill (1984) Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods, Journal of the American Statistical Association, 79:387, 531-554, DOI: 10.1080/01621459.1984.10478080\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr\nHadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\nSimon Garnier, Noam Ross, Robert Rudis, Ant√¥nio P. Camargo, Marco Sciaini, and C√©dric Scherer (2021). Rvision - Colorblind-Friendly Color Maps for R. R package version 0.6.2."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html",
    "href": "tutorials/network_analysis/network_analysis.html",
    "title": "Introduction to Network Analysis in R",
    "section": "",
    "text": "TL;DR\n\n\n\nIn this article we will learn;\n\nWhat is a network and what are the different ways to denote the network data?\nHow to convert the network data into an igraph object to analyze and plot the network via the {igraph} package in R.\nWhat are vertex and edge attributes and how do we add them to the igraph object?\nHow do we filter network attributes and how to visualize them?\nWhat are the different types of network visualizations?\nWhat are directed networks?\nWhat is path length?\nWhat are the different measures of network structure?\nWhat is network randomization test and what is its purpose?\nwhat are the different network substructures?\nHow to identify special relationships in the network?\nHow to find communities in a network?\nHow to visualize the network in 3D using the {threejs} package\n\nWe will be using the {igprah} and {threejs} packages in this tutorial."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#introduction",
    "href": "tutorials/network_analysis/network_analysis.html#introduction",
    "title": "Introduction to Network Analysis in R",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn this tutorial, we will explore social networks and how to create and analyze them. A social network is a graphical representation of relationships between a group of individuals. For instance, the figure below shows a social network that could represent friendships among a group of people, bus routes to different locations in a city, or connections between different neurons.\n\n\n\nA graph depicting a social network\n\n\nIf we take a closer look at the figure, we can see that individual A has some kind of relationship with individuals B, C, and F, but not with D and E. The individuals, such as A, B, C, and so on, are called nodes or vertices, while the connections between them are known as lines, edges, or ties.\nNetwork data can be represented in different forms, such as an adjacency matrix or an edge list. The adjacency matrix for the network shown in the figure is provided below. In this matrix, each row and column corresponds to a vertex, and a value of 1 indicates the presence of an edge between the corresponding vertices.\n\n\nCode for making the adjacency matrix for the above figure\nif (!require(igraph)) install.packages('igraph')\nlibrary(igraph)\n\nname1 &lt;- c(rep('a', 3), 'b', 'c', 'd')\nname2 &lt;- c('b','c','f','d','e','e')\nrel &lt;- data.frame(name1, name2)\nrel_ig &lt;- graph_from_data_frame(rel)\nas_adjacency_matrix(rel_ig)\n\n\n6 x 6 sparse Matrix of class \"dgCMatrix\"\n  a b c d f e\na . 1 1 . 1 .\nb . . . 1 . .\nc . . . . . 1\nd . . . . . 1\nf . . . . . .\ne . . . . . .\n\n\nAlternatively, we can represent the same information in an edge list format, where all the edges in the network are listed in a 2x2 matrix.\n\n\nCode for making the edge list for the above figure\nif (!require(igraph)) install.packages('igraph')\nlibrary(igraph)\n\nname1 &lt;- c(rep('a', 3), 'b', 'c', 'd')\nname2 &lt;- c('b','c','f','d','e','e')\nrel &lt;- data.frame(name1, name2)\nrel_ig &lt;- graph_from_data_frame(rel)\nas_edgelist(rel_ig)\n\n\n     [,1] [,2]\n[1,] \"a\"  \"b\" \n[2,] \"a\"  \"c\" \n[3,] \"a\"  \"f\" \n[4,] \"b\"  \"d\" \n[5,] \"c\"  \"e\" \n[6,] \"d\"  \"e\" \n\n\nThe edge list provided above displays each relationship or edge in the given social network through its rows. To gain a better understanding of how we can analyze and plot such data, let‚Äôs work with a dataset resembling a real-life scenario."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#creating-a-dummy-dataframe",
    "href": "tutorials/network_analysis/network_analysis.html#creating-a-dummy-dataframe",
    "title": "Introduction to Network Analysis in R",
    "section": "2 Creating a dummy dataframe",
    "text": "2 Creating a dummy dataframe\nWe will work with a synthetic dataset consisting of 10 individuals and their relationships, gender, and weekly call duration. This will allow us to explore how social network analysis can be applied to real-life scenarios. The dataset includes friendships between individuals and other attributes such as gender and call duration. Below is the code used to generate this dataset.\n\n# set seed for reproducibility\nset.seed(123)\n\n# Create a dataframe with 10 names and gender\n1df_names &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\",\n           \"Grace\", \"Henry\", \"Isabelle\", \"John\"),\n  gender = c(\"F\", \"M\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\")\n)\n\n# Create an empty dataframe to store the relationships\n2df_rel &lt;- data.frame(name1 = character(), name2 = character(),\n                     call_hours = numeric()) \n\n# Generate random relationships with call hours\n3while (nrow(df_rel) &lt; 20) {\n  # Randomly select two names\n4  name1 &lt;- sample(df_names$name, 1)\n  name2 &lt;- sample(df_names$name, 1)\n  \n  # Check if the selected names are the same or have already been added to the dataframe\n5  if (name1 != name2 & !any((df_rel$name1 == name1 & df_rel$name2 == name2) |\n                            (df_rel$name1 == name2 & df_rel$name2 == name1))) { \n    # Add the relationship with a random number of call hours between 1 and 10\n6    df_rel &lt;- rbind(df_rel, data.frame(name1 = name1, name2 = name2,\n                                       call_hours = sample(1:10, 1))) \n  }\n}\n\n# Finally our dataset\n7df_rel\n\n\n1\n\nFirst, the code creates a dataframe called df_names with 10 names and their genders.\n\n2\n\nThen, an empty dataframe called df_rel is created to store the relationships.\n\n3\n\nThe while loop is used to generate random relationships between the names until df_rel has 20 rows.\n\n4\n\nInside the while loop, two names are randomly selected from the name column of df_names using the sample function.\n\n5\n\nThe if statement checks whether the two names are the same or whether they have already been added as a relationship to df_rel. If either of these conditions is true, the loop moves on to select a new pair of names.\n\n6\n\nIf the two names are different and have not been added to df_rel, a new row is added to df_rel with the selected names and a randomly generated number between 1 and 10 as the number of call hours.\n\n7\n\nOnce df_rel has 20 rows, the loop stops, and the final dataframe is printed using print(df_rel)."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#converting-dataframe-to-network-data",
    "href": "tutorials/network_analysis/network_analysis.html#converting-dataframe-to-network-data",
    "title": "Introduction to Network Analysis in R",
    "section": "3 Converting dataframe to network data",
    "text": "3 Converting dataframe to network data\nTo convert our dataframe into network data, we‚Äôll be using the {igraph} package in R and the graph.edgelist() function. However, before we can do that, we need to make sure that our dataframe is in the right format. The graph.edgelist() function requires a 2x2 edge list matrix, which means that our dataframe needs to be converted to a matrix using the as.matrix() function. For now, we‚Äôll also remove the call_hours column.\n\n# Installing and loading the igraph package\nif (!require(igraph)) install.packages('igraph')\nlibrary(igraph)\n\n# Creating an igraph object\ndf_rel_mx &lt;- as.matrix(df_rel[,1:2])\ndf_rel_ig &lt;- graph.edgelist(df_rel_mx, directed = F)\n\n# Viewing the igraph object\ndf_rel_ig\n\nIGRAPH 5574de0 UN-- 10 20 -- \n+ attr: name (v/c)\n+ edges from 5574de0 (vertex names):\n [1] John    --Bob      Eve     --David    John    --Isabelle Isabelle--Charlie \n [5] John    --Henry    David   --Charlie  Eve     --Grace    Isabelle--Grace   \n [9] John    --Grace    Isabelle--Frank    Eve     --Henry    Isabelle--Alice   \n[13] Eve     --Frank    John    --David    Henry   --Frank    Grace   --Alice   \n[17] Bob     --Alice    Charlie --Frank    David   --Frank    Charlie --Henry   \n\n\n\nIn the first line of the output, the first number; 10 indicates that there are 10 vertices, and 20 means that there are 20 edges in the network.\nStarting from the 4th line of the output, all the edges in the network are shown. These are relationships in the dataset.\n\nNow let us plot the network using the plot() function.\n\nplot(df_rel_ig)\n\n\n\n\nIn the above graph, each person is represented as a vertex, shown in blue. The edges between them indicate that they are friends with each other.\nThe {igraph} package provides various functions to further analyze the network data. For instance, we can use the V() function to view the vertices in the dataset.\n\n# Viewing the nodes/vertices\nV(df_rel_ig)\n\n+ 10/10 vertices, named, from 5574de0:\n [1] John     Bob      Eve      David    Isabelle Charlie  Henry    Grace   \n [9] Frank    Alice   \n\n\nSimilarly, using the E() function returns all the edges.\n\n# Viewing the edges\nE(df_rel_ig)\n\n+ 20/20 edges from 5574de0 (vertex names):\n [1] John    --Bob      Eve     --David    John    --Isabelle Isabelle--Charlie \n [5] John    --Henry    David   --Charlie  Eve     --Grace    Isabelle--Grace   \n [9] John    --Grace    Isabelle--Frank    Eve     --Henry    Isabelle--Alice   \n[13] Eve     --Frank    John    --David    Henry   --Frank    Grace   --Alice   \n[17] Bob     --Alice    Charlie --Frank    David   --Frank    Charlie --Henry   \n\n\nTo get the number of vertices we can use gorder() and for getting the number of edges we can use gsize().\n\n# Viewing the no. of vertices\ngorder(df_rel_ig)\n\n[1] 10\n\n# Viewing the no. of edges\ngsize(df_rel_ig)\n\n[1] 20"
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#network-attributes",
    "href": "tutorials/network_analysis/network_analysis.html#network-attributes",
    "title": "Introduction to Network Analysis in R",
    "section": "4 Network attributes",
    "text": "4 Network attributes\n\n4.1 Vertex attributes\nLet us once again look at the igraph object output for the data we are using.\n\n# Viewing the igraph object\ndf_rel_ig\n\nIGRAPH 5574de0 UN-- 10 20 -- \n+ attr: name (v/c)\n+ edges from 5574de0 (vertex names):\n [1] John    --Bob      Eve     --David    John    --Isabelle Isabelle--Charlie \n [5] John    --Henry    David   --Charlie  Eve     --Grace    Isabelle--Grace   \n [9] John    --Grace    Isabelle--Frank    Eve     --Henry    Isabelle--Alice   \n[13] Eve     --Frank    John    --David    Henry   --Frank    Grace   --Alice   \n[17] Bob     --Alice    Charlie --Frank    David   --Frank    Charlie --Henry   \n\n\nIn the second line of the output, we can see the attributes of the vertices. The network attributes can be either categorical or numerical. In this case, the attribute ‚Äòname‚Äô is a categorical variable denoting a person‚Äôs name as vertices. Examples of numerical attributes include the age of an individual, the population of a city, or revenue of a company, etc.\nThe vertex attributes can be visualized using different shapes, colors, or sizes of the shape. To add a vertex attribute, we can use the set_vertex_attr() function. In the function arguments, we first input the igraph object, then the attribute name that we choose to give, which in this case would be ‚Äògender‚Äô, and finally the values for that attribute. Therefore, we use our df_names dataframe that we created before to fetch the gender data as the vertex attribute.\n\n# Our dataframe with 10 names and gender\ndf_names &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\",\n           \"Grace\", \"Henry\", \"Isabelle\", \"John\"),\n  gender = c(\"F\", \"M\", \"M\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\")\n)\n\n# Adding 'gender' as a vertex attribute\ndf_rel_ig_gender &lt;- set_vertex_attr(df_rel_ig, \"gender\", value = df_names$gender)\n\n# Viewing the igraph object\ndf_rel_ig_gender\n\nIGRAPH 5574de0 UN-- 10 20 -- \n+ attr: name (v/c), gender (v/c)\n+ edges from 5574de0 (vertex names):\n [1] John    --Bob      Eve     --David    John    --Isabelle Isabelle--Charlie \n [5] John    --Henry    David   --Charlie  Eve     --Grace    Isabelle--Grace   \n [9] John    --Grace    Isabelle--Frank    Eve     --Henry    Isabelle--Alice   \n[13] Eve     --Frank    John    --David    Henry   --Frank    Grace   --Alice   \n[17] Bob     --Alice    Charlie --Frank    David   --Frank    Charlie --Henry   \n\n\nIn the igraph object output, in the second line, you can see that a new vertex attribute called ‚Äògender‚Äô has been added, which is denoted by ‚Äògender (v/c)‚Äô.\n\n\n4.2 Edge attributes\nSimilar to vertex attributes, edges can also convey different information. The common form of edge attribute is by changing the width of the line connecting the vertices. Therefore, we can convey information such as the number of bus routes between two places or the number of phone calls between two friends by changing the width size.\nIn our case, we can denote edge width to denote the call hours between two people. Similar to the previous case, we can use the set_edge_attr() function. The arguments for this function are similar to set_vertex_attr(), which we saw earlier.\nWe extract the call hour information from the df_rel dataframe that we created earlier.\n\n# Adding 'donations' as an edge attribute\ndf_rel_ig_call &lt;- set_edge_attr(df_rel_ig, \"call_hours\",\n                                 value = df_rel$call_hours)\n\n# Viewing the igraph object\ndf_rel_ig_call\n\nIGRAPH 5574de0 UN-- 10 20 -- \n+ attr: name (v/c), call_hours (e/n)\n+ edges from 5574de0 (vertex names):\n [1] John    --Bob      Eve     --David    John    --Isabelle Isabelle--Charlie \n [5] John    --Henry    David   --Charlie  Eve     --Grace    Isabelle--Grace   \n [9] John    --Grace    Isabelle--Frank    Eve     --Henry    Isabelle--Alice   \n[13] Eve     --Frank    John    --David    Henry   --Frank    Grace   --Alice   \n[17] Bob     --Alice    Charlie --Frank    David   --Frank    Charlie --Henry   \n\n\nSimilar to the previous case, a new edge attribute called ‚Äòdonations‚Äô have been added denoted by ‚Äòdonations (e/n)‚Äô.\n\n\n4.3 One-shot way to add attributes\nWe can use the graph_from_data_frame() function from the {igraph} package to create an igraph object directly if we have the vertex and edge information as separate dataframes. In the function arguments, the d parameter takes the edge list dataframe, where the first two columns contain the edges and the third column represents the edge attribute. Similarly, the vertices parameter takes the dataframe for vertex attributes, where the first column represents the vertices, and the remaining columns represent the vertex attributes.\nIn our case, we can use the df_rel dataframe for the d parameter, where the first two columns represent the edges, and the third column represents the call hours attribute. For the vertices parameter, we can use the df_names dataframe, where the first column represents the vertices (person names), and the second column represents the vertex attribute (gender). By using graph_from_data_frame() function, the vertex and edge attributes will be added to the igraph object automatically.\n\n# Adding the attributes\ndrug_ig_complete &lt;- graph_from_data_frame(d = df_rel, vertices = df_names,\n                                 directed = F)\n\n# Viewing the igraph object\ndrug_ig_complete\n\nIGRAPH 55cf9a2 UN-- 10 20 -- \n+ attr: name (v/c), gender (v/c), call_hours (e/n)\n+ edges from 55cf9a2 (vertex names):\n [1] Bob     --John     David   --Eve      Isabelle--John     Charlie --Isabelle\n [5] Henry   --John     Charlie --David    Eve     --Grace    Grace   --Isabelle\n [9] Grace   --John     Frank   --Isabelle Eve     --Henry    Alice   --Isabelle\n[13] Eve     --Frank    David   --John     Frank   --Henry    Alice   --Grace   \n[17] Alice   --Bob      Charlie --Frank    David   --Frank    Charlie --Henry   \n\n\nWe can also use the functions; vertex_attr() to view the vertex attributes and edge_attr() to view edge attributes.\n\n# Viewing vertex attribute\nvertex_attr(drug_ig_complete)\n\n$name\n [1] \"Alice\"    \"Bob\"      \"Charlie\"  \"David\"    \"Eve\"      \"Frank\"   \n [7] \"Grace\"    \"Henry\"    \"Isabelle\" \"John\"    \n\n$gender\n [1] \"F\" \"M\" \"M\" \"M\" \"F\" \"M\" \"F\" \"M\" \"F\" \"M\"\n\n# Viewing edge attribute\nedge_attr(drug_ig_complete)\n\n$call_hours\n [1]  6  6  5  9  7  1 10  9  5  2  2  9  9  6  6  6  2  9  9  9"
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#filtering-attributes",
    "href": "tutorials/network_analysis/network_analysis.html#filtering-attributes",
    "title": "Introduction to Network Analysis in R",
    "section": "5 Filtering attributes",
    "text": "5 Filtering attributes\nWith different attributes in the network, we can use different filters and see the data differently. Suppose we can want to see what all edges include ‚ÄúJohn‚Äù.\n\n# Viewing all friends of John\nE(drug_ig_complete)[[.inc('John')]]\n\n+ 5/20 edges from 55cf9a2 (vertex names):\n       tail head tid hid call_hours\n1       Bob John   2  10          6\n3  Isabelle John   9  10          5\n5     Henry John   8  10          7\n9     Grace John   7  10          5\n14    David John   4  10          6\n\n\nThe output shows all friends of John.\nLet us also check which friends call for more than 5 hours.\n\n# Viewing relationships with call hours of more than 5 hours\nE(drug_ig_complete)[[call_hours &gt; 5]]\n\n+ 14/20 edges from 55cf9a2 (vertex names):\n      tail     head tid hid call_hours\n1      Bob     John   2  10          6\n2    David      Eve   4   5          6\n4  Charlie Isabelle   3   9          9\n5    Henry     John   8  10          7\n7      Eve    Grace   5   7         10\n8    Grace Isabelle   7   9          9\n12   Alice Isabelle   1   9          9\n13     Eve    Frank   5   6          9\n14   David     John   4  10          6\n15   Frank    Henry   6   8          6\n16   Alice    Grace   1   7          6\n18 Charlie    Frank   3   6          9\n19   David    Frank   4   6          9\n20 Charlie    Henry   3   8          9\n\n\nThere are 12 friendships where people call each other for more than 5 hours."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#visualizing-the-attributes",
    "href": "tutorials/network_analysis/network_analysis.html#visualizing-the-attributes",
    "title": "Introduction to Network Analysis in R",
    "section": "6 Visualizing the attributes",
    "text": "6 Visualizing the attributes\nThere are various ways to visualize both vertex and edge attributes in a network. For numerical attributes, we can change the size of the vertices and the width of the edges. For categorical attributes, we can change the color and shape of the vertices, and the line type and color of the edges.\n\n\n\nDifferent ways to visualize network attributes\n\n\n\n6.1 Vertex attributes\nWith different attributes, we can also visualize them in the network graph in different ways. In the code given below, we assign color values to each of the vertices in the network depending on their gender. Here females will be coded as red and males as blue.\n\n# Setting vertex color to gender\nV(drug_ig_complete)$color &lt;- ifelse(V(drug_ig_complete)$gender == 'F', \"red\", \"blue\")\n\n# Plotting the network\nplot(drug_ig_complete, vertex.label.color = \"black\")\n\n\n\n\n\n\n6.2 Edge attributes\nEdge attributes can be visualized in the network by changing the line type, color or by changing the width. In the code given below, we change the line width depending on the call hours between vertices (or friends).\n\n# Extracting call_hours\ncall_hours &lt;- E(drug_ig_complete)$call_hours\n\n# Plotting the network\nplot(drug_ig_complete, vertex.label.color = \"black\", edge.color = 'grey',\n     edge.width = call_hours)\n\n\n\n\nIn the above graph, the line width is directly proportional to the call hours between the vertices. Higher line width corresponds to a greater number of call hours between friends."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#network-visualizations",
    "href": "tutorials/network_analysis/network_analysis.html#network-visualizations",
    "title": "Introduction to Network Analysis in R",
    "section": "7 Network Visualizations",
    "text": "7 Network Visualizations\nJust like how we can customize different network attributes, we can also change the way our network is visualized based on our data. For instance, if we are interested in sequential events or relationships between family members, then visualizing our network in a tree form would be better than the normal way. The layout argument in the plot() function can take different types of layout functions given in the {igraph} package, providing us with a range of ways to visualize our networks. To create a tree network, we can use the function layout_as_tree(). Below are some popular ways to visualize networks.\n\nTree-like layoutCircleDrL graph layoutFruchterman-Reingold layoutSimple grid layout\n\n\nGood for showing hierarchical relations.\n\n# Plotting the graph in tree form\nplot(drug_ig_complete, vertex.label.color = \"black\", edge.color = 'grey',\n     edge.width = call_hours, layout = layout_as_tree(drug_ig_complete))\n\n\n\n\n\n\nPlace vertices on a circle, in the order of their vertex ids.\n\n# Plotting the graph in circle form\nplot(drug_ig_complete, vertex.label.color = \"black\", edge.color = 'grey',\n     edge.width = call_hours, layout = layout_in_circle(drug_ig_complete))\n\n\n\n\n\n\nDrL is a force-directed graph layout toolbox focused on real-world large-scale graphs.\n\n# Plotting the graph in DrL graph layout\nplot(drug_ig_complete, vertex.label.color = \"black\", edge.color = 'grey',\n     edge.width = call_hours, layout = layout_with_drl(drug_ig_complete))\n\n\n\n\n\n\nPlace vertices on the plane using the force-directed layout algorithm by Fruchterman and Reingold.\n\n# Plotting the graph in Fruchterman-Reingold graph layout\nplot(drug_ig_complete, vertex.label.color = \"black\", edge.color = 'grey',\n     edge.width = call_hours, layout = layout_with_fr(drug_ig_complete))\n\n\n\n\n\n\nThis layout places vertices on a rectangular grid, in two or three dimensions.\n\n# Plotting the graph in grid layout\nplot(drug_ig_complete, vertex.label.color = \"black\", edge.color = 'grey',\n     edge.width = call_hours, layout = layout_on_grid(drug_ig_complete))\n\n\n\n\n\n\n\nAlternatively, if we are unsure of which layout is best for the network at hand, we can use the function layout_nicely() which will assign the best layout automatically.\n\n# Plotting the graph in tree form\nplot(drug_ig_complete, vertex.label.color = \"black\", edge.color = 'grey',\n     edge.width = call_hours, layout = layout_nicely(drug_ig_complete))"
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#directed-networks",
    "href": "tutorials/network_analysis/network_analysis.html#directed-networks",
    "title": "Introduction to Network Analysis in R",
    "section": "8 Directed networks",
    "text": "8 Directed networks\nNetwork graphs can be either undirected or directed. Undirected graphs represent relationships between vertices as simply existing between them, whereas directed graphs indicate that the relationships have a direction. Examples of directed networks include disease outbreak data, family relationships, and school friendship networks.\nLet‚Äôs create a dummy dataset to illustrate directed networks. We‚Äôll generate data on a disease outbreak that occurred in a community of 20 people.\n\n\nCode for making the dummy data\n# Set seed for reproducibility\nset.seed(123)\n\n# create a data frame with two columns to store the information about the spread of the disease\ninfection_df &lt;- data.frame(Infected=character(), Infected_By=character(), stringsAsFactors=FALSE)\n\n# set the number of infected people\nnum_infected &lt;- 20\n\n# initialize the first infected person\ninfected &lt;- c(\"Patient Zero\")\n\n# Loop over the remaining infected people and add them to the data frame\nfor (i in 1:num_infected) {\n  # randomly choose the person infected by the current infected person\n  infected_by &lt;- sample(infected, 1)\n  \n  # add the currently infected person and the person infected by them to the data frame\n  infection_df &lt;- rbind(infection_df, data.frame(Infected=paste(\"Person\", i),\n                                                 Infected_By=infected_by, stringsAsFactors=FALSE))\n  \n  # add the currently infected person to the list of infected people\n  infected &lt;- c(infected, paste(\"Person\", i))\n}\n\n# Interchanging the columns\ninfection_df &lt;- infection_df[, c(\"Infected_By\", \"Infected\")]\n\n# print the first few rows of the resulting data frame\nhead(infection_df)\n\n\n\n\n  \n\n\n\nThe dataframe infection_df contains data on 21 individuals (20 people and 1 patient zero). The infection starts with patient zero, which then spreads across the community. The dataframe has two columns: ‚ÄúInfected_By‚Äù indicates the person who is transmitting the disease, and ‚ÄúInfected‚Äù shows the person who is getting infected by that transmission.\nNow, let‚Äôs plot the above dataframe as a network graph.\n\nlibrary(igraph)\n\n# create a graph object from the dataframe\ngraph &lt;- graph_from_data_frame(infection_df, directed=TRUE)\n\n# plot the graph\nplot(graph, vertex.label.color=\"black\", vertex.size=10, vertex.label.cex=0.7,\n     edge.arrow.size=0.5, main=\"Disease Transmission Network\")\n\n\n\n\nA notable difference from earlier graphs we have seen is that the edges now have an arrowhead indicating their direction. In the graph, the central node is ‚ÄúPatient Zero‚Äù, who is the first infected person in the community. The vertex ‚ÄúPatient Zero‚Äù has two outgoing edges, one going to ‚ÄúPerson 1‚Äù and the other to ‚ÄúPerson 2‚Äù. The arrowheads navigate us to show how the infection starting from ‚ÄúPatient Zero‚Äù infected the whole community. The directed=TRUE argument in the graph_from_data_frame() function tells R to create a directed igraph object. We can also check if a network is directed by using the is.directed() function.\n\nlibrary(igraph)\n\n# create a graph object from the dataframe\ngraph &lt;- graph_from_data_frame(infection_df, directed=TRUE)\n\n# print output\ngraph\n\nIGRAPH 585f043 DN-- 21 20 -- \n+ attr: name (v/c)\n+ edges from 585f043 (vertex names):\n [1] Patient Zero-&gt;Person 1  Patient Zero-&gt;Person 2  Person 2    -&gt;Person 3 \n [4] Person 1    -&gt;Person 4  Person 2    -&gt;Person 5  Person 1    -&gt;Person 6 \n [7] Person 1    -&gt;Person 7  Person 5    -&gt;Person 8  Person 4    -&gt;Person 9 \n[10] Person 3    -&gt;Person 10 Person 5    -&gt;Person 11 Person 8    -&gt;Person 12\n[13] Person 9    -&gt;Person 13 Person 10   -&gt;Person 14 Person 4    -&gt;Person 15\n[16] Person 2    -&gt;Person 16 Person 8    -&gt;Person 17 Person 2    -&gt;Person 18\n[19] Person 7    -&gt;Person 19 Person 6    -&gt;Person 20\n\n\nIn the first line of the output, ‚ÄòDN‚Äô stands for ‚ÄòDirected Network‚Äô, for undirected networks it will be ‚ÄòUN‚Äô which we have seen before. We can also use the function is.directed() to check if the network is directed.\n\nlibrary(igraph)\n\n# create a graph object from the dataframe\ngraph &lt;- graph_from_data_frame(infection_df, directed=TRUE)\n\n# checking if it's directed or not\nis.directed(graph)\n\n[1] TRUE\n\n\n\n8.1 Identifying edges\nSuppose we want to see if there is a disease transmission between person 2 and person 10. To check this we can use the following code;\n\n# checking if an edge exists between person 2 and person 10\ngraph['Person 2', 'Person 10']\n\n[1] 0\n\n\nThe function returned 0, this means that there is no edge between the vertices that we were interested in. The function returns 1 if there exists an edge.\nWe can also see which edges go out from a vertex and go in into the vertex using the incident() function.\n\n# printing all edged going out from patient zero\nincident(graph, 'Patient Zero', mode = c(\"out\"))\n\n+ 2/20 edges from 586cfcd (vertex names):\n[1] Patient Zero-&gt;Person 2 Patient Zero-&gt;Person 1\n\n\nFrom the output, we can see that patient zero infects person 1 and person 2.\n\n# printing all edges coming into person 3\nincident(graph, 'Person 3', mode = c(\"in\"))\n\n+ 1/20 edge from 586cfcd (vertex names):\n[1] Person 2-&gt;Person 3\n\n\nFrom the output, we can see that person 3 was infected by person 2.\n\n# printing all edges connected to person 5\nincident(graph, 'Person 5', mode = c(\"all\"))\n\n+ 3/20 edges from 586cfcd (vertex names):\n[1] Person 2-&gt;Person 5  Person 5-&gt;Person 8  Person 5-&gt;Person 11\n\n\nFrom the output we can see that person 5 was infected by person 2 and thereafter person 5 goes on to infect person 8 and person 11.\nWe can also use the head_of() function to return the set of vertices that are at the beginning of the edges in the input. It gives the list of people who infected others.\n\n# printing all vertices that are connected to an edge\nhead_of(graph, E(graph))\n\n+ 20/21 vertices, named, from 586cfcd:\n [1] Person 1  Person 2  Person 3  Person 4  Person 5  Person 6  Person 7 \n [8] Person 8  Person 9  Person 10 Person 11 Person 12 Person 13 Person 14\n[15] Person 15 Person 16 Person 17 Person 18 Person 19 Person 20\n\n\nHere, patient zero is not included in the output because it is not connected to any edges in the graph. The head_of() function returns only the vertices that are connected to edges in the graph, and since patient zero does not have any outgoing edges, it is not included in the output.\n\n\n8.2 Identifying the neighbors\nTo track the spread of a pandemic in a small community, we need to identify the individuals who were infected by patient zero. This can be accomplished by identifying the immediate neighbors of patient zero using the neighbors() function.\nHere‚Äôs an example of how we can find the infected neighbors in R using the neighbors() function:\n\n# finding the neighbors of patient zero\nneighbors(graph, 'Patient Zero', mode = c(\"all\"))\n\n+ 2/21 vertices, named, from 586cfcd:\n[1] Person 2 Person 1\n\n\nWe can also see which neighbors are common for a particular vertex using the intersect() function.\n\n# finding neighbors of person 1 \nx &lt;- neighbors(graph, 'Person 1', mode = c('all'))\n\n# finding neighbors of person 2\ny &lt;- neighbors(graph, 'Person 2', mode = c('all'))\n\n# finding common neighbors between person 1 and person 2\nintersection(x,y)\n\n+ 1/21 vertex, named, from 586cfcd:\n[1] Patient Zero\n\n\n\n\n8.3 Path length (Geodesic distance)\nA measure to see how well a network is connected is to look at the length of the edges between all pairs of vertices. The length between a vertex and its immediate neighbor will be 1 and this is called path length or also known as geodesic distance. In our network graph, from patient zero to person 1, one connection is required to traverse, which means that the path length between them is 1. Between person 7 and patient zero the path length is 2.\n\n# create a graph object from the dataframe\ngraph &lt;- graph_from_data_frame(infection_df, directed=TRUE)\n\n# plot the graph\nplot(graph, vertex.label.color=\"black\", vertex.size=10, vertex.label.cex=0.7,\n     edge.arrow.size=0.5, main=\"Disease Transmission Network\")\n\n\n\n\nA good way to see how deep the infection has traversed, we have to see the longest path in the network. This longest path is called the diameter of the network. To get the diameter of the network we can use the function farthest_vertices()\n\n# finding the longest path in the network\nfarthest_vertices(graph)\n\n$vertices\n+ 2/21 vertices, named, from 58d688a:\n[1] Patient Zero Person 12   \n\n$distance\n[1] 4\n\n\nTo see how the connections between patient zero and person 12 make up the longest path in the network we can use the function get_diameter()\n\n# finding the longest path in the network\nget_diameter(graph)\n\n+ 5/21 vertices, named, from 58d688a:\n[1] Patient Zero Person 2     Person 5     Person 8     Person 12   \n\n\nTo calculate the geodesic distances of all vertices from a particular vertex, we can use the function distances().\n\n# finding geodesic distances of all vertices from the vertex 'Patient Zero'\ndistances(graph, \"Patient Zero\")\n\n             Patient Zero Person 2 Person 1 Person 5 Person 4 Person 3 Person 8\nPatient Zero            0        1        1        2        2        2        3\n             Person 9 Person 10 Person 7 Person 6 Person 11 Person 12 Person 13\nPatient Zero        3         3        2        2         3         4         4\n             Person 14 Person 15 Person 16 Person 17 Person 18 Person 19\nPatient Zero         4         3         2         4         2         3\n             Person 20\nPatient Zero         3\n\n\nFrom the output, we can see how distance each vertex is away from patient zero. Person 1 and Person 2 seem to be immediate targets of disease transmission starting from patient zero and persons 12,13,14 and person 17 were the last people to be infected in the whole community.\nWe can also identify vertices that are reachable with N steps from a vertex of interest. Suppose we want to know vertices from patient zero that are reachable within 2 connections or 2 steps. We can use the ego() function for this task.\n\n# finding vertices that are at min. 2 edges away from patient zero\nego(graph, 2, 'Patient Zero', mode = c('all'))\n\n[[1]]\n+ 10/21 vertices, named, from 58d688a:\n [1] Patient Zero Person 2     Person 1     Person 5     Person 3    \n [6] Person 16    Person 18    Person 4     Person 7     Person 6    \n\n\nLet us plot the network to visualize how far each vertex is from ‚ÄòPatient Zero‚Äô. First we use make_ego_graph() function to get a sub-graph containing all neighbors of Patient Zero which is reachable within the diameter of the network. This essentially includes all vertices that are connected to Patient Zero. Then we calculate the distances of vertices from Patient Zero. The diameter of this network is 4, but we add 4+1 colors, as ‚ÄòPatient Zero‚Äô has zero distance, we suggest 5 colors so that ‚ÄòPatient Zero‚Äô has its color. In the final plot, we have vertex labels to denote the geodesic distances from Patient Zero.\n\nlibrary(igraph)\n\n# Make an ego graph\ngraph_ego &lt;- make_ego_graph(graph, diameter(graph), nodes = 'Patient Zero', mode = c(\"all\"))[[1]]\n\n# Get a vector of geodesic distances of all vertices from vertex Patient Zero \ndists &lt;- distances(graph_ego, \"Patient Zero\")\n\n# Create a color palette of length equal to the maximal geodesic distance plus one.\ncolors &lt;- c(\"black\", \"red\", \"orange\", \"blue\", \"dodgerblue\")\n\n# Set color attribute to vertices of network graph_ego\nV(graph_ego)$color &lt;- colors[dists+1]\n\n# Visualize the network based on geodesic distance from patient zero.\nplot(graph_ego, \n     vertex.label = dists, \n     vertex.label.color = \"white\",\n     vertex.label.cex = .6,\n     edge.color = 'black',\n     vertex.size = 15,\n     edge.arrow.size = .05,\n     main = \"Geodesic Distances from Patient Zero\"\n)"
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#measures-of-network-structure",
    "href": "tutorials/network_analysis/network_analysis.html#measures-of-network-structure",
    "title": "Introduction to Network Analysis in R",
    "section": "9 Measures of network structure",
    "text": "9 Measures of network structure\n\n9.1 Degree\nDegree describes how many edges a vertex has. In undirected networks, the degree of a vertex is simply the sum of edges connecting that vertex. But for directed networks, since we have some edges going out from a vertex and some edges going into the same vertex, we essentially have out-degrees and in-degrees. The figure given below illustrates this.\n\n\n\nIn-degrees and Out-degrees\n\n\nVertices with a high number of connections or with a high degree can be important. We can check the degrees of all vertices with the degree() function.\n\n# finding the degrees of all vertices in the network\ndegree(graph, mode = ('all'))\n\nPatient Zero     Person 2     Person 1     Person 5     Person 4     Person 3 \n           2            5            4            3            3            2 \n    Person 8     Person 9    Person 10     Person 7     Person 6    Person 11 \n           3            2            2            2            2            1 \n   Person 12    Person 13    Person 14    Person 15    Person 16    Person 17 \n           1            1            1            1            1            1 \n   Person 18    Person 19    Person 20 \n           1            1            1 \n\n\nFrom the results we can see that person 2 and person 1 have high degrees which suggests that they are important in the network. Essentially, person 1 and person 2 jump-started the infection in the community.\n\n\n9.2 Betweenness\nThis is an index of how frequently the vertex lies on the shortest paths between any two vertices in the network. It can be thought of as how critical the vertex is to the flow of information through a network. Individuals with high betweenness are key bridges between different parts of a network. We can calculate the betweenness of each vertices using the betweenness() function.\n\n# finding the betweenness of all vertices in the network\nbetweenness(graph, directed = T)\n\nPatient Zero     Person 2     Person 1     Person 5     Person 4     Person 3 \n           0           10            8            8            6            4 \n    Person 8     Person 9    Person 10     Person 7     Person 6    Person 11 \n           6            3            3            2            2            0 \n   Person 12    Person 13    Person 14    Person 15    Person 16    Person 17 \n           0            0            0            0            0            0 \n   Person 18    Person 19    Person 20 \n           0            0            0 \n\n\nFrom the output, person 2 has the most number of nodes branching outwards followed by person 1 and person 5. This might suggest that person 1, person 2, and person 5 might be playing an important role in spreading the disease.\nWe can also use the nromalized = T argument to get the normalized betweenness values.\n\n# finding the betweenness of all vertices in the network\nbetweenness(graph, directed = T, normalized = T)\n\nPatient Zero     Person 2     Person 1     Person 5     Person 4     Person 3 \n 0.000000000  0.026315789  0.021052632  0.021052632  0.015789474  0.010526316 \n    Person 8     Person 9    Person 10     Person 7     Person 6    Person 11 \n 0.015789474  0.007894737  0.007894737  0.005263158  0.005263158  0.000000000 \n   Person 12    Person 13    Person 14    Person 15    Person 16    Person 17 \n 0.000000000  0.000000000  0.000000000  0.000000000  0.000000000  0.000000000 \n   Person 18    Person 19    Person 20 \n 0.000000000  0.000000000  0.000000000 \n\n\n\n\n9.3 Eigenvector centrality\nCentrality is a measure of an individual vertexe‚Äôs structural importance in a group based on its network position. Vertices with high eigenvector centrality are those that are connected to many other vertices but also to vertices that are themselves highly connected to other vertices. Eigenvector centrality can be calculated using the function eigen_centrality(). The function returns many other values also, but for now we are just focusing on the centrality value which is saved as vector in the output.\n\n# finding the eigenvector centrality of all vertices in the network\neigen_centrality(graph)$vector\n\nPatient Zero     Person 2     Person 1     Person 5     Person 4     Person 3 \n  0.64109893   1.00000000   0.62990379   0.63313313   0.37411961   0.48145630 \n    Person 8     Person 9    Person 10     Person 7     Person 6    Person 11 \n  0.36061817   0.17408817   0.22403489   0.29311159   0.29311159   0.24903370 \n   Person 12    Person 13    Person 14    Person 15    Person 16    Person 17 \n  0.14184391   0.06847505   0.08812086   0.14715450   0.39333544   0.14184391 \n   Person 18    Person 19    Person 20 \n  0.39333544   0.11529118   0.11529118 \n\n\nHere in the output, Person 1,2,5, and patient zero are highly influential vertices in the network.\nWe can also plot the network with eigenvector centrality as a vertex size attribute.\n\n# Saving eigenvector centrality values\ngraph_eigen_vector &lt;- eigen_centrality(graph)$vector\n\n# plotting the network\nplot(graph,\n     vertex.label.color = \"black\",\n     vertex.label.cex = 0.6,\n     vertex.size = 25*(graph_eigen_vector),\n     edge.color = 'grey',\n     main = \"Disease Outbreak Network\")\n\n\n\n\n\n\n9.4 Density\nDensity is the simplest way of measuring the overall structure of the network. Density is the proportion of edges that do exist in a network out of all those that potentially could exist between every pair of vertices in the network. A density value of 1 would suggest all possible vertices are present in the network, which would also mean that the network is highly interconnected. To calculate the density we use the function edge_density().\n\n# finding the density of the network\nedge_density(graph)\n\n[1] 0.04761905\n\n\nThe value ~ 0.05 (approximately) suggests that 5% of the total possible vertices are present in our network.\n\n\n9.5 Average path length\nThe average path length is the mean of the lengths of the shortest paths between all pairs of vertices in the network. We can use the function mean_distance() to the graph and instruct the function whether the graph is undirected or directed to find the value. Lower values of average path length suggest that the network is highly interconnected.\n\n# finding the average path length of the network\nmean_distance(graph, directed = T)\n\n[1] 1.981132"
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#network-randomization-test",
    "href": "tutorials/network_analysis/network_analysis.html#network-randomization-test",
    "title": "Introduction to Network Analysis in R",
    "section": "10 Network randomization test",
    "text": "10 Network randomization test\nSo far, we have seen the different kinds of measures used to determine the overall structure of the network. To check if the obtained value is unique to the network data at hand, we randomize our network data and calculate measures of network structure and compare them with the real values. To randomize our network, we use the function erdos.renyi.game(), which uses a particular algorithm to generate networks with a set probability of creating edges between the vertices in the network.\n\n# generating a random network graph\nerdos.renyi.game(n = gorder(graph), p.or.m = edge_density(graph), type = 'gnp')\n\nIGRAPH 5972602 U--- 21 8 -- Erdos-Renyi (gnp) graph\n+ attr: name (g/c), type (g/c), loops (g/l), p (g/n)\n+ edges from 5972602:\n[1]  4-- 5  2-- 9 12--13  1--16 15--16 14--19 13--20  4--21\n\n\nHere we specify the number of vertices (given by n = gorder(graph)) and the probability of a given edge being connected between any two vertices, given by the argument p.or.m = edge_density(graph). The type = 'gnp' argument tells R that the graph has ‚Äòn‚Äô vertices, and for each edge, the probability that it is present in the graph is ‚Äòp‚Äô.\nLet us calculate the average path length for this randomized network that we just created.\n\n# setting seed for reproducibility\nset.seed(123)\n\n# generating a random network graph\nrandom_graph &lt;- erdos.renyi.game(n = gorder(graph), p.or.m = edge_density(graph), type = 'gnp')\n\n# finding the average length of the random network\nmean_distance(random_graph)\n\n[1] 1.904762\n\n\nFrom the result, we can see that the average path length is slightly less than our original average path length. However, this is not enough to conclude that the original value we got is different from the random value. Therefore, let us repeat this randomization 1000 times, calculate the average path length each time, and check how many networks are greater or lesser than the average path length we originally calculated. This way, we can confidently say whether our original value is particularly different from that of the random values. This process is called a network randomization test. In summary, a randomization test tells us whether the features of our original network are particularly unusual or not.\nSo the pipeline that we will be following for network randomization is the following:\n\n\n\n\n%%{init: {'theme': 'default'}}%%\ngraph TD\n    A[Generating 1000 random networks with the same number of vertices and with a similar edge density as that of the original network]\n    B[Calculate the average path length of the original network]\n    C[Calculate the average path length for all 1000 randomly generated networks]\n    D[Determine how many random networks are greater than or less than the average path length of the original network]\n    A--&gt;B\n    B--&gt;C\n    C--&gt;D\n\n\n\n\n\nNow let us calculate generate and calculate the average path length for 1000 random networks. We are using the graph igraph object which we created last time.\n\n# set seed for reproducibility\nset.seed(123)\n\n# generating an empty list with 1000 entries\n1list_empty &lt;- vector('list', 1000)\n\n# generating 1000 random networks and saving them in the empty list\n2for(i in 1:1000){\n  list_empty[[i]] &lt;- erdos.renyi.game(n = gorder(graph), p.or.m = edge_density(graph),\n                              type = \"gnp\")\n}\n\n# Calculate the average path length of 1000 random graphs\n3list_empty_avg_path &lt;- unlist(lapply(list_empty, mean_distance, directed = T))\n\n\n1\n\nFirst we generate an empty list with 1000 NULL entries which we will be using to store our 1000 random networks.\n\n2\n\nThe for loop generates 1000 random networks using the Erd≈ës-R√©nyi model with the same number of vertices and edge density as the original network (which is got from the graph object, which we had saved earlier). The networks are then saved in the empty list created in the previous step.\n\n3\n\nThis line uses the lapply() function to apply the mean_distance() function to each element in the list_empty list, which calculates the average path length for each of the 1000 random networks. The unlist() function is used to convert the resulting list of average path lengths into a vector.\n\n\n\n\n\n# Plot the distribution of average path lengths\nhist(list_empty_avg_path) \nabline(v = average.path.length(graph, directed = T), col = \"red\", lty = 3, lwd = 2)\n\n\n4\n\nThe above code generates a histogram of the distribution of average path lengths for the 1000 random networks using the hist() function. The second line adds a vertical line at the average path length of the original network using the abline() function. Finally, the last line calculates the proportion of random networks that have an average path length less than that of the original network using the mean() function. The comparison is done using the &lt; operator and the average.path.length() function calculates the average path length of the original network.\n\n\n\n\n\n\n# Calculate the proportion of graphs with an average path length lower than our observed\n4mean(list_empty_avg_path &lt; average.path.length(graph, directed = T))\n\n[1] 0.534\n\n\nFrom the histogram we can see that our original average path length, which is represented as a red dotted vertical line lies among the randomly generate list average path length. The high interconnectedness we observe in our original data may be due to random chance alone."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#network-substructures",
    "href": "tutorials/network_analysis/network_analysis.html#network-substructures",
    "title": "Introduction to Network Analysis in R",
    "section": "11 Network substructures",
    "text": "11 Network substructures\nUntil now we have looked at features that describe the network structure. In this section, we will learn about microstructural features of social networks that can be informative as to how a network functions.\n\n11.1 Triangles (triads)\nIn a network, a triangle is a collection of three nodes (or vertices) that are all connected. A high number of triangles in a network indicates that nodes tend to be highly interconnected, forming dense clusters or communities. In contrast, a low number of triangles suggests that nodes are more sparsely connected and less likely to form tightly-knit groups.\nWe can use the function triangles() to show all the triangles in the network. Let us the df_rel_ig igraph object that we created at the beginning of this tutorial.\n\n# plot the network\nplot(df_rel_ig)\n\n\n\n# show all triangles in the network.\nmatrix(triangles(df_rel_ig), nrow = 3)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]    1    5    5    9    9    9    9\n[2,]    5    8    9    3    3    4    6\n[3,]    8   10    6    4    7    6    7\n\n# counting triangles with Isabelle as the vertex\ncount_triangles(df_rel_ig, vids= 'Isabelle')\n\n[1] 3\n\n\nThere are 3 triangles with Isabelle as the vertex.\n\n\n11.2 Transitivity\nThe number of triangles in a network can be quantified using a metric called the clustering coefficient (transitivity). The clustering coefficient of a node is defined as the fraction of pairs of the node‚Äôs neighbors that are connected by an edge.\nGlobal transitivity (also known as the global clustering coefficient) is a measure of the proportion of triangles in the entire network. It is the ratio of the total number of triangles in a network to the total number of possible triangles. High global transitivity indicates that the nodes in the network are highly interconnected, forming tightly-knit clusters.\nTo calculate the global transitivity, we can use the function transitivity()\n\n# Calculating global transitivity \ntransitivity(df_rel_ig)\n\n[1] 0.328125\n\n\nThe value 0.32 means that 32% of the total possible triangles are represented in the given network.\nLocal transitivity (also known as local clustering coefficient) is a measure of the transitivity of individual nodes or small groups of nodes in the network. It is the proportion of triangles that exist among the neighbors of a given node to the total number of possible triangles among those neighbors. A high local transitivity indicates that a node‚Äôs neighbors are highly interconnected, forming a cluster or clique, while a low local transitivity suggests that the node‚Äôs neighbors are not well-connected to each other.\nTo calculate the local transitivity, we again use the function transitivity() but also include the following arguments to specify the individual vertex (vids =) and also specify that we want to calculate the local transitivity (type = 'local')\n\n# calculating local transitivity around Isabelle\ntransitivity(df_rel_ig, vids = 'Isabelle', type = 'local')\n\n[1] 0.3\n\n\nThe value 0.3 means that, 30% of the total possible triangles with Isabelle as a vertex is represented in the given network.\nIn summary, global transitivity measures the extent of clustering in the entire network, while local transitivity measures the extent of clustering around individual nodes or small groups of nodes in the network.\n\n\n11.3 Cliques\nA clique is a subset of nodes in a network where each node is directly connected to every other node in the subset. In other words, a clique is a fully connected subgraph, where every node in the subset has a direct edge to every other node in the subset.\nWe can use the largest_cliques() function to find the largest clique in the network\n\n# finding the largest clique in the network\nlargest_cliques(df_rel_ig)\n\n[[1]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Alice    Isabelle Grace   \n\n[[2]]\n+ 3/10 vertices, named, from 5574de0:\n[1] David Frank Eve  \n\n[[3]]\n+ 3/10 vertices, named, from 5574de0:\n[1] David   Frank   Charlie\n\n[[4]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Isabelle Frank    Charlie \n\n[[5]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Isabelle John     Grace   \n\n[[6]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Henry Frank Eve  \n\n[[7]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Henry   Frank   Charlie\n\n\nThere are 7 cliques tied with 3 vertices.\nWe can use max_cliques() function to showcase the biggest cliques for each number of vertexes (min. 2 vertices are required for a clique)\n\n# finding maximum cliques\nmax_cliques(df_rel_ig)\n\n[[1]]\n+ 2/10 vertices, named, from 5574de0:\n[1] Bob   Alice\n\n[[2]]\n+ 2/10 vertices, named, from 5574de0:\n[1] Bob  John\n\n[[3]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Alice    Isabelle Grace   \n\n[[4]]\n+ 3/10 vertices, named, from 5574de0:\n[1] David Frank Eve  \n\n[[5]]\n+ 3/10 vertices, named, from 5574de0:\n[1] David   Frank   Charlie\n\n[[6]]\n+ 2/10 vertices, named, from 5574de0:\n[1] David John \n\n[[7]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Isabelle Frank    Charlie \n\n[[8]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Isabelle John     Grace   \n\n[[9]]\n+ 2/10 vertices, named, from 5574de0:\n[1] John  Henry\n\n[[10]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Henry Frank Eve  \n\n[[11]]\n+ 3/10 vertices, named, from 5574de0:\n[1] Henry   Frank   Charlie\n\n[[12]]\n+ 2/10 vertices, named, from 5574de0:\n[1] Grace Eve  \n\n\n The figure given above illustrates a network featuring both closed and open triangles. In the figure, A-K-F, A-K-B, E-G-L are some of the closed triangles in the networks, similarly, J-H-I, E-G-J, E-A-H are open triangles with only two edges, H-I,C and F-C,H are open triangles with only one edge and E,J,I and H,E,C are some of the open triangles with no edges. Here A-C-F-K forms a clique. It should also be noted that a clique of size 3 (3 vertices) is a triangle."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#identifying-special-relationships",
    "href": "tutorials/network_analysis/network_analysis.html#identifying-special-relationships",
    "title": "Introduction to Network Analysis in R",
    "section": "12 Identifying special relationships",
    "text": "12 Identifying special relationships\nIn this section, we will further explore the partitioning of networks into sub-networks and determine which vertices are more highly related to one another than others. We will also develop visualization methods by creating three-dimensional visualizations.\n\n12.1 Assortativity\nLet us take our initial dataset. In this dataset, we have illustrated the relationships between 20 people. One of the interesting questions that we can ask about this network is whether individuals preferably make friends with the same gender or with their same age groups. This is determined by the measure called ‚ÄòAssortativity‚Äô. Assortativity in network analysis refers to the tendency of nodes in a network to be connected to other nodes with similar properties or characteristics. In other words, nodes in an assortative network tend to be connected to other nodes that are like them in some way, such as having similar degrees, attributes, or other characteristics. In our networks, people might be preferably chosen to be in a relationship with other people who are of the same gender or age group.\nThe function to calculate the assortativity of a network is assortativity(). This function takes in two arguments, first is the igraph object and second is the attribute that is common among the vertices. In our dataset, let us use gender as the common attribute. If the common attribute is numeric, then we can directly input the values, but if it‚Äôs categorical we have to first change it into numeric factor. In our case, we have to change the values of M and F to 0 and 1.\n\n# setting gender as the vertex attribute\ndf_rel_ig_gender &lt;- set_vertex_attr(df_rel_ig, \"gender\", value = df_names$gender)\n\n# plotting our network\nplot(df_rel_ig_gender)\n\n\n\n# converting gender values to numeric values\ngender_value &lt;- as.numeric(factor(V(df_rel_ig_gender)$gender))\n\n# calculating the assortativity\nassortativity(df_rel_ig_gender, gender_value)\n\n[1] -0.1027569\n\n\nAssortativity value ranges from -1 to 1. A value of -1 means that the vertices actively avoid forming relationships with similar vertices, 0 means that the vertices have no preferences in making connections and 1 means that the vertices actively like to form relationships with similar vertices. Here, we have a value of -0.1 which is approximately close to zero, which means the vertices form relationships with each other irrespective of their gender.\n\n\n12.2 Assortativity degree\nAnother similar value is the assortativity degree. It refers to the extent to which nodes in a network tend to be connected to other nodes with similar degrees. It measures the correlation between the degrees of connected nodes in a network and can be quantified using the degree assortativity coefficient.\nThe degree assortativity coefficient ranges from -1 to 1, with values close to 1 indicating a highly assortative network, where nodes with high degrees tend to be connected to other nodes with high degrees, and nodes with low degrees tend to be connected to other nodes with low degrees. Values close to -1 indicate a disassortative network, where high-degree nodes tend to be connected to low-degree nodes and vice versa. Finally, a value close to 0 indicates a random network, where there is no particular correlation between the degrees of connected nodes.\nWe can use the function assortativity.degree() to calculate the assortativity degree.\n\n# calculating the assortativity degree\nassortativity.degree(df_rel_ig_gender, directed = F)\n\n[1] 0.01639344\n\n\nWe have a value of 0.01 which is again close to 0, which means that the vertices in our network do not care about the degree of other vertices while forming connections.\n\n\n12.3 Reciprocity\nReciprocity in networks refers to the extent to which pairs of nodes in a network have mutual connections. In other words, if node A is connected to node B and node B is also connected to node A, then there is reciprocity between the two nodes.\nReciprocity can be measured using the reciprocity coefficient, which is the ratio of the number of reciprocated edges (i.e., edges that connect two nodes that are mutually connected) to the total number of edges in the network. The reciprocity coefficient ranges from 0 to 1, with higher values indicating a greater degree of reciprocity in the network.\nReciprocity is an important concept in network analysis because it can have significant implications for the structure and dynamics of networks. Networks with high levels of reciprocity tend to have stronger ties between nodes and be more densely connected, while networks with low levels of reciprocity tend to be more sparsely connected and have weaker ties between nodes.\nReciprocity can be calculated using the function reciprocity().\n\n# calculating the reciprocity\nreciprocity(df_rel_ig_gender)\n\n[1] 1\n\n\nSince our original graph object is undirected, we get the value 1."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#community-detection",
    "href": "tutorials/network_analysis/network_analysis.html#community-detection",
    "title": "Introduction to Network Analysis in R",
    "section": "13 Community detection",
    "text": "13 Community detection\nNetworks sometimes form clusters or communities where a group of vertices with similar attributes form close connections compared to others. This is referred to as community defection. Community detection in network analysis refers to the process of identifying groups or clusters of nodes that are densely connected within a larger network. These groups are often referred to as communities, clusters, or modules, and can provide insights into the organization and function of the network.\nThere are many algorithms to identify communities in a network. We will be looking at two of them; greedy optimization algorithm (cluster_fast_greedy()) and Girvan-Newman algorithm (cluster_edge_betweenness()).\nThe greedy optimization algorithm iteratively merges nodes or communities in the network to maximize the modularity of the resulting partition. Modularity is a measure of the density of edges within communities compared to the expected density of edges in a randomized network. Maximizing modularity is a common criterion for community detection in networks because it reflects the degree to which the network is organized into cohesive and densely connected communities. The greedy optimization algorithm starts with each node in its community and iteratively merges the most similar communities until the modularity of the resulting partition can no longer be increased.\nThe Girvan-Newman algorithm iteratively removes the edges with the highest betweenness centrality in the network, which are the edges that are most frequently traversed by the shortest paths between pairs of nodes in the network. The removal of these edges gradually breaks the network into smaller and smaller clusters or communities. It is widely used for community detection in complex networks because it is relatively fast and can detect communities of varying sizes and shapes.\n\n13.1 Fast-greedy community detection\nUsing the cluster_fast_greedy() function let us find if there exist any communities in our dataset df_rel_ig_gender\n\n# Perform fast-greedy community detection on the network graph\nfg = cluster_fast_greedy(df_rel_ig_gender)\n\nUsing the sizes() function, we can see the number of members in each of the communities.\n\n# Determine the sizes of each community\nsizes(fg)\n\nCommunity sizes\n1 2 \n6 4 \n\n\nThe output shows that Community 1 has 6 members and Community 2 has 4 members.\nThe membership() function shows to which community each of the members belongs to.\n\n# Determine which individuals belong to which community\nmembership(fg)\n\n    John      Bob      Eve    David Isabelle  Charlie    Henry    Grace \n       1        1        1        2        1        2        2        1 \n   Frank    Alice \n       2        1 \n\n\nTo plot the community structure, we can simply use input from the ‚Äòcommunities‚Äô object that we have created using the algorithm function.\n\n# Plot the community structure of the network\nplot(fg, df_rel_ig_gender)\n\n\n\n\n\n\n13.2 Edge-betweenness community detection\nSimilar to the earlier case, we use the cluster_edge_betweenness () function to find the communities in our dataset df_rel_ig_gender.\n\n# Perform edge-betweenness community detection on the network graph\neb = cluster_edge_betweenness(df_rel_ig_gender)\n\n# Determine sizes of each community\nsizes(eb)\n\nCommunity sizes\n1 2 3 \n3 2 5 \n\n\nThe output shows that Community 1 has 3 members community 2 has 2 members and Community 3 has 5 members.\n\n# Determine which individuals belong to which community\nmembership(eb)\n\n    John      Bob      Eve    David Isabelle  Charlie    Henry    Grace \n       1        2        3        3        1        3        3        1 \n   Frank    Alice \n       3        2 \n\n\nLet us now compare both graphs.\n\n# Plotting both graphs side-wise\npar(mfrow = c(1, 2))\nplot(fg, df_rel_ig_gender, main= \"Fast-greedy\")\nplot(eb, df_rel_ig_gender, main= \"Edge-betweenness\")\n\n\n\n\nIn general, the cluster-fast greedy algorithm is faster and more scalable than the cluster_edge_betweenness algorithm and is a good choice for large networks with many nodes and edges. It can detect communities of varying sizes and shapes and is less sensitive to noise.\nOn the other hand, the cluster edge betweenness algorithm is more computationally expensive and may be more appropriate for smaller networks. It can also be useful for identifying communities that are not well-separated or that overlap with each other."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#interactive-network-visualizations",
    "href": "tutorials/network_analysis/network_analysis.html#interactive-network-visualizations",
    "title": "Introduction to Network Analysis in R",
    "section": "14 Interactive network visualizations",
    "text": "14 Interactive network visualizations\nOne of the coolest things about network analysis is the visualization process. Up until now, we have only plotted static graphs. In this section, we will plot networks that can be physically (with a mouse pointer) interacted with using the {threejs} package in R. We will only cover the basics of 3D visualization.\nPlotting the graph is as simple as using the function graphjs() from the {threejs} package.\n\nif (!require(threejs)) install.packages('threejs')\nlibrary(threejs)\n\n# plotting our previous network\ngraphjs(df_rel_ig_gender)\n\n\n\n\n\nWe can finally move our networks!\nLet us re-plot the network we got after using the fast-greedy community algorithm. Since the size of the network is 2, we denote two colors to represent each of the two communities. You can also hover over the vertices to display its name on top.\n\nlibrary(threejs)\n\n# Perform fast-greedy community detection on network graph\nfg = cluster_fast_greedy(df_rel_ig_gender)\n\n# Create an object 'i' containing the memberships of the fast-greedy community detection\ni &lt;-  membership(fg)\n\n# Check the number of different communities\nsize_fg &lt;- sizes(fg)\n\n# Add a color attribute to each vertex, setting the vertex color based on community membership\ng &lt;- set_vertex_attr(df_rel_ig_gender, \"color\", value = c(\"yellow\", \"blue\")[i])\n\n# Plot the graph using threejs\ngraphjs(g, vertex.label = df_names$name)\n\n\n\n\n\nWith that, we have successfully learned the basics of network analysis using R."
  },
  {
    "objectID": "tutorials/network_analysis/network_analysis.html#conclusion",
    "href": "tutorials/network_analysis/network_analysis.html#conclusion",
    "title": "Introduction to Network Analysis in R",
    "section": "15 Conclusion",
    "text": "15 Conclusion\nIn short, we learned about;\n\nWhat is a network and what are the different ways to denote the network data?\nHow to convert the network data into an igraph object to analyze and plot the network via the {igraph} package in R.\nWhat are vertex and edge attributes and how do add them to the igraph object?\nHow do filter attributes and how can we visualize them?\nWhat are the different types of network visualizations?\nWhat are directed networks?\nWhat is path length?\nWhat are the different measures of network structure?\nWhat is network randomization test and why is its purpose?\nwhat are the different network substructures?\nHow to identify special relationships in the network?\nHow to find communities in a network?\nHow to visualize the network in 3D using the {threejs} package ?\n\nThis tutorial is based on my notes from attending the ‚ÄòNetwork Analysis in R‚Äô course on DataCamp. As with any personal notes, there may be mistakes and some of the information presented may not accurately reflect the topic. If you notice any errors, please feel free to leave a comment in the section below.\nI am particularly interested in learning about 3D visualization techniques, which I find to be fascinating. If I am able to learn more about this topic, I will write a sequel to this tutorial. Thank you for reading!"
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html",
    "href": "tutorials/stat_basic/inter_reg.html",
    "title": "Intermediate Regression in R",
    "section": "",
    "text": "require(\"https://cdn.jsdelivr.net/npm/juxtaposejs@1.1.6/build/js/juxtapose.min.js\")\n  .catch(() =&gt; null)"
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#introduction",
    "href": "tutorials/stat_basic/inter_reg.html#introduction",
    "title": "Intermediate Regression in R",
    "section": "\n1 Introduction",
    "text": "1 Introduction\nThis tutorial is a sequel to the previous tutorial which covered the basics of regression. Here we will extend our knowledge to build models using more than one explanatory variable, how to visualise them and how to interpret the model summaries."
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#linear-regression-using-multiple-variables",
    "href": "tutorials/stat_basic/inter_reg.html#linear-regression-using-multiple-variables",
    "title": "Intermediate Regression in R",
    "section": "\n2 Linear regression using multiple variables",
    "text": "2 Linear regression using multiple variables\nIn the previous tutorial, we have seen how to build a linear model with a single exploratory variable. Now we will see how to build linear models using two variables.\nWe will use the HorsePrices dataset from the Stat2Data package in R. The dataset contains the price and related characteristics of horses listed for sale on the internet. Using the dataset, we will see how the price of a horse (Price) changes according to its age (Age) and also dues to its sex (Sex) using a linear model. Here, we have a numerical and a categorical variable as the explanatory variables. First, let us plot the data.\n\nif (!require(Stat2Data)) install.packages('Stat2Data')\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting the data\nHorsePrices %&gt;% ggplot(aes(Age, Price, col = Sex)) + geom_point() +\n  labs(title = \"Change in the price of the horse as a function of age and sex?\",\n       x = \"Age (years)\",\n       y = \"Price (dollars)\") +\n  theme_bw()\n\n\n\n\nAt a glance, for intermediately aged horses, the price is high compared to the rest of the age range. Also, female horses seem to have to be priced less compared to males. Let us see if that is the case by building a linear model. To predict the response variable using multiple explanatory variables, we will append them using the + sign.\n\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model\nmodel_lm &lt;- lm(Price ~ Age + Sex, data = HorsePrices)\n\n# Printing model output\nsummary(model_lm)\n\n\nCall:\nlm(formula = Price ~ Age + Sex, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24988 -10296  -1494   8286  27653 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  18199.7     4129.3   4.407 6.03e-05 ***\nAge           -220.1      393.8  -0.559    0.579    \nSexm         17008.6     3639.6   4.673 2.52e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12540 on 47 degrees of freedom\nMultiple R-squared:  0.3283,    Adjusted R-squared:  0.2997 \nF-statistic: 11.48 on 2 and 47 DF,  p-value: 8.695e-05\n\n\nUnlike the earlier case with one explanatory variable, we now have two coefficient values and an intercept value. We will soon see how to interpret the coefficient values. But for now, we can see that their age does not significantly explain the variance seen in price but sex does.\nNow let us try plotting the model. Instead of using the geom_smooth() function in the ggplot2 package in R, we will use the geom_parallel_slopes() function from the moderndive package in R. We will later see why we did not use the geom_smooth() function.\n\nif (!require(moderndive)) install.packages('moderndive')\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting the data\nHorsePrices %&gt;% ggplot(aes(Age, Price, col = Sex)) + geom_point() +\n  labs(title = \"Change in the price of the horse as a function of age and sex?\",\n       x = \"Age (years)\",\n       y = \"Price (dollars)\") +\n  geom_parallel_slopes() +\n  theme_bw()\n\n\n\n\nThere is a difference in price between male and female horses but their prices are not changing across age. This is exactly what we saw in the model summary results. You can also see that the lines are parallel to each other."
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#understanding-the-summary-of-a-multivariate-linear-model",
    "href": "tutorials/stat_basic/inter_reg.html#understanding-the-summary-of-a-multivariate-linear-model",
    "title": "Intermediate Regression in R",
    "section": "\n3 Understanding the summary of a multivariate linear model",
    "text": "3 Understanding the summary of a multivariate linear model\nGiven below is the summary output of the multivariate model we built earlier.\n\n\nIn section 1, we have the model formula we used for building the model.\nIn section 2, we have the first quartile, median and third quartile of the residuals of the model. This tells us whether the residuals are normally distributed if the median value is close to zero and, the first and third quartile values are approximately the same in magnitude.\nIn section 3, we have the coefficient values of the exploratory variables. Please remember that the variable ‚Äòsex‚Äô has two levels; female and male. By default, R calculates the intercept value for the first level in the first categorical response variable in the model formula. Here, in our case, that first level is female. Therefore, the intercept value gives us the average price value for female horses when age is 0. So here, the y-intercept value of 18199 corresponds to the average price of female horses when age is 0. The second value is the slope of Age, which is -220.1, which corresponds to a decrease in the price by a value of 220.1 dollars when the age of the horse increases by 1 year. The third value corresponds to the difference in the y-intercept value for males as compared to the main intercept value, which is for females and is 18199. In short, it tells us that male horses with age 0 are priced 17008 dollars more on average when compared to female horses. If we want to see the individual averages of the levels in the categorical exploratory variables, then we should use -1 in the model formula, which is something we saw in the earlier tutorial.\nSection 4 contains metrics which assess model performance. In the earlier tutorial, we saw what residual standard error and R squared values means. Adjusted R square value is a metric similar to R squared value but is calculated for models with multiple explanatory variables. Like R squared, it tells us how much variance seen in the response variable is explained by the explanatory variables and the value lies between 0 and 1. Here we have around 30% variance seen in price explained by both age and sex of the horses."
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#making-predictions-using-a-multivariate-linear-model",
    "href": "tutorials/stat_basic/inter_reg.html#making-predictions-using-a-multivariate-linear-model",
    "title": "Intermediate Regression in R",
    "section": "\n4 Making predictions using a multivariate linear model",
    "text": "4 Making predictions using a multivariate linear model\nAs seen earlier, we can use the models to predict values outside the dataset. We will use the expand_grid() function to make a data frame containing the values for which we want the model to predict.\nLet us predict the price of a 20-year-old male horse.\n\nlibrary(Stat2Data)\nlibrary(tidyr)\n\ndata(\"HorsePrices\")\n\n# Building a linear model\nmodel_lm &lt;- lm(Price ~ Age + Sex, data = HorsePrices)\n\n# Making a exploratory data frame\nexplorartory_values &lt;- expand_grid(Sex = \"m\",\n                                   Age = 20)\n\n# Predicting values\npredict(model_lm, explorartory_values)\n\n       1 \n30806.54 \n\n\nThe model predicts that the price of such a horse would be 30806 dollars."
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#assessing-a-multivariate-linear-model-fit",
    "href": "tutorials/stat_basic/inter_reg.html#assessing-a-multivariate-linear-model-fit",
    "title": "Intermediate Regression in R",
    "section": "\n5 Assessing a multivariate linear model fit",
    "text": "5 Assessing a multivariate linear model fit\n\n5.1 Adjusted R square value\nFor a simple linear model, we have seen how to interpret the R-squared value, Residual standard error (RSE) value and the Root mean square error (RMSE) value. For a multivariate linear model, the R squared value will be greater compared to a simple linear model and this is because as you add more variables to explain the response variable, the model tends to best fit the available dataset. And in doing so, the model will no longer be useful to make inferences about the general population. So the adjusted R-square value is a metric that describes the percentage of variance explained by the explanatory variable but is measured by penalising the model with multiple explanatory variables by assigning a small penalty term for each additional explanatory variable. The formula for calculating the adjusted R square value is as follows;\n\\bar{R^2} = 1-(1-R^2)\\frac{n_{obs}-1}{n_{obs}-n_{var}-1} \n\n\n\\bar{R^2} = Adjusted R squared value\n\nR^2 = R squared value\n\nn_{obs} = Number of observations\n\nn_{var} = Number of variables in the model\n\nPlease note that the adjusted R square value is always lower than the R squared value.\nIn R, we can extract the adjusted R square value of a model using the glance() and pull() functions in the broom and dplyr packages in R. The adjusted R square in the table would be under the adj.r.squared column.\n\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model\nmodel_lm &lt;- lm(Price ~ Age + Sex, data = HorsePrices)\n\n# Extracting adjusted R square value\nmodel_lm %&gt;% glance() %&gt;%\n  pull(adj.r.squared)\n\n[1] 0.2996728"
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#linear-regression-with-interacting-terms",
    "href": "tutorials/stat_basic/inter_reg.html#linear-regression-with-interacting-terms",
    "title": "Intermediate Regression in R",
    "section": "\n6 Linear regression with interacting terms",
    "text": "6 Linear regression with interacting terms\nSo far we have built linear models using the HorsePrices dataset where we looked at the association between age and sex on the price of the horses. Now consider another variable, which is the height of the horses (Height). First, let us visualize the heights of the horses of their respective sex.\n\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting the data\nHorsePrices %&gt;% ggplot(aes(Sex, Height, col = Sex)) + geom_boxplot() +\n  labs(title = \"Height of horses with respect to their sex\",\n       x = \"Sex\",\n       y = \"Height (in hands)\") +\n  theme_bw()\n\n\n\n\nFrom the graph, it seems like male horses are typically taller when compared to female horses according to the dataset at hand. Now if we are building a linear model, where we are interested to see the association of height and sex of horses on the price of the horses, we will have to take into consideration that female and male horses have different height distribution, as seen with the box plot above.\nOne way to account for this is to build separate models using a filtered dataset for each the sex.\n\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Filtering horses by their sex\nmale_horses &lt;- HorsePrices %&gt;% filter(Sex == \"m\")\nfemale_horses &lt;- HorsePrices %&gt;% filter(Sex == \"f\")\n\n# Building linear models\nmale_lm &lt;- lm(Price ~ Height, data = male_horses)\nfemale_lm &lt;- lm(Price ~ Height, data = female_horses)\n\n# Printing model outputs\nsummary(male_lm)\n\n\nCall:\nlm(formula = Price ~ Height, data = male_horses)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18671.5  -6325.5     29.9   9001.5  24001.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -120546      66679  -1.808   0.0818 .\nHeight          9346       4016   2.327   0.0277 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11830 on 27 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1671,    Adjusted R-squared:  0.1362 \nF-statistic: 5.416 on 1 and 27 DF,  p-value: 0.0277\n\nsummary(female_lm)\n\n\nCall:\nlm(formula = Price ~ Height, data = female_horses)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15366  -6145  -2716   6127  27277 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    57330      74949   0.765    0.455\nHeight         -2515       4710  -0.534    0.601\n\nResidual standard error: 11550 on 16 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.01751,   Adjusted R-squared:  -0.0439 \nF-statistic: 0.2851 on 1 and 16 DF,  p-value: 0.6007\n\n\nThe coefficient values for the two models are very different. For male horses, a unit change in height contributes to a 9346 dollar increase in the price of the horse, but for female horses, the opposite occurs, a unit change in height decreases the price by 2515 dollars (though this difference is not significantly different). Thus we can say that the height of a horse depends on its sex. In other words, the variable height and the variable sex interact with each other. Instead of making separate models like this, R has an easy way to show interacting terms. Instead of using the +, we can use the * sign to indicate an interaction between two variables. The : sign can also be used to explicitly mention which terms interact. The codes given below show how to account for interaction between height and sex for the given dataset.\n\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Building linear model with interaction\nlm_model_1 &lt;- lm(Price ~ Height * Sex, data = HorsePrices)\n\n# Same model as above but explicitly mentioning interaction terms\nlm_model_2 &lt;- lm(Price ~ Height + Sex + Height:Sex, data = HorsePrices)\n\n# Printing model outputs\nsummary(lm_model_1)\n\n\nCall:\nlm(formula = Price ~ Height * Sex, data = HorsePrices)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18671.5  -6410.3   -671.5   6971.0  27276.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    57330      76078   0.754   0.4552  \nHeight         -2515       4781  -0.526   0.6016  \nSexm         -177876     100786  -1.765   0.0847 .\nHeight:Sexm    11861       6222   1.906   0.0633 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11730 on 43 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.4086,    Adjusted R-squared:  0.3673 \nF-statistic: 9.901 on 3 and 43 DF,  p-value: 4.377e-05\n\nsummary(lm_model_2)\n\n\nCall:\nlm(formula = Price ~ Height + Sex + Height:Sex, data = HorsePrices)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18671.5  -6410.3   -671.5   6971.0  27276.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    57330      76078   0.754   0.4552  \nHeight         -2515       4781  -0.526   0.6016  \nSexm         -177876     100786  -1.765   0.0847 .\nHeight:Sexm    11861       6222   1.906   0.0633 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11730 on 43 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.4086,    Adjusted R-squared:  0.3673 \nF-statistic: 9.901 on 3 and 43 DF,  p-value: 4.377e-05\n\n\nSince the female is the first level in the variable Sex, the intercept is estimated for female horses. This is the same value that we got from the female_lm model we made earlier. We also got the slope value for height, which is calculated for female horses (the same value as that from the female_lm model). The value for male horses is given as Sexm as is -177876, this value is relative to female horses and is given as the difference between the y-intercepts. We can find the actual y-intercept value for males by adding the Sexm value and the (Intercept) value which is -120546 ( = 57330 + -177876). This is the intercept value for male horses which we got from the male_lm model earlier. Now let us look at the Height:Sexm value, which is given as 11861. This is the slope value showing how the price change when height change for male horses, but it is calculated relative to female horses. To get the actual slope for male horses, we have to add the slope of Height and Height:Sexm values. In doing so we get a value of 9346 ( = -2515 + 11861) which is the slope of Height, which we got from the male_lm model earlier."
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#plotting-linear-model-with-an-interaction-term",
    "href": "tutorials/stat_basic/inter_reg.html#plotting-linear-model-with-an-interaction-term",
    "title": "Intermediate Regression in R",
    "section": "\n7 Plotting linear model with an interaction term",
    "text": "7 Plotting linear model with an interaction term\nWhen we were using the geom_parallel_slopes() function from the moderndive package, I mentioned that we will not use geom_smooth() function from the ggplot2 package. This is because when we specify the col variable, the geom_smooth() function automatically plots the trend line by considering the x-axis variable and the colour variable to be interacting.\n\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting linear model with interaction terms\nHorsePrices %&gt;% ggplot(aes(Height, Price, col = Sex)) + geom_point() +\n  labs(title = \"Price of horses with respect to their height and sex\",\n       x = \"Height (in hands)\",\n       y = \"Price (in dollars)\") +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\n\nThe slope for male horses is steeper than for female horses. This indicates that male horses have their price increased significantly as their height increased. Whereas female horses have their price change non significantly when their height change. This is exactly what we see in the male_lm and female_lm model coefficients."
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#simpsons-paradox",
    "href": "tutorials/stat_basic/inter_reg.html#simpsons-paradox",
    "title": "Intermediate Regression in R",
    "section": "\n8 Simpson‚Äôs Paradox",
    "text": "8 Simpson‚Äôs Paradox\nSimpson‚Äôs paradox occurs when a result conveyed using the whole dataset is different from the results conveyed using the different groups within the same dataset. In simple words, the results of the model change drastically due to the inclusion of a variable which was previously not included in the model. Let‚Äôs see what this means.\nWe will use the penguins dataset from the palmerpenguins package in R. The dataset contains body measurements taken from three different species of penguins from Antarctica. Let us build a model where the body mass (body_mass_g) is predicted by bill depth (bill_depth_mm) alone. Let us also build another build with the same model formula as before but this time, we will include an interaction term between bill depth and species (species).\n\nif (!require(palmerpenguins)) install.packages('palmerpenguins')\nlibrary(palmerpenguins)\n\n# Building linear model\nmodel_lm &lt;- lm(body_mass_g  ~ bill_depth_mm, data = penguins)\n\n# Building linear model with interaction term\nmodel_lm_interaction &lt;- lm(body_mass_g  ~ bill_depth_mm * species, data = penguins)\n\n# Printing model outputs\nsummary(model_lm)\n\n\nCall:\nlm(formula = body_mass_g ~ bill_depth_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1607.38  -510.10   -66.96   462.43  1819.28 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    7488.65     335.22   22.34   &lt;2e-16 ***\nbill_depth_mm  -191.64      19.42   -9.87   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 708.1 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.2227,    Adjusted R-squared:  0.2204 \nF-statistic: 97.41 on 1 and 340 DF,  p-value: &lt; 2.2e-16\n\nsummary(model_lm_interaction)\n\n\nCall:\nlm(formula = body_mass_g ~ bill_depth_mm * species, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-845.89 -254.74  -28.46  228.01 1161.41 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     -283.28     437.94  -0.647   0.5182    \nbill_depth_mm                    217.15      23.82   9.117   &lt;2e-16 ***\nspeciesChinstrap                 247.06     829.77   0.298   0.7661    \nspeciesGentoo                   -175.71     658.43  -0.267   0.7897    \nbill_depth_mm:speciesChinstrap   -12.53      45.01  -0.278   0.7809    \nbill_depth_mm:speciesGentoo      152.29      40.49   3.761   0.0002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 354.9 on 336 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.807, Adjusted R-squared:  0.8041 \nF-statistic:   281 on 5 and 336 DF,  p-value: &lt; 2.2e-16\n\n\nWe get a rather interesting and contradicting set of results when we compare the summary outputs for both models. In the model without the species variable, greater bill depth predicts lower body mass in penguins. But, when we add the species variable, the trend reverses, increasing bill depth is associated with an increase in body mass.\nIt is easier to plot the models and see. Use the slider to see the difference between the plots.\n\nlibrary(palmerpenguins)\n\n# Plotting the model wihtout interaction term\nggplot(penguins, aes(bill_depth_mm, body_mass_g)) +\n  geom_point() +\n  labs(title = \"Body mass predcited by bill depth in three different species of penguins\",\n       subtitle = \"body_mass_g  ~ bill_depth_mm + species\",\n       x = \"Bill depth (mm)\",\n       y = \"Body mass (g)\") +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n# Plotting the model with interaction term\nggplot(penguins, aes(bill_depth_mm, body_mass_g, col = species)) +\n  geom_point() +\n  labs(title = \"Body mass predcited by bill depth in three different species of penguins\",\n       subtitle = \"body_mass_g  ~ bill_depth_mm * species\",\n       x = \"Bill depth (mm)\",\n       y = \"Body mass (g)\") +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\nlibrary(palmerpenguins)\n\n# Plotting the model wihtout interaction term\nggplot(penguins, aes(bill_depth_mm, body_mass_g)) +\n  geom_point() +\n  labs(title = \"Body mass predcited by bill depth in three different species of penguins\",\n       subtitle = \"body_mass_g  ~ bill_depth_mm + species\",\n       x = \"Bill depth (mm)\",\n       y = \"Body mass (g)\") +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\n# Plotting the model with interaction term\nggplot(penguins, aes(bill_depth_mm, body_mass_g, col = species)) +\n  geom_point() +\n  labs(title = \"Body mass predcited by bill depth in three different species of penguins\",\n       subtitle = \"body_mass_g  ~ bill_depth_mm * species\",\n       x = \"Bill depth (mm)\",\n       y = \"Body mass (g)\") +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\n\n\nSo it is always better to visualize the dataset and build models according to the question at hand."
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#visualizing-linear-models-with-two-numerical-explanatory-variables",
    "href": "tutorials/stat_basic/inter_reg.html#visualizing-linear-models-with-two-numerical-explanatory-variables",
    "title": "Intermediate Regression in R",
    "section": "\n9 Visualizing linear models with two numerical explanatory variables",
    "text": "9 Visualizing linear models with two numerical explanatory variables\nOften you will be building models with two or more two explanatory variables. If some of those explanatory variables are categorical then it is easier to plot them as we can assign different colour or shape variables and we can also use faceting. But if they are all numerical variables then it is kind of difficult to plot them. We can either rely on three-dimensional plots or use a continuous colour scale gradient in a two-dimensional plot. Let us see how to plot the graphs.\nWe will again use the penguins dataset from the palmerpenguins package in R. We will build a model to predict body mass (body_mass_g) using bill length (bill_length_mm) and bill depth (bill_depth_mm). Let us first try to plot a 3D plot using the scatter3D() function from the {plot3D} package in R.\n\nif (!require(plot3D)) install.packages('plot3D')\nlibrary(plot3D)\nlibrary(palmerpenguins)\n\n# Plotting a 3D plot\nscatter3D(penguins$body_mass_g, penguins$bill_depth_mm, penguins$bill_length_mm)\n\n\n\n\nWell, 3D plotting is a mess, as you can see from the above plot. It is very hard to make out any useful findings using the graph above. So instead of this, we can return to our conventional 2D plot where we can use a continuous colour gradient to visualize the third numerical variable.\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"penguins\")\n\n# Plotting the graph\npenguins %&gt;% ggplot(aes(bill_depth_mm, body_mass_g, col = bill_length_mm)) +\n  geom_point() +\n  labs(title = \"Body mass predcited by bill depth and bill length in three different species of penguins\",\n       subtitle = \"body_mass_g  ~ bill_depth_mm * species\",\n       x = \"Bill depth (mm)\",\n       y = \"Body mass (g)\") +\n  scale_color_viridis_c(option = \"inferno\") +\n  theme_bw()\n\n\n\n\nFrom the graph, it roughly seems like penguins with greater body mass have a longer and narrower bill. We can further subdivide this plot by the species category using faceting.\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"penguins\")\n\n# Making a function to make line breaks in text\naddline_format &lt;- function(x,...){\n  gsub(',','\\n',x)}\n\n# Plotting the graph\npenguins %&gt;% ggplot(aes(bill_depth_mm, body_mass_g, col = bill_length_mm)) +\n  geom_point() +\n  labs(title = addline_format(\"Body mass predcited by bill depth and bill length,in three different species of penguins\"),\n       x = \"Bill depth (mm)\",\n       y = \"Body mass (g)\") +\n  scale_color_viridis_c(option = \"inferno\") +\n  facet_wrap(~ species) +\n  theme_bw()\n\n\n\n\nWhen we group the data according to different species, we get the opposite result as compared before. Here, we can see that penguins with heavier body mass have broader and longer bills.\nNow let us plot the predicted values from the linear models to see the general trend of the data.\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\ndata(\"penguins\")\n\n# Making prediction\nmodel_lm &lt;- lm(body_mass_g ~ bill_depth_mm * bill_length_mm, data = penguins)\n\nexplorartory_values &lt;- expand_grid(\n  bill_depth_mm = seq(15,25,1),\n  bill_length_mm = seq(30,60,1),\n)\n\npredicted_values &lt;- explorartory_values %&gt;% mutate(\n  body_mass_g = predict(model_lm, explorartory_values)\n)\n\n# Plotting the graph with predicted values\npenguins %&gt;% ggplot(aes(bill_depth_mm, body_mass_g, col = bill_length_mm)) +\n  geom_point() +\n  labs(title = \"Body mass predcited by bill depth and bill length \nin three different species of penguins\",\n       x = \"Bill depth (mm)\",\n       y = \"Body mass (g)\") +\n  scale_color_viridis_c(option = \"inferno\") +\n  facet_wrap(~ species) +\n  geom_point(data = predicted_values, aes(y = body_mass_g), size = 3, shape = 15) +\n  theme_bw()\n\n\n\n\nWe got a nice gradient-coloured graph. From the predicted values we got from the model we can understand how body mass changes with bill depth and bill length. We can see two clear patterns. For penguins with lower bill length values, the body mass increases as bill depth increases. The opposite is true for penguins with larger values for bill length. For these penguins, body mass decreases as bill depth increases."
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#visualizing-logistic-models-with-two-numerical-explanatory-variables",
    "href": "tutorials/stat_basic/inter_reg.html#visualizing-logistic-models-with-two-numerical-explanatory-variables",
    "title": "Intermediate Regression in R",
    "section": "\n10 Visualizing logistic models with two numerical explanatory variables",
    "text": "10 Visualizing logistic models with two numerical explanatory variables\nWe have seen how to include two or more variables in a linear model. The same procedure applies to logistic models. Now we will see how to visualize a logistic model with two or more numerical variables.\nWe will use the CreditCard dataset in the {AER} package in R. The dataset contains the credit history of a sample of applicants for a type of credit card. We will predict to see the success of the acceptance of the credit card application (card) using the yearly income (income) and age (age) of the applicant. The code for making the plot is very similar to that we have seen earlier.\n\nif (!require(AER)) install.packages('AER')\nlibrary(AER)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\ndata(\"CreditCard\")\n\n# Making prediction\nmodel_logistic &lt;- glm(card ~ income * age, data = CreditCard, family = \"binomial\")\n\nexplorartory_values &lt;- expand_grid(\n  income = seq(0,20,1),\n  age = seq(18,90,1),\n)\n\npredicted_values &lt;- explorartory_values %&gt;% mutate(\n  card = predict(model_logistic, explorartory_values, type = \"response\")\n)\n\n# Plotting the graph with predicted values\nggplot(CreditCard, aes(income, as.integer(card)-1, col = age)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Does income and age affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Probability of success in accepting a credit card application\") +\n  scale_color_gradient2() +\n  geom_point(data = predicted_values, aes(y = card), size = 3, shape = 15) +\n  theme_bw()\n\n\n\n\nWe get some interesting results. People with higher yearly incomes have a higher chance of getting a credit card. But for age, younger people with lower yearly income have more chance of getting a credit card as compared to older people with the same level of yearly income. But this trend reverses when yearly income crosses 50,000 dollars."
  },
  {
    "objectID": "tutorials/stat_basic/inter_reg.html#conclusion",
    "href": "tutorials/stat_basic/inter_reg.html#conclusion",
    "title": "Intermediate Regression in R",
    "section": "\n11 Conclusion",
    "text": "11 Conclusion\nIt is time to wrap up! In this tutorial we have learned the following things;\n\nHow to build models with two or more explanatory variables\nHow to interpret multivariate models\nHow to predict values using a multivariate model\nAssessing a multivariate model\nBuilding models with interaction terms and plotting them\nWhat is Simpson‚Äôs paradox\nVisualising models with two numerical explanatory variables"
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html",
    "href": "tutorials/stat_basic/intro_reg.html",
    "title": "Introduction to Regression in R",
    "section": "",
    "text": "TL;DR\n\n\n\nIn this article you will learn;\n\nWhat is linear regression / linear model\nHow to build a linear model and use it to predict\nUnderstanding the linear model summary output\nHow to view model outputs as data frames using the broom package\nAssessing linear model performance using; R-squared, Residual standard error and Root mean square error\nVisualizing model fit using; Residual vs.¬†Fitted plot, Q-Q plot and Scale-location plot\nAnalysing outliers using; Leverage (hat values) and Influence (cook‚Äôs distance)\nWhat is logistic regression / logistic model\nHow to build a logistic model\nDifferent types of predictions using the logistic model: Probability values, Most like outcomes, Odds ratio and Log odds ratio\nUnderstanding the logistic model summary output\nAssessing logistic model performance using; Confusion matrix, Accuracy, Sensitivity and Specificity"
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#introduction",
    "href": "tutorials/stat_basic/intro_reg.html#introduction",
    "title": "Introduction to Regression in R",
    "section": "\n1 Introduction",
    "text": "1 Introduction\nIn this tutorial, we will learn the basics of linear regression and logistic regression using R. We will learn how interpret the model summaries of both of them, how to predict values using models, how to assess and visualize model performance and so on."
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#linear-regression",
    "href": "tutorials/stat_basic/intro_reg.html#linear-regression",
    "title": "Introduction to Regression in R",
    "section": "\n2 Linear regression",
    "text": "2 Linear regression\nRegression models are a part of statistical models which are used to find relationships between variables in a dataset. Linear regression or linear models are the simplest of the models which describe a linear relationship between two variables. Throughout this tutorial (and also in other tutorials) I might use regression and model interchangeably, please don‚Äôt get confused and just have it in your mind that they generally mean the same.\nWe will use the HorsePrices dataset from the Stat2Data package in R. The dataset contains the price and related characteristics of horses listed for sale on the internet. Using the dataset, we will see how the price of a horse (Price) changes according to its age (Age) using a linear model. We will plot this association using the geom_smooth() function in the ggplot2 package in R. But first let us visualize the dataset.\n\nif (!require(Stat2Data)) install.packages('Stat2Data')\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting the data\nHorsePrices %&gt;% ggplot(aes(Age, Price)) + geom_point() +\n  labs(title = \"Does the price of the horse decrease as they age?\",\n       x = \"Age (years)\",\n       y = \"Price (dollars)\") +\n  theme_bw()\n\n\n\n\nNow let us fit a model to the graph using the geom_smooth() function. To fit a linear model, we will use the method = \"lm\" argument. By default, the linear trend line comes with confidence intervals, we can disable this by specifying se = F.\n\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting a linear model\nHorsePrices %&gt;% ggplot(aes(Age, Price)) + geom_point() +\n  geom_smooth(method = \"lm\", se  = F) +\n  labs(title = \"Does the price of the horse decrease as they age?\",\n       x = \"Age (years)\",\n       y = \"Price (dollars)\") +\n  theme_bw()\n\n\n\n\nFrom the linear trend line, we can see that there is some evidence suggesting that the price of the horse decreases as they age.\nIn the above linear regression plot, the relationship between the price of the horse and the age of the horse is described as a straight line which is the feature of linear regression. Thus, this trend line can be described by the equation of the straight line;\ny=mx+c\nHere m is the slope and c is the y-intercept.\nNow let‚Äôs try to get the slope and y-intercept values for the model we just plotted. We will use the lm() function to model the regression of price and age of the horses. The syntax for the lm() function is as follows;\n\nlm(dependent variable ~ independent variable, dataset_name)\n\nHere, we want to see how the price of the horse, which is the dependent variable, depends on the age of the horse, which is the independent variable. As mentioned before, linear regression is a type of statistical model. So in other words, we can also say that we are building a linear model to predict the price of the horse using the age of the horse.\n\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Printing the model results\nprint(model_lm)\n\n\nCall:\nlm(formula = Price ~ Age, data = HorsePrices)\n\nCoefficients:\n(Intercept)          Age  \n    29796.8       -415.9  \n\n\nWe get two values, an ‚Äòintercept‚Äô value and a value under the column ‚ÄòAge‚Äô.\n\nHere the slope of the line is denoted under the ‚ÄòAge‚Äô column and is -415.9\nThe y-intercept is given under the column intercept and is 29796.8.\n\nNow, what do these values mean according to our model? Recall that the slope is the rate of change of y for x. Therefore, the value of -415.9 means that, as the horse ages one year, the price of the horse decreases by 415.9 dollars. Similarly, the y-intercept value is got when the x value is zero. So the intercept value of 29796.8 means that if the horse has an age of 0 years, or a newborn horse has an average price of 29796.8 dollars. The y-intercept and the slopes got from the model output are called the coefficients.\nWe can also predict the values of the response variable using a categorical explanatory variable. In the earlier case, the age of the horse was a numerical value, and we calculated the slope of it. Then what values do we get for a categorical explanatory variable?\nTo test this we will use the Hawks dataset from the Stat2Data package in R. The dataset contains measurements of three hawk species. We will see if body weight (Weight) is different between the three hawk species (Species). The three different hawk species in this dataset are; CH=Cooper‚Äôs, RT=Red-tailed, and SS=Sharp-Shinned.\n\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Weight ~ Species, data = Hawks)\n\n# Printing the model results\nprint(model_lm)\n\n\nCall:\nlm(formula = Weight ~ Species, data = Hawks)\n\nCoefficients:\n(Intercept)    SpeciesRT    SpeciesSS  \n      420.5        673.9       -272.5  \n\n\nWe got the coefficient values where we have an intercept value and the values for two of the hawk species (RT and SS), so what happened to the third species (CH)? And what do the values for each species mean?\nThe intercept value shows the mean body weight for the third species (CH) and the rest of the values by their magnitude and sign tell us how greater or less their average body weight is. For example, the average body weight of RT is 420.5 + 673.9 = 1094.4g. We can change the model formula syntax by adding -1, to calculate individual averages for each level in our categorical explanatory variable.\n\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Weight ~ Species - 1, data = Hawks)\n\n# Printing the model results\nprint(model_lm)\n\n\nCall:\nlm(formula = Weight ~ Species - 1, data = Hawks)\n\nCoefficients:\nSpeciesCH  SpeciesRT  SpeciesSS  \n    420.5     1094.4      148.0  \n\n\nThus we essentially get the back the average body masses of all the hawk species in our data. We can cross check this by checking the means individually as shown below.\n\nlibrary(Stat2Data)\n\ndata(\"Hawks\")\n\n# Calculating the mean\nHawks %&gt;% select(Species, Weight) %&gt;%\n  group_by(Species) %&gt;%\n  summarise(mean_weight = mean(Weight, na.rm = T))"
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#making-predicition-using-a-linear-model",
    "href": "tutorials/stat_basic/intro_reg.html#making-predicition-using-a-linear-model",
    "title": "Introduction to Regression in R",
    "section": "\n3 Making predicition using a linear model",
    "text": "3 Making predicition using a linear model\nOne cool use of models is that we can use them to predict values for which data is not available. Let us go back to the first model we created where we have the price of horses predicted by their age. Let us look at the plot again.\n\nif (!require(Stat2Data)) install.packages('Stat2Data')\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"HorsePrices\")\n\n# Plotting a linear model\nHorsePrices %&gt;% ggplot(aes(Age, Price)) + geom_point() +\n  geom_smooth(method = \"lm\", se  = F) +\n  labs(title = \"Does the price of the horse decrease as they age?\",\n       x = \"Age (years)\",\n       y = \"Price (dollars)\") +\n  theme_bw()\n\n\n\n\nWe have age values from 1 year to 20 years. What if we want to know what the price of the course would be for a 30-year-old horse given the trend seen without collected data? We can know this price with the use of the model we created. Using the predict() function, we can input our model and the value for which we want the prediction.\n\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\nage_30 &lt;- data.frame(Age=c(30))\n\n# Predicting price for a 30 years old horse\npredict(model_lm, newdata = age_30)\n\n       1 \n17320.89 \n\n\nOur model predicts that the price of a 30-year-old horse would be 17320.89 dollars. Therefore, even though we did not have data for a 30-year-old horse, with help of our model, we were able to extrapolate to find the price for the 30-year-old horse."
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#understanding-the-summary-of-a-linear-model",
    "href": "tutorials/stat_basic/intro_reg.html#understanding-the-summary-of-a-linear-model",
    "title": "Introduction to Regression in R",
    "section": "\n4 Understanding the summary of a linear model",
    "text": "4 Understanding the summary of a linear model\nWe can use the summary() function to get the results of the linear model. Let us look at what each section in summary means;\n\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Printing model outputs\nsummary(model_lm)\n\n\n\nIn section 1, we have the model formula used for modelling\nIn section 2, we have the 1st quartile, median and the 3rd quartile of the residuals. The residual is the distance or difference between the predicted value and the actual value in the dataset. One of the important requirements for a linear model to be applied to a dataset is that the residuals should be normally distributed. We can check if the residuals are normally distributed by looking at this section. For normally distributed residuals, the median should be close to zero and the 1st and 3rd quartiles should have similar absolute values.\nIn section 3, we have the y intercept value and the slope for the Age variable. We also have the standard error value and the t-statistics coupled with a corresponding p-value. The level of significance is noted by the number of asterisks.\nIn section 4, we can evaluate the model performance. There are mainly two metrics that assess the model fit; the residual standard error and the R-squared value. We will see more about these values later."
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#getting-the-model-output",
    "href": "tutorials/stat_basic/intro_reg.html#getting-the-model-output",
    "title": "Introduction to Regression in R",
    "section": "\n5 Getting the model output",
    "text": "5 Getting the model output\nUsing various functions in the broom package in R, we can extract or view different model outputs.\n\nWe can use the tidy() function to view or save the coefficients of the model as a data frame which is easier to manipulate using different functions in the dplyr package.\n\n\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Visualising coefficients and other metrics as a data frame\ntidy(model_lm)\n\n\n\n  \n\n\n\n\nWe can use the augment() function to return the dataset values with their corresponding fitted value and various other values. The .fitted column contains the fitted values and the .resid column contains the residuals.\n\n\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Visualising dataset and model output values\naugment(model_lm)\n\n\n\n  \n\n\n\n\nThe glance() function return section 4 part of the model or the model performance metrics as a data frame.\n\n\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Visualising model performance metrics\nglance(model_lm)"
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#assessing-linear-model-fit",
    "href": "tutorials/stat_basic/intro_reg.html#assessing-linear-model-fit",
    "title": "Introduction to Regression in R",
    "section": "\n6 Assessing linear model fit",
    "text": "6 Assessing linear model fit\nOnce we fit a model, we can predict values outside the dataset. A good model will be able to predict values more accurately, so how do we know how good a model is? To assess model fit, we mainly use three metrics and they are described below.\n\n6.1 R-squared value\n\nIn section 4 of the model summary, we can see an R-squared value of 0.01612. The R-squared value explains how much of the variance seen in the response variable is explained by the explanatory variables. The value of R-squared is between 0 and 1. The value 0 corresponds to the worst possible fit and 1 corresponds to the best fit. Here we have a value close to 0, which means our model is poorly fit. The R-squared value is typically just the squared value of the correlation between the response and the explanatory variable.\n\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Extracting the R-squared value\nmodel_lm %&gt;% glance() %&gt;%\n  pull(r.squared)\n\n[1] 0.01612329\n\n# R-squared is nothing but squared of correlation value\nHorsePrices %&gt;% summarise(R_squared = cor(Price, Age)^2)\n\n\n\n  \n\n\n\n\n6.2 Residual standard error (RSE)\nResidual is the difference between the predicted and actual value in the model. The residual standard error estimated on average how much difference is there between the predicted and actual value in the model. A lower value suggests that the predicted value closely aligns with the values in the dataset suggesting a good fit.\nIn the previous model the residual standard error is;\n\nlibrary(Stat2Data)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Extracting the RSE value\nmodel_lm %&gt;% glance() %&gt;%\n  pull(sigma)\n\n[1] 15012.95\n\n\nWe got a value of 15012, this means that there is, on average a difference of 15012 dollars between the predicted and actual values in the dataset.\nThe residual standard error is calculated by taking the square root of the sum of the squared residuals divided by the number of degrees of freedom. The degrees of freedom are calculated as the difference between the total observations and the number of variables used in the model (we used Price and Age variables, so we used 2 variables in total).\n\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Calculating the RSE value\nHorsePrices %&gt;%\n  mutate(residual_sq = residuals(model_lm)^2) %&gt;%\n  summarise(sum_residual_sq = sum(residual_sq),\n            degrees_of_freedom = n() - 2,\n            rse = sqrt(sum_residual_sq/degrees_of_freedom))\n\n\n\n  \n\n\n\n\n6.3 Root mean square error (RMSE)\nThe root mean square error is another metric which is similar to that of the residual standard error value and functions the same way in quantifying how inaccurate model predictions are. It is typically not preferred but it is another metric that is sometimes used. The only difference in the calculation between RSE and RMSE is that instead of divining by the degrees of freedom, RMSE is calculated by dividing the total number of observations.\n\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Calculating the RMSE value\nHorsePrices %&gt;%\n  mutate(residual_sq = residuals(model_lm)^2) %&gt;%\n  summarise(sum_residual_sq = sum(residual_sq),\n            total_observations = n(),\n            rse = sqrt(sum_residual_sq/total_observations))"
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#visualizing-linear-model-fit",
    "href": "tutorials/stat_basic/intro_reg.html#visualizing-linear-model-fit",
    "title": "Introduction to Regression in R",
    "section": "\n7 Visualizing linear model fit",
    "text": "7 Visualizing linear model fit\nIn addition, to assess model fit using different metrics, we can also visualize the model fit using different methods. These methods include Residual vs.¬†Fitted plot, Q-Q plot and Scale-location plot. We will be using the autoplot() function in the ggfortify package in R to visualize all three of them. We will use the which = argument inside the autoplot() function to specify which plot to visualize. The argument takes in the following values;\n\n\n1 : Residual vs.¬†Fitted plot\n\n2 : Q-Q plot\n\n3 : Scale-location plot\n\nNow let us see what each of these plots corresponds to.\n\n7.1 Residual vs.¬†Fitted plot\n\nif (!require(ggfortify)) install.packages('ggfortify')\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting residual vs. fitted plot\nautoplot(model_lm, which = 1, ncol = 1) + theme_bw()\n\n\n\n\nRecall that the residual is the difference between the predicted value and the data value. In the plot shown above, the x-axis has the fitted or predicted values of the model and its corresponding residual value on the y-axis. If the predicted values closely resemble the actual data values, then the blue line will be very similar to the dotted horizontal line at y = 0. In our case, however, the model is a mess! as you can see the blue line very much deviates from the y = 0 line.\n\n7.2 Q-Q plot\nThe Q-Q plot tells us whether the residuals are normally distributed.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting Q-Q plot\nautoplot(model_lm, which = 2, ncol = 1) + theme_bw()\n\n\n\n\nYou can see that the tail ends of the Q-Q plot deviate away from the straight line, suggesting that at lower and higher age values, the model does a poor fit to predict the price values.\n\n7.3 Scale-location plot\nThe scale-location plot is plotted with the square root values of the standardized residual values on the y-axis and fitted or predicted values on the x-axis. The plot tells us whether, along the predicted value, the residuals get bigger or smaller.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting scale-location plot\nautoplot(model_lm, which = 3, ncol = 1) + theme_bw()\n\n\n\n\nIn the plot shown above, starting from lower price values, the residuals decrease in size and for higher price values, the residuals increase in the size. Overall this suggests that for lower and higher age values in our dataset, the model struggles to output a good fit.\nWe can plot all three by inputting all the number arguments;\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting all visualizations concerning the model fit \nautoplot(model_lm, which = 1:3) + theme_bw()"
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#outliers-leverage-and-influence",
    "href": "tutorials/stat_basic/intro_reg.html#outliers-leverage-and-influence",
    "title": "Introduction to Regression in R",
    "section": "\n8 Outliers, leverage, and influence",
    "text": "8 Outliers, leverage, and influence\nOutliers are data values which are either considerably large or small when compared to the rest of the data values. They rarely occur and can be due to pure chance or measurement errors. But they can have a strong influence on the linear relationship between two variables and can mess up our inferences while doing linear modelling. So there are two ways to assess the presence and effect of outliers in the dataset; they are called leverage and influence.\n\n8.1 Leverage\nLeverage value denotes extreme values in our dataset. Higher the leverage, the more extreme the data point is. We can find the leverage values from a model object using the hatvalues() function. Alternatively, using the augment() function from the broom package, we can extract the leverage value from the .hat column.\n\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Finding the leverage values using hatvalues()\nleverage &lt;- hatvalues(model_lm)\n\n# Finding the leverage values using augment()\nmodel_lm %&gt;% augment() %&gt;%\n  select(Price, Age, .hat) %&gt;%\n  arrange(desc(.hat)) %&gt;% # Finding the extreme values\n  head()\n\n\n\n  \n\n\n\nLooks like we have three outliers in this dataset which have significantly large leverage values.\n\n8.2 Influence\nThe influence of a data point measures how much of a difference will the model have if that value is excluded while building the model. It is represented by a metric called Cook‚Äôs distance where bigger values denoted higher influence and thus the presence of an outlier. We can use the cooks.distance() function to calculate the influence of data points in a model object. Alternatively, we can, like before, use the augment() function and extract the .cooksd column to get the Cook‚Äôs distance values.\n\nlibrary(Stat2Data)\nlibrary(dplyr)\nlibrary(broom)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Finding the influence values using cooks.distance()\ncooks_distance &lt;- cooks.distance(model_lm)\n\n# Finding the influence values using augment()\nmodel_lm %&gt;% augment() %&gt;%\n  select(Price, Age, .cooksd) %&gt;%\n  arrange(desc(.cooksd)) %&gt;% # Finding the extreme values\n  head()\n\n\n\n  \n\n\n\n\n8.3 Plotting leverage and influence\nLike before, we can use the autoplot() function in the ggfortify package in R to visualize leverage and influence. The input values for which = corresponding leverage and influence are;\n\n\n4 : Cook‚Äôs distance\n\n5 : Residuals vs.¬†Leverage\n\n6 : Cook‚Äôs distance vs.¬†Leverage\n\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(ggfortify)\n\ndata(\"HorsePrices\")\n\n# Building a linear model \nmodel_lm &lt;- lm(Price ~ Age, data = HorsePrices)\n\n# Plotting all visualizations concerning leverage and influence\nautoplot(model_lm, which = 4:6) + theme_bw()\n\n\n\n\nFrom the plots, it seems like we have three outliers which can significantly affect the model."
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#logistic-regression",
    "href": "tutorials/stat_basic/intro_reg.html#logistic-regression",
    "title": "Introduction to Regression in R",
    "section": "\n9 Logistic regression",
    "text": "9 Logistic regression\nLinear models are best used for the continuous response variable. But sometimes we can have a binary response variable like; survival data: dead or alive, choice data: yes or no, survey data: disagree or agree. In these cases, linear models will not work and we should rely on logistic models, which model using a binomial distribution. The logistic model is built in R using the glm() function with family = argument set to binomial. This is shown below;\n\nBuilding a logistic model: glm(response ~ predictor, data, family = \"binomial\")\n\n\nLet us look at the CreditCard dataset in the {AER} package in R. The dataset contains the credit history of a sample of applicants for a type of credit card. We will predict to see the success of the acceptance of the credit card application (card) using yearly income (income). Let us look at the dataset first.\n\nif (!require(AER)) install.packages('AER')\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Viewing the data\nhead(CreditCard)\n\n\n\n  \n\n\n\nHere, the response variable card has only two values and thus is a binary response variable. Let us plot the data and see how the values are distributed.\n\nlibrary(AER)\nlibrary(ggplot2)\n\ndata(\"CreditCard\")\n\n# Plotting the data\nggplot(CreditCard, aes(income, card)) + geom_jitter(width = 0, height = 0.05) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n  theme_bw()\n\n\n\n\nWe get some interesting results, even at a low yearly income, we have a high number of successful credit applicants but at the same time, we also have many applicants getting rejected from acquiring a credit card. Very few people have their credit card requests rejected when they have a high yearly income. Overall, the plot seems to suggest that yearly income is not a good predictor for looking at the success of credit card applications. Let us see if that is the case by modelling the data using a logistic model.\n\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log &lt;- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Printing model outputs\nsummary(model_log)\n\n\nCall:\nglm(formula = card ~ income, family = \"binomial\", data = CreditCard)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2501   0.5301   0.7005   0.7502   0.8733  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.73452    0.15842   4.636 3.55e-06 ***\nincome       0.15582    0.04597   3.390 0.000699 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1404.6  on 1318  degrees of freedom\nResidual deviance: 1391.6  on 1317  degrees of freedom\nAIC: 1395.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nEven though from the model it was not apparent that yearly income had an association with the success of a credit application, the model suggests that yearly income does significantly explain the variance seen in the success of the credit applications. We will come back to interpreting the model summary soon.\nAs seen earlier for linear models, we can also plot logistic models. We have to specify method = \"glm\" in the and then specify the family by inputting method.args = list(family = binomial). Now to the plot, we have to change the y-axis variable to boolean values (0 or 1). We can either use the recode() function before plotting and modifying the dataset or straight away input as.numeric() to the y-axis variable and subtract 1 to rescale the values to boolean.\n\nlibrary(AER)\nlibrary(ggplot2)\n\ndata(\"CreditCard\")\n\n# Plotting the logistic model\nggplot(CreditCard, aes(income, as.numeric(card) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial)) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n  theme_bw() \n\n\n\n\nIn the y-axis, you can see values ranging from 0 to 1 and the blue trend line shifting towards 1 for high values of yearly income. This suggests that, as the yearly income increase, the probability of success in credit card application increases.\nNow one question that might come to your mind (I had this question when I was learning regression) is why can‚Äôt we apply linear regression if it is just 0 and 1 values for the response variable. So let us plot both linear and logistic regression for the above data.\n\nlibrary(AER)\nlibrary(ggplot2)\n\ndata(\"CreditCard\")\n\n# Plotting the logistic model\nggplot(CreditCard, aes(income, as.numeric(card) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = \"lm\", se = F, col = \"red\") +\n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial)) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n  theme_bw() \n\n\n\n\nHA! If we got a similar trend line as that of the logistic model, then why bother with a logistic model? Well, hold your horses! If we take a closer look at the trend line, we can see that the red line overshoots the y-axis value of 1. This means we have a probability value greater than 1 which is absurd. Perhaps the best way to see why linear models don‚Äôt work for binary response variables is by predicting values using them with values outside the dataset.\n\nlibrary(AER)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log &lt;- glm(card ~ income, data = CreditCard, family = \"binomial\")\nmodel_lm &lt;- lm((as.numeric(card) - 1) ~ income, data = CreditCard)\n\n# # Generating random income values\nexplorartory_values &lt;- tibble(income = seq(-50,50,2))\n\n# Predicting the values for linear and logistics\n# We use the 'type = response' to calculate probabilities\npredict_values &lt;- explorartory_values %&gt;%\n  mutate(predict_lm = predict(model_lm, explorartory_values),\n         predict_log = predict(model_log, explorartory_values, type = \"response\"))\n\n# Plotting the predicted values as a line graph\nggplot(predict_values, aes(income)) +\n  geom_line(data = predict_values, aes(y = predict_lm, col = \"linear\")) +\n  geom_line(data = predict_values, aes(y = predict_log, col = \"logistic\")) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n   scale_colour_manual(\"Distribution\", values = c(\"red\", \"blue\")) +\n  theme_bw() \n\n\n\n\nYou can see that the predicted values using the linear model are going beyond 0 and 1 whereas the logistic model predicts values between 0 and 1. Also, notice the sigmoid curve or the S-shaped curve which is a characteristic of the logistic model. Thus for a binary response variable, logistic modelling is the best."
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#making-predictions-using-a-logistic-model",
    "href": "tutorials/stat_basic/intro_reg.html#making-predictions-using-a-logistic-model",
    "title": "Introduction to Regression in R",
    "section": "\n10 Making predictions using a logistic model",
    "text": "10 Making predictions using a logistic model\nLike in the linear model, we use the predict() function to predict values using a logistics model. We can change the type of values we get from the predict() function using the type = argument. By default, the predict function outputs log odds ratio (which is type = \"link\"), another option is type = \"response\" which outputs probability values. Other ways for expressing predicted values from the model include ‚Äòodds ratio‚Äô and ‚Äômost likely outcomes.\n\n10.1 Probability values\nOne of the easier ways of expressing the predicted values from a logistic model. Simply it just calculates the probability of the levels in the response variable.\n\nlibrary(AER)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log &lt;- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Generating random income values\nexplorartory_values &lt;- tibble(income = seq(-50,50,2))\n\n# Calculating the probability values\npredicted_Values &lt;- explorartory_values %&gt;%\n  mutate(probability = predict(model_log, explorartory_values, type = \"response\"))\n\n# Printing probability values\nhead(predicted_Values)\n\n\n\n  \n\n\n# Plotting the probability values\nggplot(predicted_Values, aes(income, probability)) +\n  geom_point() +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       subtitle = \"Probability values\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Probability of application being accepted\") +\n  theme_bw()\n\n\n\n\n\n10.2 Most likely outcome\nThis is the easiest way to express logistic model predicted values. It rescales the probability values into stating whether an event happens or not. Simply put, it rounds the probability values to give either 0 or 1.\n\nlibrary(AER)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log &lt;- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Generating random income values\nexplorartory_values &lt;- tibble(income = seq(-50,50,2))\n\n# Calculating the most likely values\npredicted_Values &lt;- explorartory_values %&gt;%\n  mutate(probability = predict(model_log, explorartory_values, type = \"response\"),\n         most_likely = round(probability))\n\n# Printing most likely values\nhead(predicted_Values)\n\n\n\n  \n\n\n# Plotting the most likely values\nggplot(predicted_Values, aes(income, most_likely)) +\n  geom_point() +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       subtitle = \"Most likely values\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Number of predicted applications being accepted\") +\n  theme_bw()\n\n\n\n\n\n10.3 Odds ratio\nThe odds ratio is calculated by taking the probability of success divided by the probability of losing. In simple terms, it tells the odds of one event happening compared to the other.\nOdds\\,ratio = \\frac{p(success)}{1-p(success)} = \\frac{p(success)}{p(fail)}\nWe can visualize this from the predicted values.\n\nlibrary(AER)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log &lt;- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Generating random income values\nexplorartory_values &lt;- tibble(income = seq(-50,50,2))\n\n# Calculating the odds ratio values\npredicted_Values &lt;- explorartory_values %&gt;%\n  mutate(probability = predict(model_log, explorartory_values, type = \"response\"),\n         odds_ratio = probability / (1 - probability))\n\n# Printing odds ratio values\nhead(predicted_Values)\n\n\n\n  \n\n\n# Plotting the odds ratio values\nggplot(predicted_Values, aes(income, odds_ratio)) +\n  geom_point() +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       subtitle = \"Odds ratio\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Odds ratio\") +\n  geom_hline(yintercept = 1, col = \"red\") +\n  theme_bw()\n\n\n\n\nThe plot has a horizontal line plotted with y = 0. The points on this line correspond to equal odds of being rejected and accepted for a credit card application (equal odds of winning and losing). The y values for example 1000 correspond to 1000 successes for every one failure in a credit card application. The plot suggests that after the 18,000 dollars yearly income mark, the odds of success to failure rise exponentially.\n\n10.4 Log odds ratio\nThe log odds ratio is the log-transformed value of the odds ratio. We will see how this value is superior to all the other values when we plot the values. Since we are transforming the y-axis values to log, we convert the scale of the y-axis to log using the scale_y_log10() function. By default, the predict() function outputs log odds ratio.\n\nlibrary(AER)\nlibrary(dplyr)\n\ndata(\"CreditCard\")\n\n# Building models\nmodel_log &lt;- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Generating random income values\nexplorartory_values &lt;- tibble(income = seq(-50,50,2))\n\n# Calculating the log odds ratio values\npredicted_Values &lt;- explorartory_values %&gt;%\n  mutate(probability = predict(model_log, explorartory_values, type = \"response\"),\n         odds_ratio = probability / (1 - probability),\n         log_odds_ratio = log(odds_ratio),\n         log_odds_ratio_1 = predict(model_log, explorartory_values))\n\n# Printing log odds ratio values\nhead(predicted_Values)\n\n\n\n  \n\n\n# Plotting the log odds ratio values\nggplot(predicted_Values, aes(income, odds_ratio)) +\n  geom_point() +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       subtitle = \"Log Odds ratio\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Log Odds ratio\") +\n  geom_hline(yintercept = 1, col = \"red\") +\n  scale_y_log10() +\n  theme_bw()\n\n\n\n\nLog transforming the odds ratio values resulted in a linear trend line which is easier to understand the overall relationship. We can see that as yearly income increases, the probability of success in credit card application will also rise.\nGiven below is the summary of the predicted value types and their usage concerning the audience in question. Some values like most likely outcomes might be best in explaining the trends in the data for non-experts or common people, whereas log odds might be best for experts.\n\nDifferent scales of predicted values from a logistic model\n\n\n\n\n\n\n\nScale\nEasier to explain values?\nEasier to explain changes in the response variable?\nIs it precise?\n\n\n\nProbability\nYes\nNot easy for x values in the middle\nYes\n\n\nMost likely outcomes\nEasiest out of all\nQuickly compare high and low x values\nNo\n\n\nOdds ratio\nKind of easy to explain.Odds of one event against another\nNot really\nYes\n\n\nLog odds ratio\nLog transformed values are harder to explain\nLinear relation after log transformationmakes it easy to interpret changes\nYes"
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#understanding-the-summary-of-a-logistic-model",
    "href": "tutorials/stat_basic/intro_reg.html#understanding-the-summary-of-a-logistic-model",
    "title": "Introduction to Regression in R",
    "section": "\n11 Understanding the summary of a logistic model",
    "text": "11 Understanding the summary of a logistic model\nNow let us look at the summary of the logistic model we built earlier.\n\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log &lt;- glm(card ~ income, data = CreditCard, family = \"binomial\")\n\n# Printing model outputs\nsummary(model_log)\n\n\n\nSection 1 is the model formula we used to build the model.\nSection 2 is the quantile values of the residuals in the model, which is to check if the residuals are normally distributed.\nSection 3 outlines the model coefficients which are in log odds values which we discussed in detail above.\nSection 4. I don‚Äôt know what these values are, so once I learn I will update this text.\nSection 5. I don‚Äôt know what these values are, so once I learn I will update this text."
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#assessing-logistic-model-fit",
    "href": "tutorials/stat_basic/intro_reg.html#assessing-logistic-model-fit",
    "title": "Introduction to Regression in R",
    "section": "\n12 Assessing logistic model fit",
    "text": "12 Assessing logistic model fit\nTo assess the performance of a logistic model we have several different metrics we can use that are different from the linear model metrics. Let us look at each of them.\n\n12.1 Confusion matrix\nThe confusion matrix is a 2 by 2 matrix showcasing the number of correct predictions and no of wrong predictions.\n\nThe framework of a confusion matrix\n\nScale\nActually false\nActually true\n\n\n\nPredicted false\nTrue negative\nFalse negative\n\n\nPredicted true\nFalse positive\nTrue positive\n\n\n\nLet us make a confusion matrix of a logistic model where credit card success (card) is predicted by the number of major derogatory reports against the applicant (reports).\n\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log &lt;- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values &lt;- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values &lt;-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix &lt;- table(predicted_values, actual_values)\nconfusion_matrix\n\n                actual_values\npredicted_values    0    1\n               0  104   18\n               1  192 1005\n\n\nWe have 1109 (104 + 1005) correct predictions, 18 false negatives and 192 false positives. We can plot the confusion matrix with the help of yardstick and ggfortify packages.\n\nif (!require(yardstick)) install.packages('yardstick')\nlibrary(AER)\nlibrary(ggplot2)\nlibrary(yardstick)\nlibrary(ggfortify)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log &lt;- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values &lt;- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values &lt;-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix &lt;- table(predicted_values, actual_values)\n\n# Converting the confusion matrix into an object that can be plotted\nconf_matrix &lt;- conf_mat(confusion_matrix)\n\n# Plotting the confusion matrix\nautoplot(conf_matrix) +\n  labs(title = \"Mosaic plot of confusion matrix\",\n       x = \"Actual value\",\n       y = \"Predicted value\") +\n  theme_bw()\n\n\n\n\nNow let us see how to interpret this plot. In the x-axis, you can see the actual values, 0 corresponds to failure in getting the credit card application accepted and 1 corresponds to the success of the same. The width of the bars corresponds to the proportion of the values of 0 and 1 in the dataset. There are more success values than failure values. Now, for the corresponding actual value, we have the bar split into respective predicted values. For actual failure values, we have a lot of predicted values falsely marking it as success. On the other hand, for the actual success value, most of the predicted values were correct. Thus the first bar under 0 tells us about the false positive cases and the second bar under 1 tells us about the false negative cases. We can see that our model has more false positive cases than false positive cases which is what we saw in the confusion matrix.\n\n12.2 Accuracy\nAccuracy is simply the proportion of correct predictions compared to all predictions.\nAccuracy = \\frac{true\\,positives + true\\,negatives}{total\\,predictions}\nWe can find the accuracy from the summary of the confusion matrix we made using the summary() and slice() functions. The event_level = \"second\" is to denote the second column of the confusion matrix as the success value which is 1.\n\nlibrary(AER)\nlibrary(yardstick)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log &lt;- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values &lt;- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values &lt;-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix &lt;- table(predicted_values, actual_values)\n\n# Converting the confusion matrix into an object that can be plotted\nconf_matrix &lt;- conf_mat(confusion_matrix)\n\n# Finding accuracy of the model\nsummary(conf_matrix, event_level = \"second\") %&gt;% slice(1)\n\n\n\n  \n\n\n\nWe got an accuracy value of 84% for the model we built.\n\n12.3 Sensitivity\nSensitivity tells us the proportion of true positives. Higher values suggest that there are fewer false negatives.\nSensitivity = \\frac{true\\,positives}{false\\,negatives + true\\,positives}\n\nlibrary(AER)\nlibrary(yardstick)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log &lt;- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values &lt;- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values &lt;-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix &lt;- table(predicted_values, actual_values)\n\n# Converting the confusion matrix into an object that can be plotted\nconf_matrix &lt;- conf_mat(confusion_matrix)\n\n# Finding sensitivity of the model\nsummary(conf_matrix, event_level = \"second\") %&gt;% slice(3)\n\n\n\n  \n\n\n\nWe have around very 2% false negatives (100 - 98).\n\n12.4 Specificity\nSensitivity tells us the proportion of true negatives. Higher values suggest that there are fewer false positives.\nSensitivity = \\frac{true\\,negatives}{false\\,positives + true\\,negatives}\n\nlibrary(AER)\nlibrary(yardstick)\n\ndata(\"CreditCard\")\n\n# Building a logistic model\nmodel_log &lt;- glm(card ~ reports, data = CreditCard, family = \"binomial\")\n\n# Extracting actual values\nactual_values &lt;- as.numeric(CreditCard$card) - 1\n\n# Extracting predicted values\npredicted_values &lt;-  round(fitted(model_log))\n\n# Building a confusion matrix\nconfusion_matrix &lt;- table(predicted_values, actual_values)\n\n# Converting the confusion matrix into an object that can be plotted\nconf_matrix &lt;- conf_mat(confusion_matrix)\n\n# Finding specificity of the model\nsummary(conf_matrix, event_level = \"second\") %&gt;% slice(4)\n\n\n\n  \n\n\n\nWe have around 64% false positives (100 - 35)."
  },
  {
    "objectID": "tutorials/stat_basic/intro_reg.html#conclusion",
    "href": "tutorials/stat_basic/intro_reg.html#conclusion",
    "title": "Introduction to Regression in R",
    "section": "\n13 Conclusion",
    "text": "13 Conclusion\nHurray! We completed the basics of regression using R. This tutorial will lay the foundations for learning statistical modelling which I have covered in the tutorial section. Let us recap what we studied in this tutorial;\n\nWhat is linear regression / linear model\nHow to build a linear model and use it to predict\nUnderstanding the linear model summary output\nHow to view model outputs as data frames using the broom package\nAssessing linear model performance using; R-squared, Residual standard error and Root mean square error\nVisualizing model fit using; Residual vs.¬†Fitted plot, Q-Q plot and Scale-location plot\nAnalysing outliers using; Leverage (hat values) and Influence (cook‚Äôs distance)\nWhat is logistic regression / logistic model\nHow to build a logistic model\nDifferent types of predictions using the logistic model: Probability values, Most like outcomes, Odds ratio and Log odds ratio\nUnderstanding the logistic model summary output\nAssessing logistic model performance using; Confusion matrix, Accuracy, Sensitivity and Specificity"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html",
    "href": "tutorials/stat_basic/intro_stat.html",
    "title": "Introductory statistics with R",
    "section": "",
    "text": "TL;DR\n\n\n\nIn this article you will learn;\n\nDescriptive and Inferential statistics\nMeasures of centre: mean, median and mode\nMeasures of spread: variance, standard deviation, mean absolute deviation and interquartile range\nDistributions: binomial, normal, standard normal, Poisson, exponential, student‚Äôs t and log-normal\nCentral limit theorem\nData transformation: Skewness, Q-Q plot and data transformation functions"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#introduction",
    "href": "tutorials/stat_basic/intro_stat.html#introduction",
    "title": "Introductory statistics with R",
    "section": "\n1 Introduction",
    "text": "1 Introduction\nThis tutorial will cover the basics of statistics and help you to utilize those concepts in R. Most of the topics that I will be discussing here is from what I learned from my undergraduate course: Biological Data Analysis, which was offered in 2019 at IISER-TVM. Then by reading online and by taking courses at DataCamp I further continued my journey to master statistics on my own. If you find any mistakes in any of the concepts that I have discussed in this tutorial, kindly raise it as a GitHub issue or comment them down or you can even mail me (the mail address is given in the footnote). Without further ado let us start!"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#why-do-we-need-statistics",
    "href": "tutorials/stat_basic/intro_stat.html#why-do-we-need-statistics",
    "title": "Introductory statistics with R",
    "section": "\n2 Why do we need statistics?",
    "text": "2 Why do we need statistics?\nIf you are a student and are currently pursuing an undergrad course in college or a university, then chances are that you would come across scientific publications in your respective fields. And while you are reading them, for most of the articles, there will be a separate section highlighting the statistical methods used in the published study. So what was the purpose of it? One of the most important purpose statistical tests fulfil is that it gives evidence to test if the results given in the study occurred due to pure chance alone or due to the experiment that the authors of the paper did."
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#descriptive-and-inferential-statistics",
    "href": "tutorials/stat_basic/intro_stat.html#descriptive-and-inferential-statistics",
    "title": "Introductory statistics with R",
    "section": "\n3 Descriptive and Inferential statistics",
    "text": "3 Descriptive and Inferential statistics\nStatistics is broadly categorized into two: Descriptive statistics and Inferential statistics\nDescriptive statistics: Descriptive statistics are used to describe and summarise data. Suppose we have data on the food preferences of a bird in a small area of an island. We found that out of the 100 birds we observed, 20 of them prefer fruits (20%), 30 of them prefer insects (30%) and the remaining 50 prefer reptiles (50%). So with the available data, this is what we know about the birds we have observed.\nInferential statistics: Inferential statistics, as suggested by the name is used to make inferences about the population using the sample collected from that population. Using the above data, we can make inferences about the food preferences of all the birds on the island. Thus, we can find what is the percentage of birds on that island that would be preferring reptiles to eat."
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#a-note-on-data",
    "href": "tutorials/stat_basic/intro_stat.html#a-note-on-data",
    "title": "Introductory statistics with R",
    "section": "\n4 A note on data",
    "text": "4 A note on data\nData can be broadly categorised into two: Quantitative and Qualitative. Quantitative data are numerical values and can be continuous or discrete. They can also be arranged in a defined order (as they are numbers) and are thus called ordinal data. Qualitative data are often categorical data which can be either ordinal or nominal.\n\nExamples of quantitative and qualitative data\n\n\n\n\n\nExamples of Quantitative data\nExamples of Qualitative data\n\n\n\nSpeed of train\nSurvival data: Alive or Dead\n\n\nAge of a person\nOutcomes: Win or Lose\n\n\nProportion of visits to a shop\nChoices: Kannur or Pune\n\n\nChange in prices of an item\nMarital status: Married or Unmarried\n\n\nGrowth of bacteria\nOrdered choices: Agree, Somewhat agree, Disagree"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#measures-of-centre",
    "href": "tutorials/stat_basic/intro_stat.html#measures-of-centre",
    "title": "Introductory statistics with R",
    "section": "\n5 Measures of centre",
    "text": "5 Measures of centre\nA good way to summarise data is by looking at their measure of the centre which can be mean, median and mode. I am sure that you all know how to find these measures.\n\nMean is best used to describe data that are normally distributed or don‚Äôt have any outliers.\nMedian is best for data with non-normal distribution as it is not affected much by outliers in the data. For a median value of 10, what it means is that 50% of our data is above the value of 10 and 50% of the remaining data is below the value of 10.\nMode is best used if our data have a lot of repeated values and is best used for categorical data as for calculating mode, the data does not need to be in ordinal scale.\n\nLet us visualize the measure of centres.\nWe will use the penguins dataset from the palmerpenguins package in R. Let‚Äôs plot the distribution curve for the ‚Äúbody mass‚Äù of the ‚ÄúChinstrap‚Äù species of penguins. In R, function for calculating mean is mean() and for median is median(). There is no base function to calculate the mode, so we will write a function in R which can calculate the mode value.\n\nCodeif (!require(palmerpenguins)) install.packages('palmerpenguins')\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Creating a function to calculate the mode value\ngetmode &lt;- function(v) {\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\n# Calculating the mean, median and mode\npen_avg &lt;- penguins %&gt;% filter(species == \"Chinstrap\") %&gt;% summarise(mean = mean(body_mass_g),\n                                                          median = median(body_mass_g),\n                                                          mode = getmode(body_mass_g))\n\n# Plotting the data\npenguins %&gt;% filter(species == \"Chinstrap\") %&gt;% \n  ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + ggtitle(\"Body mass distribution\") + \n  geom_density(fill = \"grey\") + \n  labs(subtitle = paste0(\"N=\",penguins %&gt;% filter(species == \"Chinstrap\") %&gt;% nrow())) +\n  geom_vline(aes(xintercept = pen_avg$mean, colour = \"red\")) +\n  geom_text(aes(x=pen_avg$mean, label=\"mean\", y=4e-04), colour=\"red\", angle=90, vjust = 1.2, text=element_text(size=11)) +\n  geom_vline(aes(xintercept = pen_avg$median, colour = \"blue\")) +\n  geom_text(aes(x=pen_avg$median, label=\"median\", y=4e-04), colour=\"blue\", angle=90, vjust = -1.2, text=element_text(size=11)) +\ngeom_vline(aes(xintercept = pen_avg$mode, colour = \"green\")) +\n  geom_text(aes(x=pen_avg$mode, label=\"mode\", y=4e-04), colour=\"green\", angle=90, vjust = -1.2, text=element_text(size=11)) +\n  theme_bw() + theme(legend.position=\"none\")\n\n\n\n\nFor a normal distribution, which is what we have in this case, the mean, median and mode are all the same value and are in the middle of the curve. For a skewed dataset or a dataset with outliers, all the measures of the centre will be different and they depend on the skewness of the data.\nConsider the plot of the diamonds dataset from the ggplot2 package in R. The data is skewed in nature.\n\nCodelibrary(tidyverse)\n\n# Create function to get mode\ngetmode &lt;- function(v) {\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\n# Plotting the data\ndiamonds %&gt;% filter(cut == \"Fair\") %&gt;%\n  ggplot(aes(x = carat)) + \n  xlab(\"carats\") + ylab(\"Density\") + ggtitle(\"Diamond carat distribution\") + \n  geom_density(fill = \"gold\") + \n  labs(subtitle = paste0(\"N=\",diamonds %&gt;% filter(cut == \"Fair\") %&gt;% nrow())) +\n  geom_vline(aes(xintercept = mean(diamonds$carat), colour = \"red\")) +\n  geom_text(aes(x=mean(diamonds$carat), label=\"mean\", y=1), colour=\"red\", angle=90, vjust = 1.2, text=element_text(size=11)) +\n  geom_vline(aes(xintercept = median(diamonds$carat), colour = \"blue\")) +\n  geom_text(aes(x=median(diamonds$carat), label=\"median\", y=1), colour=\"blue\", angle=90, vjust = -1.2, text=element_text(size=11)) +\ngeom_vline(aes(xintercept = getmode(diamonds$carat), colour = \"green\")) +\n  geom_text(aes(x=getmode(diamonds$carat), label=\"mode\", y=1), colour=\"green\", angle=90, vjust = -1.2, text=element_text(size=11)) +\n  theme_bw() + theme(legend.position=\"none\")\n\n\n\n\nHere the curve is right-skewed, as the mean is skewed towards the right side of the mode value. If it was the other way around then the curve will be called left-skewed.\n\n\nMeasures of centre in relation to skeweness"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#measures-of-spread",
    "href": "tutorials/stat_basic/intro_stat.html#measures-of-spread",
    "title": "Introductory statistics with R",
    "section": "\n6 Measures of spread",
    "text": "6 Measures of spread\nLike the measures of the centre, we can also summarise the data by measuring the spread of the data. Spread tell us how close or how far each of the data points is distributed in the dataset. There are many methods to measure the spread of the data, let us look at each of them one by one.\n\n6.1 Variance\nThe variance of data is defined as the average squared difference from the mean of the data. It tells us how far each of our data points is from the mean value.\nThe formula for finding the variance is as follows;\ns = \\sigma^2 = \\frac{\\sum (x_{i} - \\bar{x})^{2}}{n - 1}\nWhere s is sample variance, x_{i} is your data point, \\bar{x} is mean, n is the sample size and n-1 is called as the degrees of freedom. Here \\sigma is the standard deviation which is explained below.\nThe function to calculate variance in R is var()\n\nlibrary(palmerpenguins)\n\n# Calculating the variance of bill length\nvar(penguins$bill_length_mm, na.rm = T)\n\n[1] 29.80705\n\n\n\n6.2 Standard deviation\nStandard deviation like the variance tells us how much each of our data points is spread around the mean and is easier to understand as they are not being squared like in the case with variance.\nThe formula to calculate the standard deviation of the mean is;\n\\sigma = \\sqrt{\\frac{\\sum (x_{i} - \\bar{x})^{2}}{n - 1}}\nWhere \\sigma is the standard deviation, x_{i} is your data point, \\bar{x} is the sample mean, n is the sample size and n-1 is called the degrees of freedom. Here \\sigma^2 gives the variance of the data.\nThe function to calculate variance in R is sd()\n\nlibrary(palmerpenguins)\n\n# Calculating the variance of bill length\nsd(penguins$bill_length_mm, na.rm = T)\n\n[1] 5.459584\n\n# Calculating the variance\n(sd(penguins$bill_length_mm, na.rm = T))^2\n\n[1] 29.80705\n\n\n\n6.3 Mean absolute deviation\nMean absolute deviation takes the absolute value of the distances to the mean and then takes the mean of those differences. While this is similar to standard deviation, it‚Äôs not the same. Standard deviation squares distances, so longer distances are penalized more than shorter ones, while mean absolute deviation penalizes each distance equally.\nThe formula to calculate mean absolute deviation is;\nMAD = (\\frac{1}{n})\\sum_{i=1}^{n}\\left | x_{i} - \\bar{x} \\right |\nWhere n is the sample size, x_{i} is your data point, \\bar{x} is the sample mean, n is the sample size and n-1 is called the degrees of freedom. Here \\sigma^2 gives the variance of the data.\nTo calculate absolute deviation in R, there is no base function to do so, so we have to manually calculate it.\n\nlibrary(palmerpenguins)\n\n# Calculating the distances\ndistances &lt;- penguins$bill_length_mm - mean(penguins$bill_length_mm, na.rm = T)\n\n# Calculating the mean absolute deviation\nmean(abs(distances), na.rm = T)\n\n[1] 4.706797\n\n\n\n6.4 Interquartile range (IQR)\nQuantiles of data can be calculated using the quantile() function in R. By default, the data is split into four equal parts which is why it‚Äôs called a ‚Äòquartile‚Äô.\n\nlibrary(palmerpenguins)\n\n# Calculating the quartile\nquantile(penguins$bill_length_mm, na.rm = T)\n\n    0%    25%    50%    75%   100% \n32.100 39.225 44.450 48.500 59.600 \n\n\nHere the data is split into four equal parts which are called the quartiles of the data. In the output, we can see that 25% of the data points are between 32.1 and 39.225, and another 25% of the data points are between 39.225 and 44.450 and so on. Here the 50% quartile is 44.450 which is the median value.\nWe can manually specify the splitting. Below given code splits the data into five parts and hence is called a quantile. The splitting is specified by the argument probs inside the quantile() function. You can either manually put the proportions of the split or use the seq() function to provide the proportions.\n\nlibrary(palmerpenguins)\n\n# Calculating the quantile\nquantile(penguins$bill_length_mm, probs =  seq(0, 1, 0.2), na.rm = T)\n\n   0%   20%   40%   60%   80%  100% \n32.10 38.34 42.00 46.00 49.38 59.60 \n\n\n\nIQR is the distance between the second quartile and the third quartile of the data or the height of the box plot.\n\nThe interquartile range can be calculated using the IQR() function.\n\n# Finding the IQR\nIQR(penguins$bill_length_mm, na.rm = T)\n\n[1] 9.275\n\n\nThe interquartile range is overall the best way to summarise the spread of the data and forms the crux of a boxplot design.\n\nlibrary(palmerpenguins)\n\n# Plotting a boxplot\nggplot(penguins, aes(species, body_mass_g, fill = species)) + \n  geom_boxplot() +\n  labs(title = \"Body masses of three different species of penguins\",\n       x = \"Penguin species\",\n       y = \"Body mass (g)\") +\n  theme_bw()\n\n\n\n\nIn the box plot, the middle dark line is the median or the 50% quartile or the second quartile, the bottom of the box is the first quartile (25%) and the top of the box is the third quartile (75%). You can see two data points outside the box for ‚ÄòChinstrap‚Äô species, these are outliers. What makes a data point an outlier? That‚Äôs where the whiskers of the box come in. Typically the outliers are calculated in the following format;\n\nOutliers: x_i &lt; Q_1 - 1.5 * IQR or x_i &gt; Q_3 + 1.5 * IQR\n\n\nHere Q_1 is the first quartile, Q_3 is the third quartile and IQR is the interquartile distance.\n\n\nCharacteristics of a boxplot"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#distributions",
    "href": "tutorials/stat_basic/intro_stat.html#distributions",
    "title": "Introductory statistics with R",
    "section": "\n7 Distributions",
    "text": "7 Distributions\nWe will come across different types of distributions while analysing data. Let us look at each of them.\n\n7.1 Binomial distribution\nThe binomial distribution describes the probability of the number of successes in a sequence of independent trials. You might have seen this type of distribution in your introductory probability classes. It is easy to visualize this using a coin toss event. Imagine we have a fair coin, we are tossing it to see the number of times we get heads. So here getting a head is a success and getting a tail is a failure.\nIn R we can simulate this using the rbinom() function.\nWe want to see the results when we are tossing a coin once. Since the functions take random values, to be concise I will set a seed so that you can repeat the codes that I have given to get the same results.\n\n# Setting seed\nset.seed(123)\n\n# Tossing a coin one time\nrbinom(1,1,0.5)\n\n[1] 0\n\n\nWe got a value of zero, which corresponds to the event of getting a tail. Here, in the rbinom() function, we have the following syntax;\nrbinom(no. of flips, no. of coins, the probability of getting a head)\nTo see the number of heads we get by flipping a single coin three times would be with equal chances of getting a head and a tail is;\n\n# Setting seed\nset.seed(123)\n\n# Tossing a one coins three times\nrbinom(3,1,0.5)\n\n[1] 0 1 0\n\n\nIn the first flip we got zero heads, in the second we got one head and in the third, we got zero head.\nChecking to see how total number of heads we get by flipping three coins one time;\n\n# Setting seed\nset.seed(258)\n\n# Tossing a three coins one time\nrbinom(1,3,0.5)\n\n[1] 2\n\n\nSo we get a total of 2 heads when we flipped three coins one time.\nChecking to see the total number of heads we get by flipping four coins three times.\n\n# Setting seed\nset.seed(258)\n\n# Tossing a four coins three times\nrbinom(3,4,0.5)\n\n[1] 3 3 2\n\n\nIn the first flip of three coins, we got three heads, in the second we got again three heads and lastly, we got 2 heads.\nWe can also calculate the results if our coin is biased.\nChecking to see the total number of heads we get by flipping four coins three times, the coin only has a 25% probability of falling on heads.\n\n# Setting seed\nset.seed(258)\n\n# Tossing a four coins three times, but coins is unfair\nrbinom(3,4,0.25)\n\n[1] 2 2 1\n\n\nHence the rbinom() function is used to sample from a binomial distribution.\nWhat about finding discrete probabilities like are chance of getting 7 heads if we flipped a coin 10 times? To find that we use the function dbinom().\nChecking the probability of getting exactly 7 heads while tossing a coin 10 times;\n\n# Setting seed\nset.seed(258)\n\n# Probability of getting 7 heads in 10 coin flips\n# dbinom(no. of heads, no. of flips, chance of getting a head)\ndbinom(7,10,0.5)\n\n[1] 0.1171875\n\n\nLikewise, we can also find the probability of getting 7 or fewer heads while tossing the 10 times using the pbinom() function.\n\n# Setting seed\nset.seed(258)\n\n# Probability of getting 7 heads in 10 coin flips\n# pbinom(no. of heads, no. of flips, chance of getting a head)\npbinom(7,10,0.5)\n\n[1] 0.9453125\n\n\nTo find the probability of getting more than 7 heads in 10 trials would be;\n\n# Setting seed\nset.seed(258)\n\n# Probability of getting more than 7 heads in 10 coin flips\n# Set lower.tail = FALSE\npbinom(7,10,0.5, lower.tail = F)\n\n[1] 0.0546875\n\n# Alternatively you find it in the following also\n1 - pbinom(7,10,0.5)\n\n[1] 0.0546875\n\n\nThe expected value of an event would be equal to; E = n*p\nWhere n is the number of trails and p is the probability of successes.\nNow let us visualize the binomial distribution using the function we have covered till now;\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a binomial distribution\ndata.frame(heads = 0:10, \n           pdf = dbinom(0:10, 10, prob = 0.5)) %&gt;%\nggplot(aes(x = factor(heads), y = pdf)) +\n  geom_col() +\n  geom_text(\n    aes(label = round(pdf,2), y = pdf + 0.01),\n    position = position_dodge(0.9),\n    size = 3,\n    vjust = 0\n  ) +\n  labs(title = \"Probability of X = x successes.\",\n       subtitle = \"Binomial distribution (n = 10, p = 0.5)\",\n       x = \"No of heads (successes)\",\n       y = \"Probability\") +\n  theme_bw()\n\n\n\n\nThis is a binomial distribution showing the probability of getting heads when a single coin is flipped 10 times. For a fair coin, we get the expected probability that most times we will get 5 heads when the coin is flipped 10 times.\n\n7.2 Normal distribution\nWe saw an example of the normal distribution when we were discussing the measures of the centre. The normal distribution or also known as the Gaussian distribution is one of the most important distributions in statistics as it is one of the requirements the data has to fulfil for numerous statistical analyses.\nLet us look at the penguins dataset from the palmerpenguins package in R. We will plot the distribution curve for the ‚Äúbody mass‚Äù of the ‚ÄúChinstrap‚Äù species of penguins.\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Plotting a normal distribution\npenguins %&gt;% filter(species == \"Chinstrap\") %&gt;% ggplot(aes(x = body_mass_g)) + \n  xlab(\"Body Mass (g)\") + ylab(\"Density\") + \n  ggtitle(\"Body mass distribution of Chinstrap penguins\") + \n  geom_density(fill = \"darkred\") + \n  labs(subtitle = paste0(\"N=\", penguins %&gt;%\n                           filter(species == \"Chinstrap\") %&gt;% nrow())) +\n  theme_bw()\n\n\n\n\nAs you can see, the data distribution closely resembles a ‚Äúbell-shaped‚Äù curve. On closer look, you can also see that the area under the curve is almost symmetrical to both sides and there is only a single peak present. We can also visualize the variance of the data by looking at the width of the curve. A wider curve means variance is higher and a narrower curve means variance in the data is smaller. Also for a normal distribution, there are no outliers present. We saw earlier that the mean, median and mode for a normal distribution are the same.\nAs seen earlier, we can use the rnorm() function to sample from the normal distribution. The syntax for the function is as follows;\nrnorm(number of samples, mean, sd)\nMany real-life datasets closely resemble a normal distribution, like the one which is plotted above. Since they approximate a normal distribution, we can use different base functions in R to answer some interesting questions.\n\nWhat percentage of Chinstrap penguins are below 3500g? To get this answer we approximate the body mass distribution of Chinstrap penguins to a normal distribution and use the pnrom() function.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary &lt;- penguins %&gt;% filter(species == \"Chinstrap\") %&gt;%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Finding the percentage of penguins below 3500g\npnorm(3500, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\n\n[1] 0.2721009\n\n\nSo there are about 27% of penguins who have their body mass below 3500g\n\nWhat percentage of Chinstrap penguins are above 4000g? To get this answer we again use the pnrom() function but use the lower.tail = F argument.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary &lt;- penguins %&gt;% filter(species == \"Chinstrap\") %&gt;%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Finding the percentage of penguins above 4000g\npnorm(4000, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass,\n      lower.tail = F)\n\n[1] 0.2436916\n\n\nThere are about 24% of penguins in the dataset have a body mass above 4000g.\n\nWhat percentage of Chinstrap penguins have their body masses between 3000g and 4000g? To find this answer, we first find the percentage of penguins who have masses below 3000g and then below 4000g. Then we subtract them to get our answer.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary &lt;- penguins %&gt;% filter(species == \"Chinstrap\") %&gt;%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Finding the percentage of penguins between 3000g and 4000g\nbelow_3000 &lt;- pnorm(3000, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\nbelow_4000 &lt;- pnorm(4000, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\n\nbelow_4000 - below_3000\n\n[1] 0.7280752\n\n\nAbout 73% of penguins have body masses between 3000g and 4000g.\n\nAt what body mass is 60% of the penguins weigh lower than? We can get the answer using the qnorm() function.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary &lt;- penguins %&gt;% filter(species == \"Chinstrap\") %&gt;%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Body mass at which 60% of the penguins weigh lower than\nqnorm(0.6, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass)\n\n[1] 3830.458\n\n\nWe find that 60% of the penguins weigh lower than 3830g.\n\nAt what body mass is 30% of the penguins weigh greater than? We can get the answer using the qnorm() function.\n\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n# Finding mean and sd of the body masses of Chinstrap penguins\nchin_body_mass_summary &lt;- penguins %&gt;% filter(species == \"Chinstrap\") %&gt;%\n  summarise(mean_body_mass = mean(body_mass_g),\n            sd_body_mass = sd(body_mass_g))\n\n# Body mass at which 30% of the penguins weigh greater than\nqnorm(0.3, mean = chin_body_mass_summary$mean_body_mass,\n      sd = chin_body_mass_summary$sd_body_mass, lower.tail = F)\n\n[1] 3934.634\n\n\nWe find that 30% of the penguins weigh greater than 3935g.\n\n7.3 Standard normal distribution\nA normal distribution with mean (\\bar{x}) = 0 and variance (\\sigma^{2}) = 1 is called standard normal distribution or a z-distribution. With the help of standard deviation, we can split the area under the curve of standard normal distribution into three parts. This partitioning of the area is known as the 68‚Äì95‚Äì99.7 rule.\nWhat it means is that;\n\n68% of the data will lie within ¬±\\sigma from \\bar{x} or they lie within 1 standard deviation from the mean\n95% of the data will lie within ¬±2\\sigma from \\bar{x} or they lie within 2 standard deviations from the mean\n99.7% of the data will lie within ¬±3\\sigma from \\bar{x} or they lie within 3 standard deviations from the mean\n\n\nCodelibrary(ggplot2)\n\n# Plotting a standard normal distribution\nggplot(data.frame(x = c(-4,4)), aes(x)) +\n  # Plot the pdf\n  stat_function(\n    fun = dnorm,\n    n = 101, args = list(mean = 0, sd = 1),\n    geom = \"area\", color = \"grey75\", fill = \"grey75\", alpha = 0.4) +\n  # Shade below -2\n  stat_function(\n    fun = function(x) ifelse(x &lt;= -2, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  # Shade below -1\n  stat_function(\n    fun = function(x) ifelse(x &lt;= -1, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  # Shade above 2\n  stat_function(\n    fun = function(x) ifelse(x &gt;= 2, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  # Shade above 1\n  stat_function(\n    fun = function(x) ifelse(x &gt;= 1, dnorm(x, mean = 0, sd = 1), NA),\n    aes(x),\n    geom = \"area\", color = NA, fill = \"grey40\", alpha = 0.7) +\n  ggtitle(\"The standard normal distribution\") +\n  xlab(expression(italic(z))) +\n  ylab(expression(paste(\"Density, \", italic(f(z))))) +\n  geom_text(x = 0.5, y = 0.25, size = 3.5, fontface = \"bold\",\n            label = \"34.1%\") +\n  geom_text(x = -0.5, y = 0.25, size = 3.5, fontface = \"bold\",\n            label = \"34.1%\") +\n  geom_text(x = 1.5, y = 0.05, size = 3.5, fontface = \"bold\",\n            label = \"13.6%\") +\n  geom_text(x = -1.5, y = 0.05, size = 3.5, fontface = \"bold\",\n            label = \"13.6%\") +\n  geom_text(x = 2.3, y = 0.01, size = 3.5, fontface = \"bold\",\n            label = \"2.1%\") +\n  geom_text(x = -2.3, y = 0.01, size = 3.5, fontface = \"bold\",\n            label = \"2.1%\") +\n  geom_vline(xintercept=0, col = \"red\") +\n  annotate(\"text\", x=-0.25, y=0.15, label=\"Mean(xÃÖ) = 0\", angle=90) +\n  theme_bw()\n\n\n\n\n\nThe lightest grey area contains 68% (~ 34.1 + 34.1) of the data and it corresponds to \\bar{x}¬±\\sigma.\nThe second lightest grey area contains 95% (~ 34.1 + 34.1 + 13.6 + 13.6) and it corresponds to \\bar{x}¬±2\\sigma.\nThe darkest grey area contains 99.7% (~ 34.1 + 34.1 + 13.6 + 13.6 + 2.1 + 2.1) and it corresponds to \\bar{x}¬±3\\sigma.\nFor a standard normal distribution; the area under the curve is 1, the mean is 0 and the variance is 1.\n\n7.4 Poisson distribution\nPoisson distribution describe poisson processes. A Poisson process is when events happen at a certain rate but are completely random. For example; the number of people visiting the hospital, the number of candies sold in a shop, and the number of meteors falling on earth in a year. Thus the Poisson distribution describes the probability of some number of events happening over a fixed time. Thus the Poisson distribution is only applicable for datasets containing 0 and positive integers. It won‚Äôt work with negative or decimal values.\nThe Poisson distribution has the same value for its mean and variance and is denoted by \\lambda.\nAs seen in earlier distributions, the function to sample from a Poisson distribution is rpois(). Given below is an example of a Poisson distribution for \\lambda = 3, where the number of events ranges from 0 to 10.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a poisson distribution\ndata.frame(events = 0:10, \n           pdf = dpois(x = 0:10, lambda = 3)) %&gt;%\nggplot(aes(x = factor(events), y = pdf)) +\n  geom_col() +\n  geom_text(\n    aes(label = round(pdf,2), y = pdf + 0.01),\n    position = position_dodge(0.9),\n    size = 3,\n    vjust = 0\n  ) +\n  labs(title = \"Probability of X = x Events\",\n       subtitle = \"Poisson distribution (Œª = 3)\",\n       x = \"Events (x)\",\n       y = \"Probability\") +\n  theme_bw()\n\n\n\n\nLet us take a scenario where the number of people visiting the hospital per week is 25. This is a Poisson process and we can model this scenario using a Poisson distribution. Here \\lambda = 25. As seen earlier, we can use different base functions in R to answer different questions concerning the Poisson distribution.\n\nWhat is the probability that 20 people will visit the hospital in a week given that 25 people on average visit the hospital in a week?\n\n\n# Calculating the probability that 20 people will visit the hospital in a week\ndpois(20, lambda = 25)\n\n[1] 0.05191747\n\n\nWe get a 5.2% chance that 20 people will visit the hospital in a week.\n\nWhat is the probability that 15 or fewer people will visit the hospital given that 25 people on average visit the hospital in a week? To get the answer to this question, we use the ppois() function.\n\n\n# Calculating the probability that 15 or fewer people will visit the hospital in a week\nppois(15, lambda = 25)\n\n[1] 0.02229302\n\n\nThere is a 2.2% chance that 15 or fewer people will visit the hospital.\n\nWhat is the probability that more than 5 people will visit the hospital given that 25 people on average visit the hospital in a week? To get the answer to this question, we use the ppois() function but with the lower.tail = F argument.\n\n\n# Calculating the probability that more than 5 people will visit the hospital in a week\nppois(5, lambda = 25, lower.tail = F)\n\n[1] 0.9999986\n\n\nWe get a 100% chance that more than 5 people will visit the hospital.\n\n7.5 Exponential distribution\nThe exponential distribution describes the probability distribution of time between events in a Poisson process. Exponential distribution can be used to predict the probability of waiting 10 minutes between two visitors in a hospital, the probability of elapsing 5 minutes between the sale of two candies, probability of having 6 months between two meteor showers on earth.\nSome real-life examples which exhibit exponential distribution are bacterial growth rate, oil production, call duration, and my parent‚Äôs patience (quickly decaying curve).\nThe exponential distribution is also described by the same parameter \\lambda as that of the Poisson distribution but it‚Äôs measured as a ‚Äòrate‚Äô.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a exponential distribution\nx &lt;- seq(0, 20, length.out=1000)\ndat &lt;- data.frame(time =x, px=dexp(x, rate=0.6))\n\nggplot(dat, aes(x=x, y=px)) +\n  geom_line() +\n  labs(title = \"Probability of X = x Time\",\n       subtitle = \"Exponential distribution (Œª = 0.6 or rate = 0.6)\",\n       x = \"Time\",\n       y = \"Probability\") +\n  theme_bw()\n\n\n\n\nYou can use the function rexp() to sample across an exponential distribution, similarly use dexp() and pexp() to find the probability of events like shown for other distributions.\n\n7.6 Student‚Äôs t distribution\nThe student‚Äôs t distribution or simply called the t distribution is a probability distribution similar to a normal distribution but is estimated with a low sample size collected from a population whose standard deviation is unknown. The parameter in estimating the t-distribution is called the degrees of freedom.\n\nCode# Plotting the t distribution and comparing to the standard normal distribution\nggplot(data = data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = function(x) dt(x, df = 2),\n                aes(color = \"t\")) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1),\n                aes(color = \"normal\")) +\n  labs(title = \"Student's t-distribution vs Standard normal distribution\",\n       subtitle = \"t-distribution (degrees of freedom = 2)\",\n       x = \"t or z\",\n       y = \"Probability\") +\n  scale_colour_manual(\"Distribution\", values = c(\"red\", \"blue\")) +\n  theme_bw()\n\n\n\n\nIn the graph shown above, the t-distribution with degrees of freedom = 2 is plotted alongside the standard normal distribution. You can see that both the tail ends of the t-distribution are thicker as compared to the normal distribution and hence for the t-distribution the values are more away from the mean. But as the degrees of freedom increase, the t-distribution tends to become similar to that of a normal distribution.\n\nCode# Plotting the t distribution and comparing to the standard normal distribution\nggplot(data = data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = function(x) dt(x, df = 2),\n                aes(color = \"t_2\")) +\n  stat_function(fun = function(x) dt(x, df = 25),\n                aes(color = \"t_25\")) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1),\n                aes(color = \"normal\")) +\n  labs(title = \"Student's t-distributions vs Standard normal distribution\",\n       subtitle = \"t-distribution (df = 2 and df = 25)\",\n       x = \"t or z\",\n       y = \"Probability\") +\n  scale_colour_manual(\"Distribution\", values = c(\"red\", \"blue\", \"darkgreen\")) +\n  theme_bw()\n\n\n\n\nIn the above graph, you can see that the t-distribution with degrees of freedom = 25 (green) is very similar to the standard normal distribution.\n\n7.7 Log-normal distribution\nThe log-normal distribution is a probability distribution of variable ‚Äòx‚Äô whose log-transformed values follow a normal distribution. Like the normal distribution, log-normal distribution has a mean value and a standard deviation which are estimated from log-transformed values. Some real-life examples which follow the log-normal distribution are; the length of chess games, blood pressure in adults, and the number of hospitalizations in the 2003 SARS outbreak.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\n\n# Plotting a log normal distribution\nggplot(data = data.frame(x = c(-4,4)), aes(x)) +\n  stat_function(fun = dlnorm, args = list(meanlog = 2.2, sdlog = 0.44), \n                colour = \"red\") +\n  labs(title = \"Log normal distribution\",\n       subtitle = \"Log normal distribution [mean_log = 2.2 and sd_log = 0.44]\",\n       x = \"x\",\n       y = \"Probability\") +\n  theme_bw()"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#the-central-limit-theorem",
    "href": "tutorials/stat_basic/intro_stat.html#the-central-limit-theorem",
    "title": "Introductory statistics with R",
    "section": "\n8 The central limit theorem",
    "text": "8 The central limit theorem\nImagine we are rolling a die 5 times and we are calculating the mean of the results.\n\ndie &lt;- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times\nsample_of_5 &lt;- sample(die, 5, replace = T)\nsample_of_5\n\n[1] 1 1 5 6 2\n\n# Calculating the mean fo the results\nmean(sample_of_5)\n\n[1] 3\n\n\nNow imagine we are repeating the experiment of rolling a die 5 times for 10 trials and then we are calculating the mean for each trial.\n\nlibrary(dplyr)\n\ndie &lt;- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repreating it 10 times\nsample_of_5 &lt;- replicate(10, sample(die, 5, replace = T) %&gt;% mean())\n\n# Mean values for the 10 trials\nsample_of_5\n\n [1] 3.2 4.0 2.4 4.8 4.8 3.2 3.0 4.8 3.2 3.6\n\n\nLet us go further and repeat this experiment for 100 and 1000 trials and visualize the means.\n\n\nFor 10 trials\nFor 100 trials\nFor 1000 trials\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie &lt;- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 &lt;- replicate(10, sample(die, 5, replace = T) %&gt;% mean())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample means\")\n\n\n\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie &lt;- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 &lt;- replicate(100, sample(die, 5, replace = T) %&gt;% mean())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample means\")\n\n\n\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie &lt;- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 &lt;- replicate(1000, sample(die, 5, replace = T) %&gt;% mean())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample means\")\n\n\n\n\n\n\n\nYou can see that as the number of trials increases, the distribution of means reaches a normal distribution. This is a result of the central limit theorem.\n\nThe central limit theorem states that a sampling distribution of a statistic will approach a normal distribution as the number of trials increases, provided that the samples are randomly sampled and are independent.\n\nIn the above case, we can see that the mean of the samples approaches the central value or the ‚Äòexpected value‚Äô of 3.5.\nThe central limit theorem also applies to other statistics such as the standard deviation.\n\n\nFor 10 trials\nFor 100 trials\nFor 1000 trials\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie &lt;- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 &lt;- replicate(10, sample(die, 5, replace = T) %&gt;% sd())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample standard deviation\")\n\n\n\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie &lt;- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 &lt;- replicate(100, sample(die, 5, replace = T) %&gt;% sd())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample standard deviation\")\n\n\n\n\n\n\n\nCodeset.seed(123)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndie &lt;- c(1,2,3,4,5,6)\n\n# Rolling a die 5 times and repeating it 10 times\nsample_of_5 &lt;- replicate(1000, sample(die, 5, replace = T) %&gt;% sd())\n\n# Plotting the graph\nhist(sample_of_5, xlab = \"Sample standard deviation\")\n\n\n\n\n\n\n\nThus as a result of the central limit theorem, we can take multiple samples from a large population and estimate different statistics which would be an accurate estimate of the population statistic. Thus we can circumvent the difficulty of sampling the whole population and still be able to measure population statistics and make useful inferences."
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#data-transformation",
    "href": "tutorials/stat_basic/intro_stat.html#data-transformation",
    "title": "Introductory statistics with R",
    "section": "\n9 Data transformation",
    "text": "9 Data transformation\nSometimes it is easier to visualise data after transforming it rather than trying to see it in its raw format, especially if we are working with skewed data. Given below are different datasets which have different skewness to them. For each data, the quantile-quantile plot is also plotted. The quantile-quantile plot or simply called the Q-Q plot plots the normal quantiles of the data distribution on the x-axis and the quantiles of the dataset on the y-axis. If our data is normally distributed, then the normal quantile and the data quantile would be the same and will be in a straight line. Deviation from this linear nature can be a result of the skewness of the dataset and can be visualized easily using a Q-Q plot.\n\n9.1 Normal distribution\nWe have seen what a normal distribution is, now let us look at its Q-Q plot.\n\n\nNormal distriubtion\nQ-Q plot\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a normal distribution\ndata.frame(events = 0:100000, \n           pdf = rnorm(n = 0:100000, 1)) %&gt;%\n  ggplot(aes(pdf)) +\n  geom_density() +\n  labs(title = \"Normal distribution\",\n       x = \"x\",\n       y = \"Probability\") +\n  theme_bw()\n\n\n\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a q-q plot\ndata &lt;- data.frame(events = 0:100000, \n           pdf = rnorm(n = 0:100000, 1))\nggplot(data, aes(sample = pdf)) + stat_qq() + stat_qq_line() +\n    labs(title = \"Q-Q plot\",\n       subtitle = \"For a normal distribution, Q-Q plot is a straight line\",\n       x = \"Normal quantiles\",\n       y = \"Data quantiles\") +\n  theme_bw()\n\n\n\n\n\n\n\nIf our data is normally distributed then it is easier to do statistical analyses with it. Finding a correlation between two normally distributed variables also becomes straightforward. In R, we can use the cor() function to find the correlation estimate between two variables. The correlation coefficient (r) lies between -1 and 1. The magnitude of the ‚Äòr‚Äô denotes the strength of the relationship and the sign denotes the type of relationship.\n\nCorrelation coefficient (r) = 0: No relationship between the two variables\nCorrelation coefficient (r) = -1: Strong negative relationship between the two variables\nCorrelation coefficient (r) = 1: Strong positive relationship between the two variables\n\n\ndata &lt;- data.frame(x = 0:100, \n           y = 100:200)\n\n# Plotting x and y\nggplot(data, aes(x,y)) + geom_point() +\n  labs(title = \"Relationship between x and y\") +\n  theme_bw()\n\n\n\n# Finding correlation between x and y\ncor(data$x, data$y)\n\n[1] 1\n\n\nWe have a linear relationship between x and y. And we got a correlation coefficient value of 1 (in real life scenario this is next to impossible to obtain). But there are instances where we would not get a straightforward linear relationship between two variables and we would have to transform our data to make it easier to find the relationship. Before getting into data transformation, let us see different types of skewed distribution and plot their Q-Q plots.\n\n9.2 Negative or left-skewed distribution\nGiven below are graphs showing negative or left-skewed distribution, and its Q-Q plot.\n\n\nNegative or left-skewed distribution\nQ-Q plot\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a negative or left skewed distribution\ndata.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 5, 2)) %&gt;%\n  ggplot(aes(y)) +\n  geom_density() +\n  labs(title = \"Negative or left skewed distribution\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_bw()\n\n\n\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a q-q plot\ndata &lt;- data.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 5, 2))\nggplot(data, aes(sample = y)) + stat_qq() + stat_qq_line() +\n    labs(title = \"Q-Q plot for negative or left skewed distribution\",\n       subtitle = \"The tail ends of Q-Q plot is bend towards the right side of the straight line\",\n       x = \"Normal quantiles\",\n       y = \"Data quantiles\") +\n  theme_bw()\n\n\n\n\n\n\n\nYou can see that for negative or left-skewed distribution, the tail ends of the Q-Q plot are bent towards the right side of the straight line.\n\n9.3 Positive or right-skewed distribution\n\n\nPositive or right-skewed distribution\nQ-Q plot\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a positive or right skewed distribution\ndata.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 2, 5)) %&gt;%\n  ggplot(aes(y)) +\n  geom_density() +\n  labs(title = \"Positive or right skewed distribution\",\n       x = \"x\",\n       y = \"Density\") +\n  theme_bw()\n\n\n\n\n\n\n\nCodelibrary(ggplot2)\n\n# Plotting a q-q plot\ndata &lt;- data.frame(x = 0:10000, \n           y = rbeta(n = 0:10000, 2, 5))\nggplot(data, aes(sample = y)) + stat_qq() + stat_qq_line() +\n    labs(title = \"Q-Q plot for positive or right skewed distribution\",\n       subtitle = \"The tail ends of Q-Q plot is bend towards the left side of the straight line\",\n       x = \"Normal quantiles\",\n       y = \"Data quantiles\") +\n  theme_bw()\n\n\n\n\n\n\n\nYou can see that for positive or right-skewed distribution, the tail ends of the Q-Q plot bend towards the left side of the straight line.\n\n9.4 Different types data transformations\nDepending on the nature of the dataset, different transformations can be applied to better visualize the relationship between two variables. Given below are different data transformation functions which are used depending on the skewness of the data (S 2010).\n\nFor right-skewed data, the standard deviation is proportional to the mean. In this case, logarithmic transformation (log(x)) works best. The square root transformation (\\sqrt{x}) can also be used.\nIf the variance is proportional to the mean (in the case of Poisson distributions), square root transformation (\\sqrt{x}) is preferred. This happens more in the case of variables which are measured as counts e.g., number of malignant cells in a microscopic field, number of deaths from swine flu, etc.\nIf the standard deviation is proportional to the mean squared, a reciprocal transformation (\\frac{1}{x}) can be performed. Reciprocal transformation is carried out for highly variable quantities such as serum creatinine.\nOther transformations include the square transformation (x^2) and exponential transformation (e^x) which can be used for left skewed data.\n\n\nDifferent types of data transformations and their usage criteria\n\n\n\n\n\n\nTransformation\nWhen is it used?\nWhen it cannot be used?\n\n\n\nLogarithmic transformation(log(x))\nScaling large values to small values and making them fit a normal distribution\nCannot be used when there are 0 and negative values.Can be circumvented by subtracting the whole datasetwith the min value in the database\n\n\nSquare root transformation(\\sqrt{x})\nTo inflate small values and to stabilize large values\nNot applicable to negative values\n\n\nReciprocal transformation(\\frac{1}{x})\nHighly varying data\nNot applicable when there are zero values"
  },
  {
    "objectID": "tutorials/stat_basic/intro_stat.html#conlusion",
    "href": "tutorials/stat_basic/intro_stat.html#conlusion",
    "title": "Introductory statistics with R",
    "section": "\n10 Conlusion",
    "text": "10 Conlusion\nWe have completed the basics of statistics and also learned how to implement them in R. In summary we learned about;\n\nDescriptive and Inferential statistics.\nMeasures of centre: mean, median and mode\nMeasures of spread: variance, standard deviation, mean absolute deviation and interquartile range\nDistributions: binomial, normal, standard normal, Poisson, exponential, student‚Äôs t and log-normal\nCentral limit theorem\nData transformation: Skewness, Q-Q plot and data transformation functions\n\nIn the next chapter will we see how to use R for hypothesis testing."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html",
    "href": "tutorials/stat_model/glmer_stat_model.html",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "",
    "text": "TL;DR\n\n\n\nIn this article you will learn;\n\nWhat is a hierarchical model/mixed effects model?\nWhat are fixed effects and random effects?\nBuilding a linear mixed effects model\nAdding random intercept group and random effect slope\nRandom effect syntaxes used in lme4 packages\nPlotting linear mixed effects model, the residuals and the confidence intervals\nModel comparison using ANOVA\nBuilding generalized mixed effects models: Logistic and Poisson\nPlotting generalized mixed effects models\nRepeated measures data\nPaired t-test and repeated measures ANOVA is a special case of mixed effects model"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#prologue",
    "href": "tutorials/stat_model/glmer_stat_model.html#prologue",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n1 Prologue",
    "text": "1 Prologue\nThis is the fourth tutorial in the series: Statistical modelling using R. Over the past three tutorials we learned about linear models and generalized linear models. In this tutorial, we will learn how to build and understand both linear mixed effect models and generalised linear mixed effect models. They are needed to analyse datasets which have nested structures. We will learn more about it in the coming sections."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#making-life-easier",
    "href": "tutorials/stat_model/glmer_stat_model.html#making-life-easier",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n2 Making life easier",
    "text": "2 Making life easier\nPlease install and load the necessary packages and datasets which are listed below for a seamless tutorial session. (Not required but can be very helpful if you are following this tutorial code by code)\n\n# Run the following lines of code\n\n# Packages used in this tutorial\ntutorial_packages &lt;- rlang::quos(AER, broom, lme4, broom.mixed, ggeffects,\n                                 lmerTest, datasets)\n\n# Install required packages\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  install.packages,\n  character.only = TRUE\n)\n\n# Loading the libraries\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  library,\n  character.only = TRUE\n)\n\n# Datasets used in this tutorial\ndata(\"CASchools\")\ndata(\"CreditCard\")\ndata(\"RecreationDemand\")\ndata(\"sleep\")"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#what-is-a-hierarchical-model",
    "href": "tutorials/stat_model/glmer_stat_model.html#what-is-a-hierarchical-model",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n3 What is a hierarchical model?",
    "text": "3 What is a hierarchical model?\nConsider a dataset of test scores of students in different classrooms across different schools. If we take a closer look into this dataset, we can see that students can be grouped into their respective genders. This group of genders will be ‚Äònested‚Äô in the group of students. Likewise the student group is nested inside the group of classrooms which is further nested into the group of schools. This is shown in the flowchart given below (Use the scroll bar below the flowchart to navigate).\n\n\n\n\n%%{init: {'securityLevel': 'loose', 'theme':'base'}}%%\nflowchart TB\n  A[Education] --&gt; B(School 1)\n  A --&gt; C(School 2)\n  A --&gt; D(School 3)\n  B --&gt; E(Classroom 1)\n  B --&gt; F(Classroom 2)\n  C --&gt; G(Classroom 1)\n  D --&gt; H(Classroom 1)\n  D --&gt; I(Classroom 2)\n  D --&gt; J(Classroom 3)\n  E --&gt; K(Males: 30, Females: 25)\n  F --&gt; L(Males: 15, Females: 45)\n  G --&gt; M(Males: 23, Females: 10)\n  H --&gt; N(Males: 10, Females: 40)\n  I --&gt; O(Males: 0, Females: 30)\n  J --&gt; P(Males: 45, Females: 0)\n  K --&gt; Q(Test scores)\n  L --&gt; R(Test scores)\n  M --&gt; S(Test scores)\n  N --&gt; T(Test scores)\n  O --&gt; U(Test scores)\n  P --&gt; V(Test scores)\n  subgraph G_1[Schools]\n  B\n  C\n  D\n  end\n  style G_1 fill:#f77f00\n  subgraph G_2[Classrooms]\n  E\n  F\n  G\n  H\n  I\n  J\n  end\n  style G_2 fill:#fcbf49\n  subgraph G_3[Students]\n  K\n  L\n  M\n  N\n  O\n  P\n  end\n  style G_3 fill:#eae2b\n  subgraph G_4[Test Scores]\n  Q\n  R\n  S\n  T\n  U\n  V\n  end\n  style G_4 fill:#f8f1ae\n\n\n\n\n\nStudent test scores are nested under the group of students. Students themselves form a group of males and females. Building from this, students are nested under classrooms. Now classrooms follow the same route and group themselves inside schools which further forms yet another group. This is an example of nested data or hierarchical data where the data is nested within itself or has a well-defined hierarchy within itself.\nLet us take a scenario where we are introducing a new study method to students which have the potential to raise test scores. We can train students using the new method and compare the test scores to the earlier test scores before the introduction of the novel method.\nOur goal is to see if the new study method imparts an effect on the test scores of students and we hypothesise that it will improve the scores. But because of the nature of the data, the test scores can also be affected by the sample size of the students, the sex of the student, the classroom and the school, all of which can impact the test scores. Further, we are sampling the scores of the same students over time after the introduction of the novel test method. This makes the test scores a repeated measure variable and they are not independent across time. Thus our dataset is both nested and sampled across time.\nSo how do we test if the novel study method affects the test scores by accounting for nestedness and repeated measures?\nWe can see that linear models won‚Äôt do a good job at this. Then, from our understanding of generalized linear models, we might be able to tackle this question. Since we test scores which count data, we can use the Poisson model to predict ‚Äòtest scores‚Äô using ‚Äòtest method‚Äô as our main explanatory variable and use other variables like student sex, classroom size, school etc. as covariates. But in doing so we are also including the variances in test scores resulting from all the covariates present in our data. This is where mixed effect models or hierarchical models shine as they treat the covariates as ‚Äòrandom effects‚Äô and pool shared information on the means of the test scores across different groups. Mixed effect models try to account for the nestedness of the data by eliminating some of the variance brought by the random effect variables on the response variable by ‚Äòcontrolling‚Äô them. Also, since we have different sample sizes for classroom students, classrooms with lower sample sizes will be impacted more by outliers. Thus treating classroom size as a random effect can mitigate this problem.\nHere, the ‚Äòrandom effects‚Äô are the variables; sex of the student, classroom size, and school and the ‚Äòfixed effect‚Äô, which is the variable that we are interested in, is the study method. A statistical model which has both random and fixed effect variables is called a ‚Äòmixed effect‚Äô model or a ‚Äòhierarchical‚Äô model.\nThe definitions of random effect and fixed effect are ambiguous in the scientific literature and there are multiple definitions of what constitutes a random effect and a fixed effect (Gelman 2005).\nGelman (2005) suggest parting ways with the terms ‚Äòrandom‚Äô and ‚Äòfixed‚Äô and propose to view them as constant and varying;\n\nWe define effects (or coefficients) in a multilevel model as constant if they are identical for all groups in a population and varying if they are allowed to differ from group to group. For example, the model y_{ij} = Œ±_j +Œ≤x_{ij} (of units i in groups j ) has a constant slope and varying intercepts, and y_{ij} = Œ±_j + Œ≤_j x_{ij} has varying slopes and intercepts. (Gelman 2005)"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#building-a-mixed-effects-model",
    "href": "tutorials/stat_model/glmer_stat_model.html#building-a-mixed-effects-model",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n4 Building a mixed-effects model",
    "text": "4 Building a mixed-effects model\nWe will start with a linear mixed-effects model. As seen before, we use this if our residuals follow a normal or simply if our data is normally distributed. If we have count data or binomial data which is not normally distributed, then instead of building a linear mixed effects model we build a generalized linear mixed effects model. We will use the lme4 package in R to build these models.\nWe will be using the CASchools dataset from the {AER} package in R. The dataset contains test scores which are on the Stanford 9 standardized test administered to 5th-grade students. The dataset has values for 420 elementary schools and the test scores are given as the mean score in reading and their ability to do maths. Other variables correspond to the school characteristics which we will see later along the way."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#linear-mixed-effects-model",
    "href": "tutorials/stat_model/glmer_stat_model.html#linear-mixed-effects-model",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n5 Linear mixed effects model",
    "text": "5 Linear mixed effects model\nWith the CASchools dataset, we are interested in seeing if the reading scores (read) are predicted by the school expenditure (expenditure). Let us start by plotting the data.\n\nif (!require(AER)) install.packages('AER')\nlibrary(AER)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Plotting the data\nggplot(CASchools, aes(expenditure, read)) + geom_point() +\n  theme_bw() + labs(title = \"Reading scores vs Expenditure\",\n                    x = \"Expenditure per student (in dollars)\",\n                    y = \"Average reading score\")\n\n\n\n\nThere does not seem to be a general trend that higher expenditure leads to higher scores. Also, most counties tend to have expenditures between 4500 and 5500 dollars.\nNow let us try to build a linear model to predict the average reading score (read) with expenditure per student (expenditure) using the lm() function.\n\nif (!require(broom)) install.packages('broom')\nlibrary(AER)\nlibrary(broom)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm &lt;- lm(read ~ expenditure, data = CASchools)\n\n# Extracting coefficients from the model\ntidy(model_lm)\n\n\n\n  \n\n\n\nFrom the coefficient values, we can see that the slope value for expenditure per student is almost zero. This was more or less visible to us from the plot. So does it mean that expenditure has no association with the average reading scores? Before jumping to a conclusion, if we take a closer look at the data, we can see that the scores are nested within counties (county). So let us try adding the variable county to the linear model.\n\nlibrary(AER)\nlibrary(broom)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm &lt;- lm(read ~ expenditure + county, data = CASchools)\n\n# Extracting coefficients from the model\ntidy(model_lm)\n\n\n\n  \n\n\n\nThere are a total of 45 counties, so we get 44 intercept values and a global intercept value for the first county in the dataset (which is Alameda). We can also see that the slope for expenditure decreased furthermore and the standard error for the slope estimate also decreased as compared to the previous model. As for the intercepts for the respective counties, all counties as compared to Alameda county have negative intercepts. Now let us check the coefficients for the respective counties without any base county comparisons.\n\nlibrary(AER)\nlibrary(broom)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm &lt;- lm(read ~ expenditure + county - 1, data = CASchools)\n\n# Extracting coefficients from the model\ntidy(model_lm)\n\n\n\n  \n\n\n\nThe coefficient for Alameda county is the greatest out of all the counties, which is why we got all the intercept values as negative in the earlier case. Also, the p-value associated with expenditure (p = 0.54) is greater than 0.05 suggesting that expenditure does not explain the variance seen in the average reading scores.\n\n5.1 Adding a random intercept\nWhat if we only want to see how expenditure affects the reading score without the county differences? Let us build a linear mixed effects model with county as the ‚Äòrandom effect‚Äô and ‚Äòexpenditure‚Äô as the ‚Äòfixed effect‚Äô. To build a linear mixed effects model we use the function lmer() from the lme4 package in R.\nThe syntax for building a linear mixed effects model is very similar to the syntax we used while using the lm() and glm() functions. Here we are considering county as a random effect, moreover, the county variable is categorical and therefore, we are essentially considering county as a random intercept.\nThe notation for specifying random effect within the model formula is (1 | county). If we do not specify a random effect term in the model formula, the lmer() function won‚Äôt run. Without further ado, let‚Äôs see it in action.\nThe tidy() function from the broom package in R only work for models built using lm() and glm(). To extend its usage to mixed effects models, we have to install an additional package called broom.mixed\n\nif (!require(lme4)) install.packages('lme4')\nif (!require(broom.mixed)) install.packages('broom.mixed')\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer &lt;- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Extracting coefficients from the model\ntidy(model_lmer)\n\n\n\n  \n\n\n\nIf we look at the slope for expenditure, it is greater than the slope value we got from the linear model but still is almost zero. The standard error for the estimated slope is also less as compared to the earlier built models. But there is a strong role of counties on the average reading score of students. How did I know that? Let us first understand the summary of the linear mixed effects model using the summary() function.\n\nsummary(model_lmer)\n\n\n\nSummary of the linear mixed effects model\n\n\n\nIn section 1, we can see the model formula that we used to model the data. It also tells us that the model has been fitted by REML, which stands for restricted maximum likelihood method, the method by which lmer models fit the data.\nIn section 2, there is a value denoting the REML criterion at convergence. I don‚Äôt know exactly what this is, so if I learn more about it and understand it, then I will update this text. From what I read so far, I learned that this value can be a helpful model diagnostic if our model is not ‚Äòconverging‚Äô.\nSection 3 outputs the quantile, min-max and median values of the residual distances of the model.\nIn section 4, the variance and the standard deviation of both the random intercept variable (county) and the residual (as explained by expenditure) are given. You can see that the variance of counties is more than half the variance of the residual. This variance value is meant to capture all the influences of counties on the average reading score. If we estimate the percentage of variance county contributes as compared to the total variance, then we have;\n\n\ncounty_variance = 138.5\nresidual_variance = 253.7\ntotal_variance = county_variance + residual_variance\n\n# Calculating the percentage of variance of county to the total variance\ncounty_variance / total_variance\n\n[1] 0.3531362\n\n\nSo the differences between counties explain ~35% of the variance that‚Äôs ‚Äúleftover‚Äù after the variance explained by our fixed effects (which is expenditure here). If we had county variance as 0, then the percentage of the variance of counties to the total variance would be 0, which suggests that county is truly a random variable which does not explain the variance seen in the average reading scores.\n\nIn section 5, we have the coefficient estimates of the fixed effect variable, something which we have seen extensively in the previous tutorials. You can see that the slope of expenditure (0.001955) is close to zero which indicates that expenditure does not explain the differences seen in the average reading score. We saw the same case when we built a linear model using the same dataset.\nIn section 6, intercept and expenditure coefficients correlate -0.966. This is another value which I am not sure what it means. From what I read, it tells us that if we were to repeat the analysis using a newly sampled dataset, then with newly estimated coefficients of the fixed effect variable, the correlation value tells us how many of those fixed effect coefficients might be associated. You can read more about it here.\n\nNow that we got a sense of the model summary of a linear mixed effects model, let us see some more functions in R that would be useful for extracting information from the model.\n\nTo extract the coefficients of the fixed effect variables, use the fixef()\n\n\n\n# Extracting the coefficients of the fixed effects\nfixef(model_lmer)\n\n (Intercept)  expenditure \n6.454923e+02 1.954755e-03 \n\n\n\nTo extract the coefficients of the random effect variables, use the ranef()\n\n\n\n# Extracting the coefficients of the fixed effects\nranef(model_lmer)\n\n$county\n                 (Intercept)\nAlameda          11.87352086\nButte            -7.99357120\nCalaveras         2.53892633\nContra Costa     15.33621739\nEl Dorado        10.40224040\nFresno          -17.67664299\nGlenn             6.54693608\nHumboldt          7.30873017\nImperial        -13.90694256\nInyo              2.52932318\nKern            -16.86794250\nKings            -6.14912869\nLake             -7.25773124\nLassen            6.16396434\nLos Angeles      -9.68442015\nMadera           -2.26427074\nMarin            22.22849626\nMendocino        -5.18932165\nMerced          -17.55856455\nMonterey        -13.20359371\nNevada           10.83326539\nOrange           -2.23662764\nPlacer           11.61203226\nRiverside       -11.18209338\nSacramento      -13.86645226\nSan Benito       -4.01536933\nSan Bernardino   -5.97196372\nSan Diego         3.11246174\nSan Joaquin      -9.12283927\nSan Luis Obispo   6.55996266\nSan Mateo        11.56548692\nSanta Barbara     9.16868646\nSanta Clara       8.23860811\nSanta Cruz       15.92725798\nShasta            3.42269880\nSiskiyou         -3.53860484\nSonoma           10.40368948\nStanislaus        1.60937097\nSutter           -0.00741143\nTehama           -1.72590741\nTrinity          10.21145799\nTulare          -16.47382765\nTuolumne          1.76305887\nVentura          -7.73323819\nYuba              4.27007248\n\nwith conditional variances for \"county\" \n\n\n\nTo calculate the confidence intervals of the fixed effect variables, use the confint()\n\n\n\n# Calculating the confidence intervals of the fixed effects\nconfint(model_lmer)\n\n                    2.5 %       97.5 %\n.sig01       8.886716e+00 1.526132e+01\n.sigma       1.483855e+01 1.711469e+01\n(Intercept)  6.298888e+02 6.608656e+02\nexpenditure -8.340782e-04 4.796441e-03\n\n\nBy default the confint() function calculates the confidence intervals at 95% level. The displayed result from the function is symmetric over the 95% level. The interpretation of 95% CI for expenditure, in this case, would be; that if we were to repeat the experiment 100 times, then, the 95 of those mean values of the slope would fall between -0.000834 and 0.004796. We have a rather wide confidence interval which includes both a negative and a positive limit which shows that we cannot say whether expenditure decrease or increase the average reading scores.\n\nWe can also use the tidy() function by loading both the broom and broom.mixed packages to display the estimates and the confidence intervals.\n\n\nlibrary(broom)\nlibrary(broom.mixed)\n\n# Calculating the estimates and the confidence intervals of the model\ntidy(model_lmer, conf.int = T)\n\n\n\n  \n\n\n\nYou can see that the random effect variables do not have standard error values. If we compare the model summary between the linear model and the linear mixed effects model we built, we can also see that p-values are not reported for the linear mixed effects model. The author of the lme4 package, Dr.¬†Doug Bates views random effects as latent variables which do not have standard deviations and thus standard errors or p-values are not calculated for them. In addition to this, it is an open research question on how to calculate the p-values for random effects in a mixed-effects model. But using the lmerTest package in R we can perform ad-hoc analyses to report p-values for the fixed effect variables. We will see how to do this further down the section.\n\n5.2 Adding a random slope\nSo far we have seen how to input random intercepts, now we will see how to input random slopes. The default syntax for random effect in R is (continuous_predictor | random_effect_group). Thus a random slope is calculated, it also estimates a random effect intercept.\nIn the CASchools dataset, we looked at whether the average reading score is predicted by the expenditure by treating counties as the random intercept group. Now in our datasets, we also have data on the number of students and this could be different for different counties and can affect the average reading score. So this time let us build a model by treating students as the random effect variable.\n\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model using random slope and random intercept\nmodel_lmer2 &lt;- lmer(read ~ expenditure + (students | county), data = CASchools)\n\n# Extracting coefficients from the model\nsummary(model_lmer2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: read ~ expenditure + (students | county)\n   Data: CASchools\n\nREML criterion at convergence: 3593.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.67995 -0.64452  0.00953  0.66738  2.52830 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.  Corr \n county   (Intercept) 3.475e+02 18.642621      \n          students    1.354e-06  0.001164 -0.95\n Residual             2.342e+02 15.303481      \nNumber of obs: 420, groups:  county, 45\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) 6.467e+02  7.706e+00  83.918\nexpenditure 6.341e-04  1.413e-03   0.449\n\nCorrelation of Fixed Effects:\n            (Intr)\nexpenditure -0.958\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThe variance of students is very less and therefore might not be affecting the scores that much. Also, note that we got an error message in the output saying boundary (singular) fit. This generally indicates that the estimate for the random effect variable that we used is very small. In our, it means that the random effect slopes of students are very small which we can see using the ranef() function. So we can remove students from our model.\n\nranef(model_lmer2)\n\n$county\n                (Intercept)      students\nAlameda          24.4785039 -1.441929e-03\nButte            -3.4180179  1.775858e-04\nCalaveras         7.9826401 -4.681762e-04\nContra Costa     25.4891245 -1.424787e-03\nEl Dorado        17.9072929 -1.012206e-03\nFresno          -14.0623193  8.308013e-04\nGlenn            12.4209156 -7.319719e-04\nHumboldt         13.8328480 -8.081011e-04\nImperial        -12.2869252  7.016373e-04\nInyo              7.7750632 -4.532749e-04\nKern            -12.5277014  4.649777e-04\nKings            -1.0528421  1.423524e-05\nLake             -5.6736709  3.339620e-04\nLassen           12.5326580 -7.406065e-04\nLos Angeles      -2.0794607 -2.426704e-04\nMadera            2.8489240 -1.656228e-04\nMarin            34.7443486 -2.051422e-03\nMendocino        -4.6943124  2.762367e-04\nMerced          -14.2569750  7.113461e-04\nMonterey         -8.4968704  2.996187e-04\nNevada           18.6703498 -1.085232e-03\nOrange            8.6097926 -6.360256e-04\nPlacer           19.4641692 -1.099682e-03\nRiverside       -10.6949200  6.195711e-04\nSacramento      -12.6097930  7.103399e-04\nSan Benito        0.1126231 -3.550850e-05\nSan Bernardino    2.9274439 -3.988533e-04\nSan Diego        14.1621214 -8.451098e-04\nSan Joaquin      -6.2010253  3.738934e-04\nSan Luis Obispo  13.5823846 -7.993061e-04\nSan Mateo        24.7224461 -1.651387e-03\nSanta Barbara    20.4963141 -1.417024e-03\nSanta Clara      22.7310750 -1.376439e-03\nSanta Cruz       26.5648214 -1.605235e-03\nShasta           10.2142367 -6.282100e-04\nSiskiyou          2.5622804 -1.403162e-04\nSonoma           18.0499478 -1.078872e-03\nStanislaus        9.0155040 -5.928210e-04\nSutter            4.5333780 -2.714554e-04\nTehama            3.3731884 -2.120293e-04\nTrinity          19.7049606 -1.160587e-03\nTulare          -12.6199437  7.048954e-04\nTuolumne          7.6541244 -4.484524e-04\nVentura          -1.4092318 -1.002546e-04\nYuba             10.9917466 -6.477161e-04\n\nwith conditional variances for \"county\" \n\n\n\n5.3 Adding a non-correlated random effects\nPlease also note that in the model summary given above, a correlation value (-0.95) is given for the correlation between students and county. If we don‚Äôt want to specify any correlation between the random effects then instead of using | we use || to specify no correlation.\n\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model (no correlation between random effects)\nmodel_lmer3 &lt;- lmer(read ~ expenditure + (students || county), data = CASchools)\n\n# Extracting coefficients from the model\nsummary(model_lmer3)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: read ~ expenditure + ((1 | county) + (0 + students | county))\n   Data: CASchools\n\nREML criterion at convergence: 3592.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.73060 -0.62225  0.01112  0.63847  2.40069 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n county   (Intercept) 1.311e+02 11.44819\n county.1 students    2.434e-06  0.00156\n Residual             2.400e+02 15.49234\nNumber of obs: 420, groups:  county, 45\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) 6.503e+02  7.738e+00  84.039\nexpenditure 1.321e-03  1.401e-03   0.943\n\nCorrelation of Fixed Effects:\n            (Intr)\nexpenditure -0.966\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 8.66886 (tol = 0.002, component 1)\nModel is nearly unidentifiable: very large eigenvalue\n - Rescale variables?\nModel is nearly unidentifiable: large eigenvalue ratio\n - Rescale variables?\n\n\nThe model outputs a lot of error values because students and county is correlated as seen before.\n\n5.4 Adding the same variable as both fixed and random effect\nSometimes we can have a model with both the fixed effect and the random effect as the same variable. Suppose we are interested in seeing the average effect of expenditure on the average reading scores across counties. In that case, we use expenditure both as a fixed effect and as a random effect. Before using expenditure in the model formula, it is better to use the scale() function to rescale the data values for better numerical stability.\n\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Rescaling expenditure values\nCASchools_scaled &lt;- CASchools\nCASchools_scaled$expenditure_scaled &lt;- scale(CASchools_scaled$expenditure)\n\n# Building a linear mixed effects model (same fixed and random effect)\nmodel_lmer4 &lt;- lmer(read ~ expenditure + (expenditure | county), data = CASchools)\n\n# Extracting coefficients from the model\nsummary(model_lmer4)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: read ~ expenditure + (expenditure | county)\n   Data: CASchools\n\nREML criterion at convergence: 3563.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.63568 -0.62932  0.02378  0.64384  2.60189 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.  Corr \n county   (Intercept) 6.566e+02 25.623717      \n          expenditure 3.880e-05  0.006229 -0.94\n Residual             2.254e+02 15.014211      \nNumber of obs: 420, groups:  county, 45\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) 6.534e+02  9.100e+00  71.801\nexpenditure 3.237e-04  1.828e-03   0.177\n\nCorrelation of Fixed Effects:\n            (Intr)\nexpenditure -0.976\noptimizer (nloptwrap) convergence code: 0 (OK)\nunable to evaluate scaled gradient\nModel failed to converge: degenerate  Hessian with 1 negative eigenvalues\n\n\nThe table below shows all the different types of random effect syntaxes we can use in the lmer() function.\n\nDifferent random effect syntaxes used in the lme4 package\n\nRandom effect syntax\nWhat it means\n\n\n\n(1 | group)\nRandom intercept with fixed mean\n\n\n(1 | g1/g2)\nIntercepts vary among g1 and g2 within g1\n\n\n(1 | g1) + (1 | g2)\nRandom intercepts for 2 variables\n\n\nx + (x | g)\nCorrelated random slope and intercept\n\n\nx + (x || g)\nUncorrelated random slope and intercept"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#plotting-lmer-models",
    "href": "tutorials/stat_model/glmer_stat_model.html#plotting-lmer-models",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n6 Plotting lmer models",
    "text": "6 Plotting lmer models\nEarlier we had functions like fmodel() from the {statisticalModeling} package and arguments like geom_smooth() or stat_smooth() from the ggplot2 package in R to help us plot linear models and generalized linear models. Thanks to the ggeffects package in R, we have an easy of plotting our lmer models. But first, let us see how we got it with the ggplot2 package in R.\nWe will have to do some manual tidying and wrangling of our model object to extract values which we need to plot using the ggplot2 package.\nTo plot the trend line, we can use the predict() function to predict values using the training dataset itself and save those predicted values into a new column in the dataset. Then plot the trend lines them using those predicted values.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm &lt;- lm(read ~ expenditure + county, data = CASchools)\n\n# Building a linear mixed effects model\nmodel_lmer &lt;- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Saving predicted values for each model\ndata &lt;- CASchools %&gt;% mutate(predicted_lm = predict(model_lm),\n                             predicted_lmer = predict(model_lmer))\n\n# Plotting the predicted values as lines\nggplot(data, aes(x = expenditure, y = read, color = county)) +\n  geom_point() +\n  geom_line(aes(x = expenditure, y = predicted_lm)) +\n  geom_line(aes(x = expenditure, y = predicted_lmer), linetype = 'dashed') +\n  labs(title = \"Linear models vs Linear mixed effects model\",\n       x = \"Expenditure per student (in dollars)\",\n       y = \"Average reading score\") +\n  theme_bw()\n\n\n\n\nWe have 54 different counties which make the plot somewhat overwhelming.\nWe can also plot the predicted values as data points in the plot. Let us build another model without any groups using the same dataset. We will use the number of teachers in the school (teachers) as a random effect instead of the counties.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm &lt;- lm(read ~ expenditure + teachers, data = CASchools)\n\n# Building a linear mixed effects model\nmodel_lmer &lt;- lmer(read ~ expenditure + (1 | teachers), data = CASchools)\n\n# Saving predicted values for each model\ndata &lt;- CASchools %&gt;% mutate(predicted_lm = predict(model_lm),\n                             predicted_lmer = predict(model_lmer))\n\n# Plotting the predicted values as points\nggplot(data, aes(x = expenditure, y = read)) +\n  geom_point() +\n  geom_point(data = data,\n             aes(x = expenditure, y = predicted_lm),\n             color = \"red\", alpha = 0.5) +\n  geom_point(data = data,\n             aes(x = expenditure, y = predicted_lmer),\n             color = 'blue', alpha = 0.5) +\n  labs(title = \"Linear models vs Linear mixed effects model\",\n       x = \"Expenditure per student (in dollars)\",\n       y = \"Average reading score\") +\n  theme_bw()\n\n\n\n\nWith the ggeffects package, plotting a lmer model is as easy as two lines of code. Since we have 54 counties, I am not plotting them via counties, so it is not included in the terms = \"c()\" argument.\n\nif (!require(ggeffects)) install.packages('ggeffects')\nlibrary(lme4)\nlibrary(AER)\nlibrary(ggeffects)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer &lt;- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Predicting values\nmodel_lmer_predict &lt;- ggpredict(model_lmer, terms = c(\"expenditure\"))\n\n# Plotting the model\nplot(model_lmer_predict)"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#plotting-the-residuals",
    "href": "tutorials/stat_model/glmer_stat_model.html#plotting-the-residuals",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n7 Plotting the residuals",
    "text": "7 Plotting the residuals\nWe can use the base plot() function to plot the residuals of the models.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear model\nmodel_lm &lt;- lm(read ~ expenditure + county, data = CASchools)\n\n# Building a linear mixed effects model\nmodel_lmer &lt;- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Plotting the linear model diagnostics\npar(mfrow=c(2,2))\nplot(model_lm)\n\n\n\n# Plotting the linear mixed effects model diagnostics (just the residuals actually)\nplot(model_lmer)"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#plotting-the-cofidence-intervals",
    "href": "tutorials/stat_model/glmer_stat_model.html#plotting-the-cofidence-intervals",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n8 Plotting the cofidence intervals",
    "text": "8 Plotting the cofidence intervals\nWe can also plot the confidence intervals of the fixed effect variables using the ggplot() function.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer &lt;- lmer(read ~ expenditure + (1 | county), data = CASchools)\n\n# Extracting the coefficients\nmodel_lmer_coef &lt;- tidy(model_lmer, conf.int = T)\nmodel_lmer_coef1 &lt;- model_lmer_coef %&gt;% \n  filter(effect == \"fixed\" & term != \"(Intercept)\")\n\n# Plotting the 95% CI\nmodel_lmer_conf &lt;- ggplot(model_lmer_coef1, aes(x = term, y = estimate,\n                             ymin = conf.low, ymax = conf.high)) +\n  geom_hline(yintercept = 0, color = 'red') + \n  geom_point() +\n  geom_linerange() +\n  coord_flip() +\n  labs(title = \"Linear models vs Linear mixed effects model\",\n       x = \"Regression coefficient\",\n       y = \"Coefficient estimate and 95% CI\") +\n  theme_bw()\n\nmodel_lmer_conf\n\n\n\n\nLet us try including more fixed effect terms in the model. We will include the number of teachers in the school (teachers) and the number of students in the school (students) as fixed effects.\n\nlibrary(lme4)\nlibrary(AER)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(ggplot2)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer &lt;- lmer(read ~ expenditure + teachers + students + (1 | county),\n                   data = CASchools)\n\n# Extracting the coefficients\nmodel_lmer_coef &lt;- tidy(model_lmer, conf.int = T)\nmodel_lmer_coef1 &lt;- model_lmer_coef %&gt;% \n  filter(effect == \"fixed\" & term != \"(Intercept)\")\n\n# Plotting the 95% CI\nmodel_lmer_conf &lt;- ggplot(model_lmer_coef1, aes(x = term, y = estimate,\n                             ymin = conf.low, ymax = conf.high)) +\n  geom_hline(yintercept = 0, color = 'red') + \n  geom_point() +\n  geom_linerange() +\n  coord_flip() +\n  labs(title = \"Linear models vs Linear mixed effects model\",\n       x = \"Regression coefficient\",\n       y = \"Coefficient estimate and 95% CI\") +\n  theme_bw()\n\nmodel_lmer_conf"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#null-hypothesis-testing",
    "href": "tutorials/stat_model/glmer_stat_model.html#null-hypothesis-testing",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n9 Null hypothesis testing",
    "text": "9 Null hypothesis testing\nAs mentioned earlier, p values are not calculated for fixed terms using the lme4 package for various reasons. So if we have to calculate the p values, we have to use the lmerTest package in R. By loading the lmerTest package, the lme4 package is automatically loaded. We don‚Äôt have to provide any special syntax for calculating the p-values, instead, p-values are automatically calculated when calling the lmer() function.\n\nif (!require(lmerTest)) install.packages('lmerTest')\nlibrary(lmerTest)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model\nmodel_lmer &lt;- lmer(read ~ expenditure + teachers + students + (1 | county),\n                   data = CASchools)\n\n# Printing model summary\nsummary(model_lmer)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: read ~ expenditure + teachers + students + (1 | county)\n   Data: CASchools\n\nREML criterion at convergence: 3590.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7343 -0.6384  0.0471  0.6343  2.2823 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n county   (Intercept) 138.6    11.77   \n Residual             239.6    15.48   \nNumber of obs: 420, groups:  county, 45\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept) 652.737953   7.767375 412.474266  84.036   &lt;2e-16 ***\nexpenditure   0.001005   0.001404 410.295073   0.715   0.4749    \nteachers      0.085024   0.058332 391.351530   1.458   0.1458    \nstudents     -0.005188   0.002825 392.820325  -1.836   0.0671 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) expndt techrs\nexpenditure -0.964              \nteachers     0.141 -0.163       \nstudents    -0.152  0.170 -0.997\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nFor fixed effects, p-values are calculated to test if the coefficients are significantly different from zero."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#model-comparison-using-anova",
    "href": "tutorials/stat_model/glmer_stat_model.html#model-comparison-using-anova",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n10 Model comparison using ANOVA",
    "text": "10 Model comparison using ANOVA\nWe can use ANOVA to compare two models to see which one is better. The ANOVA test is called by the function anova() and it is used to compare models by comparing the amount of variability explained by each model.\nLet us test if the number of teachers significantly affects the average reading score of students across different counties. First, we will build a ‚Äònull model‚Äô with just county as the random effect. Then we will build another model with teachers as the fixed effect and the rest the same as the previous model.\n\nlibrary(lme4)\nlibrary(AER)\n\ndata(\"CASchools\")\n\n# Building a linear mixed effects model with only random effect (null model)\nmodel_lmer_null &lt;- lmer(read ~ (1 | county),\n                   data = CASchools)\n\n# Building a linear mixed effects model with fixed effect\nmodel_lmer_teacher &lt;- lmer(read ~ teachers + (1 | county),\n                   data = CASchools)\n\n# Comparing models using ANOVA\nanova(model_lmer_null, model_lmer_teacher)\n\n\n\n  \n\n\n\nFrom the ANOVA result, it seems like adding the variable teacher made the model better as it yielded significant p-values."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#generalized-linear-mixed-effects-model",
    "href": "tutorials/stat_model/glmer_stat_model.html#generalized-linear-mixed-effects-model",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n11 Generalized linear mixed effects model",
    "text": "11 Generalized linear mixed effects model\nSame way as we made the transition from linear models to generalized linear models, we now transition from linear mixed effects model to generalized mixed effects model. As you might have guessed, we will be dealing with datasets which do not follow the normal distribution and are in the form of a count or binomial type.\nWe use the glmer() function to build generalized linear mixed effects models (glmer models) and use the family = \" \" argument to specify the distribution of residuals of the data. If it‚Äôs non-normal and is count then we use family = \"poisson\" and if it‚Äôs binomial then we use family = \"binomial\".\n\n11.1 Logistic mixed effects model\nFor this exercise, we will be using the CreditCard dataset from the {AER} package in R. The dataset contains the credit history of a sample of applicants for a type of credit card. We will be building a glmer model to predict the success of the acceptance of the credit card application (card) using yearly income (income) as the fixed effect and the house ownership status (owner) as the random effect.\nLet us first build a plot and visualize the data before building the model.\n\nlibrary(ggplot2)\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Plotting the data\nggplot(CreditCard, aes(income, card, col = owner)) +\n  geom_jitter(width = 0, height = 0.05) +\n  labs(title = \"Does income affect the success in accepting a credit card application?\",\n       x = \"Yearly income (in USD 10,000)\",\n       y = \"Was the application for a credit card accepted?\") +\n  theme_bw()\n\n\n\n\nIt seems like individuals with low yearly income tend to have the most number of credit card application rejections.\nNow let us build the model.\n\nlibrary(lme4)\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic mixed effects model\nmodel_glmer_logistic &lt;- glmer(card ~ income + (1 | owner), family = \"binomial\",\n                           data = CreditCard)\n\n# Printing the summary of the model\nsummary(model_glmer_logistic)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: card ~ income + (1 | owner)\n   Data: CreditCard\n\n     AIC      BIC   logLik deviance df.resid \n  1384.1   1399.6   -689.0   1378.1     1316 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2271  0.3708  0.4626  0.6115  0.6626 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n owner  (Intercept) 0.09638  0.3105  \nNumber of obs: 1319, groups:  owner, 2\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.99357    0.27913   3.560 0.000371 ***\nincome       0.09610    0.04779   2.011 0.044338 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nincome -0.564\n\n\nFrom a quick look, it seems like income does significantly explain the variance seen in the success of accepting a credit card application.\nWe learned from logistic GLMs that it‚Äôs easier to interpret the coefficients as odds ratio by exponentiating them. The same principle follows here.\n\nlibrary(lme4)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic mixed effects model\nmodel_glmer_logistic &lt;- glmer(card ~ income + (1 | owner), family = \"binomial\",\n                           data = CreditCard)\n\n# Extracting the odds ratios\ntidy(model_glmer_logistic, exponentiate = T)\n\n\n\n  \n\n\n\nWe can see that the odds ratio for income is very close to 1, which is why the p-value associated with income is close to the level of significance of 0.05. So there is a weak effect on income in deciding the success of accepting credit card applications. There might be some other variables that we are missing to incorporate into the model.\n\n11.2 Plotting a logistic mixed effects model\nAs we saw in the case with the linear mixed effects model, we will be using the ggeffects package.\n\nlibrary(lme4)\nlibrary(ggeffects)\nlibrary(AER)\n\ndata(\"CreditCard\")\n\n# Building a logistic mixed effects model\nmodel_glmer_logistic &lt;- glmer(card ~ income + (1 | owner), family = \"binomial\",\n                           data = CreditCard)\n\n# Predicting values\nmodel_glmer_logistic_predict &lt;- ggpredict(model_glmer_logistic, \"income\")\n\n# Plotting the model\nplot(model_glmer_logistic_predict)\n\n\n\n\n\n11.3 Poisson mixed effects model\nFor this exercise, we will be using the RecreationDemand dataset from the {AER} package in R. The data is on the number of recreational boating trips to Lake Somerville, Texas, in 1980, based on a survey administered to 2,000 registered leisure boat owners in 23 counties in eastern Texas, USA.\nWe will be predicting the number of boating trips (trips) using the annual household income of the respondent (income) as the fixed effect and water-skiing status at the lake (ski) as the random effect.\nLet us first build a plot and visualize the data before building the model.\n\nlibrary(ggplot2)\nlibrary(AER)\n\ndata(\"RecreationDemand\")\n\n# Plotting the data\nggplot(RecreationDemand, aes(income, trips, col = ski)) + \n  geom_jitter(width = 0.05, height = 0) +\n  labs(title = \"Does income affect the number of recreational boating trips?\",\n       x = \"Annual household income of the respondent (in 1,000 USD)\",\n       y = \"Number of recreational boating trips\") +\n  theme_bw()\n\n\n\n\nThe graph tells us that most boating trips are made by people with less annual household income. Please note that there are many zeros in the dataset, essentially we have a zero-inflated dataset. But for now, we will ignore it. Skiing status doesn‚Äôt seem to affect the number of boating trips made.\nNow let us build the model and see.\n\nlibrary(lme4)\nlibrary(AER)\n\ndata(\"RecreationDemand\")\n\n# Building a poisson mixed effects model\nmodel_glmer_poisson &lt;- glmer(trips ~ income + (1 | ski), family = \"poisson\",\n                           data = RecreationDemand)\n\n# Printing the summary of the model\nsummary(model_glmer_poisson)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: trips ~ income + (1 | ski)\n   Data: RecreationDemand\n\n     AIC      BIC   logLik deviance df.resid \n  5454.7   5468.2  -2724.4   5448.7      656 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-2.192 -1.506 -1.291 -0.242 56.932 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ski    (Intercept) 0.09025  0.3004  \nNumber of obs: 659, groups:  ski, 2\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.42434    0.22221   6.410 1.46e-10 ***\nincome      -0.15402    0.01674  -9.199  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nincome -0.269\n\n\nSeems like income has a significant negative impact on the number of boating trips as seen earlier in the graph.\n\n11.4 Plotting a Poisson mixed effects model\nAgain we will be using the ggeffects package.\n\nlibrary(lme4)\nlibrary(ggeffects)\nlibrary(AER)\n\ndata(\"RecreationDemand\")\n\n# Building a poisson mixed effects model\nmodel_glmer_poisson &lt;- glmer(trips ~ income + (1 | ski), family = \"poisson\",\n                           data = RecreationDemand)\n\n# Predicting values\nmodel_glmer_poisson_predict &lt;- ggpredict(model_glmer_poisson,\n                                         terms = c(\"income\", \"ski\"))\n\n# Plotting the model\nplot(model_glmer_poisson_predict)"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#repeated-measures-data",
    "href": "tutorials/stat_model/glmer_stat_model.html#repeated-measures-data",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n12 Repeated measures data",
    "text": "12 Repeated measures data\nIf we measure variables across time for the same set of observations then essentially we have repeated measures data. Repeated measures are a special case of a mixed-effects model. Examples of repeated measures include; tracking test scores of students across an academic year, measuring drug effects on patients over time, measuring the growth of bacteria over time, measuring flower phenology over different seasons etc.\nWe will be using the sleep dataset from the {datasets} package in R. The data show the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients. In this case, we are measuring sleep difference in hours across time (extra) for two groups (group), essentially repeated measured data.\nFirst, let us visualize the dataset.\n\nif (!require(datasets)) install.packages('datasets')\nlibrary(datasets)\nlibrary(ggplot2)\n\ndata(\"sleep\")\n\n# Plotting the graph\nggplot(sleep, aes(group, extra, group = ID)) + geom_line() +\n  labs(title = \"Effect of two soporific drugs on sleep in 10 patients\",\n       x = \"Drug group\",\n       y = \"Increase in hours of sleep compared to control\") +\n  theme_bw()\n\n\n\n\nThe graph shows that drug 2 as compared to drug 1 increases the amount of sleep in hours when compared to control. Now let us see if this difference is statistically significant.\nIn introductory statistic classes, we would have learned that to compare means across time for this ‚Äòpaired‚Äô dataset, we can use a paired t-test. Performing a t-test in R is through the t.test() function. If we use t.test(paired = T) then essentially we are doing a paired t.test. Let us do a paired t-test on the above-mentioned dataset.\n\nlibrary(datasets)\n\ndata(\"sleep\")\n\n# Performing a paired t.test\nt.test(extra ~ group, paired = T, data = sleep)\n\n\n    Paired t-test\n\ndata:  extra by group\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58 \n\n\nThe t-test results suggest a significant difference between the mean of the sleep differences and the difference between the mean of the sleep difference between the two groups is -1.58. In other words, group 2 has a 1.58 value greater mean as compared to group 1.\nNow let us run a linear mixed effects model with extra as the response variable, group as the fixed effect and the ID as the random effect.\n\nlibrary(datasets)\n\ndata(\"sleep\")\n\n# Building a linear mixed effect model\nsummary(lmer(extra ~ group + (1 | ID), data = sleep))\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: extra ~ group + (1 | ID)\n   Data: sleep\n\nREML criterion at convergence: 70\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.63372 -0.34157  0.03346  0.31511  1.83859 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 2.8483   1.6877  \n Residual             0.7564   0.8697  \nNumber of obs: 20, groups:  ID, 10\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)   \n(Intercept)   0.7500     0.6004 11.0814   1.249  0.23735   \ngroup2        1.5800     0.3890  9.0000   4.062  0.00283 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\ngroup2 -0.324\n\n\nIf we closely compare the paired t-test result and the summary of the linear mixed effect model, we can notice some similarities.\n\nThe test statistic t-value is the same in both the results if we ignore the sign (4.062).\nThe p-value is the same (0.00283)\nThe coefficient for the group2 is the same as the mean difference denoted in the paired t-test result after ignoring the sign (1.5800).\n\nPaired t-test is a special case of repeated measures ANOVA. If we have only two dependent groups and if it‚Äôs normally distributed, then we use the paired t-test to compare their means. If we have more than 2 dependent groups, then we go repeated measures ANOVA. On the other hand, repeated measures ANOVA is a special case of the mixed effects model.\nSo to compare the means for paired data, we can go for two different statistical approaches;\n\nRegression analysis\nANOVA type approach\n\nWe already saw the regression analysis approach which is what we did earlier. Here, we build a linear mixed model and then examine if the drug group coefficient differs greatly from zero. In our case, the coefficient for group2 is 1.58 and is significantly greater than 0. So we can directly see that group2 significantly affects extra hours of sleep as compared to group1, without the need for any post hoc test which is needed if we are running an ANOVA.\nSo the null and alternate hypotheses for the regression analysis of our data would be;\n\n\nH_0 = Drug group coefficient is zero\n\nH_a = Drug group coefficient is not zero\n\nIn the second approach, we examine if the drug group covariate explains a significant amount of variability within the model. There will be no effect on the drug group if the amount of sleep the study participants get in both groups is similar. ANOVA will only tell that there is a significant difference between some groups in the model, to see which groups differ from each other, we have to go for a post hoc test.\nSo the null and alternate hypothesis for the ANOVA type approach for our data would be;\n\n\nH_0 = Drug group does not significantly explain the differences in extra sleep hours\n\nH_a = Drug group significantly explain the differences in extra sleep hours\n\nGoing forward with the ANOVA type approach can be as easy as using the function anova() by giving the model as the input.\n\nlibrary(datasets)\n\ndata(\"sleep\")\n\n# Building a linear mixed effects model\nsleep_model &lt;- lmer(extra ~ group + (1 | ID), data = sleep)\n\n# Performing a ANOVA\nanova(sleep_model)\n\n\n\n  \n\n\n\nAs we have seen earlier, the drug group does significantly explain the differences in extra sleep hours of the patients. We can also see that the p-values calculated are identical to that of the model summary we saw before."
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#fixed-effect-models-lm-and-glm-vs-mixed-effects-models-lmer-and-glmer",
    "href": "tutorials/stat_model/glmer_stat_model.html#fixed-effect-models-lm-and-glm-vs-mixed-effects-models-lmer-and-glmer",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n13 Fixed effect models (lm and glm) vs Mixed effects models (lmer and glmer)",
    "text": "13 Fixed effect models (lm and glm) vs Mixed effects models (lmer and glmer)\nWe have reached the final level of the tutorial. Here we will compare the fixed effects models and mixed effects models and get an overall picture of the differences between them.\n\nReview of fixed effects model and mixed effects model\n\n\n\n\n\nFixed effects model\nMixed effects model\n\n\n\nGenerally more impacted by outliers\nMore robust to outliers\n\n\nUse lm() or glm() in base R\nUse lmer() or glmer() from lme4 package in R\n\n\nEasier to explain\nComputationally harder to fit\n\n\nEasier to build\nMore robust to small group sizes\n\n\n\nModels nested distributions"
  },
  {
    "objectID": "tutorials/stat_model/glmer_stat_model.html#conlusion",
    "href": "tutorials/stat_model/glmer_stat_model.html#conlusion",
    "title": "Hierarchical and Mixed Effects Models in R",
    "section": "\n14 Conlusion",
    "text": "14 Conlusion\nFinally, we have completed the most needed things in statistical modelling that concerns a PhD student, especially if that student is studying Ecology. In summary, we learned the following things;\n\nWhat is a hierarchical model/mixed effects model?\nWhat are fixed effects and random effects?\nBuilding a linear mixed effects model\nAdding random intercept group and random effect slope\nRandom effect syntaxes used in lme4 packages\nPlotting linear mixed effects model, the residuals and the confidence intervals\nModel comparison using ANOVA\nBuilding generalized mixed effects models: Logistic and Poisson\nPlotting generalized mixed effects models\nRepeated measures data\nPaired t-test and repeated measures ANOVA is a special case of mixed effects model\n\nNow where to go from here? Normally, whatever is covered till now will be enough for most purposes as a student of science. Now if you the reader is still interested in learning more, then I welcome you to learn ‚ÄòNon-linear Modelling in R with Generalized Additive Models (GAM)‚Äô which would raise our modelling repertoire to even higher levels. Congratulations for making it this far, I wish you all the best üëç"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html",
    "href": "tutorials/stat_model/glm_stat_model.html",
    "title": "Generalized linear models in R",
    "section": "",
    "text": "require(\"https://cdn.jsdelivr.net/npm/juxtaposejs@1.1.6/build/js/juxtapose.min.js\")\n  .catch(() =&gt; null)"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#prologue",
    "href": "tutorials/stat_model/glm_stat_model.html#prologue",
    "title": "Generalized linear models in R",
    "section": "\n1 Prologue",
    "text": "1 Prologue\nThis is the third tutorial in the series: Statistical modelling using R. In this tutorial we will work with datasets that fail the assumptions of linear models. We will first see what these assumptions are and then we will learn about generalised linear models which is an extension of the linear model architecture. So let‚Äôs go!"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#making-life-easier",
    "href": "tutorials/stat_model/glm_stat_model.html#making-life-easier",
    "title": "Generalized linear models in R",
    "section": "\n2 Making life easier",
    "text": "2 Making life easier\nPlease install and load the necessary packages and datasets which are listed below for a seamless tutorial session. (Not required but can be very helpful if you are following this tutorial code by code)\n\n# Run the following lines of code\n\n# Packages used in this tutorial\ntutorial_packages &lt;- rlang::quos(Stat2Data, modelsummary, devtools, COUNT, broom,\n                                 cherryblossom, datasets)\n\n# Install required packages\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  install.packages,\n  character.only = TRUE\n)\ndevtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading the libraries\nlapply(lapply(tutorial_packages, rlang::quo_name),\n  library,\n  character.only = TRUE\n)\n\n# Datasets used in this tutorial\ndata(\"HorsePrices\")\ndata(\"rwm1984\")\ndata(\"run17\")\ndata(\"mtcars\")"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#understanding-the-coeffecients-in-a-linear-model",
    "href": "tutorials/stat_model/glm_stat_model.html#understanding-the-coeffecients-in-a-linear-model",
    "title": "Generalized linear models in R",
    "section": "\n3 Understanding the coeffecients in a linear model",
    "text": "3 Understanding the coeffecients in a linear model\nTo quickly recap, a linear model tries to explain the variability seen in the response variable by estimating the coefficients for explanatory variables. These coefficients are nothing but your effect size or the slope of the explanatory variable calculated from the model. Consider the following example.\nWe will use the HorsePrices dataset from the Stat2Data package in R. We will model the price of the horses in this dataset to their height and age. We will use the summary() function to output the summary of the linear model.\n\nif (!require(Stat2Data)) install.packages('Stat2Data')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building a linear model\nmodel_lm &lt;- lm(Price ~ Age + Sex, data = HorsePrices)\n\n# Plotting the model\nfmodel(model_lm) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"Price ~ Age + Height\")\n\n\n\n# Getting the summary of the linear model\nsummary(model_lm)\n\n\nCall:\nlm(formula = Price ~ Age + Sex, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24988 -10296  -1494   8286  27653 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  18199.7     4129.3   4.407 6.03e-05 ***\nAge           -220.1      393.8  -0.559    0.579    \nSexm         17008.6     3639.6   4.673 2.52e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12540 on 47 degrees of freedom\nMultiple R-squared:  0.3283,    Adjusted R-squared:  0.2997 \nF-statistic: 11.48 on 2 and 47 DF,  p-value: 8.695e-05\n\n# Calculating the effect sizes\neffect_size(model_lm, ~ Age)\n\n\n\n  \n\n\n\nIn the summary of the model, in the coefficients section, the estimate column is nothing but effect sizes. Compare the estimate for ‚Äòage‚Äô in the model summary and the effect size of ‚Äòage‚Äô calculated from the effect_size() function, both are the same.\nRecall that the equation for a straight line is;\n y = mx+c  Where m is the slope of the line and c is the y-intercept.\nWe can see from the graph that there are two straight lines, therefore there will be two equations for the straight lines. Imagine y_1 to be the equation of the straight line for females (the red line) and similarly, y_2 is for males (the blue line). The equations for the straight lines are;\n y_1 = m_1x_1 + c_1 \\\\\ny_2 = m_2x_2 + c_2\nFrom the graph, you can see that the slopes for both lines are the same. This means that the effect size of age is the same across both the sex. Also, the slope should be a negative value because as age increases, the price is decreasing for both sexes. From the model summary, the slope value for age is -220.10. Thus we have;\nm_1 = m_2 = -220.1\nNow recall how I explained that the effect size for a categorical exploratory variable is expressed in terms of change and that change is in terms of their y-intercepts.\nNow R calculates intercepts in two ways. The default way or the first way is what is shown in the model summary. In the (Intercept) row, the estimated value is 18199.7 and this value is the y-intercept value for the straight line for ‚Äòfemale‚Äô. You can easily verify this from the red line in the graph. Notice how the y-intercepts are the same as shown in the model summary and the plot. Now in the row ‚ÄòSexm‚Äô, the estimated value shows how much difference is there from the estimated value shown in the (Intercept) row or otherwise how much difference in y-intercept is there between females and males. Since the difference is positive this means the y-intercept for males is 17008.6 higher than the y-intercept for females. Thus we have;\nc_1 = 18199.7\\\\c_2 - c_1 = 17008.6\nThe intercept value by default is shown for the first level in the exploratory variable which here is female.\nThe second way is to individually show the corresponding y-intercept values for males and females. For that, we can use the expression -1 in the model formula.\n\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building a linear model showing mean for each level of the response variable\nmodel_lm &lt;- lm(Price ~ Age + Sex - 1, data = HorsePrices)\n\n# Getting the summary of the linear model\nsummary(model_lm)\n\n\nCall:\nlm(formula = Price ~ Age + Sex - 1, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24988 -10296  -1494   8286  27653 \n\nCoefficients:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nAge    -220.1      393.8  -0.559    0.579    \nSexf  18199.7     4129.3   4.407 6.03e-05 ***\nSexm  35208.2     3497.7  10.066 2.59e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12540 on 47 degrees of freedom\nMultiple R-squared:  0.8429,    Adjusted R-squared:  0.8329 \nF-statistic: 84.05 on 3 and 47 DF,  p-value: &lt; 2.2e-16\n\n\nNow you get individual y-intercept values for all the levels in the data. There will be slight decimal differences while adding up the values but the concept is the same.\nc_1 = 18199.7\\\\c_2 = 35208.2 = 17008.6 + 18199.7\nIn algebraic form, the default model formula for a linear model used in lm() is;\ny \\sim \\beta_0 + \\beta_2x_2 + \\beta_3x_3 + .. + \\epsilon And when -1 is used in the model formula, then it becomes;\ny \\sim \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + .. + \\epsilon\nSo the first way compares differences between groups by taking the first group as the reference and giving a global intercept (\\beta_0) and in a second way, we can see the actual change in the respective groups as their respective coefficients (\\beta_1, \\beta_2 etc.)."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#limitations-of-linear-models",
    "href": "tutorials/stat_model/glm_stat_model.html#limitations-of-linear-models",
    "title": "Generalized linear models in R",
    "section": "\n4 Limitations of linear models",
    "text": "4 Limitations of linear models\nFour assumptions are required to be satisfied to be able to do linear modelling;\n\nThe values in the datasets are normally distributed\nThe residuals are normally distributed\nThe values in the datasets are continuous\nVariables form a linear relationship with each other\n\nMany datasets fail these assumptions. Survival data with binary responses or data with count values are both non-normal and discontinuous. Linear modelling won‚Äôt do any good in analysing these datasets. So what is the solution? The solution is to extend linear models and generalize them so that they can accept non-normal distributions. In R, the process is simple and we have seen it being used in logistic regression in tutorial 1 using the glm() function. Therefore glm() function calls for the generalized model in R and the type of model is specified by the distribution of the data. If the data is count type then we can use the Poisson family distribution and if the data is binomial then use the binomial family distribution. We will see both of these in detail further in the tutorial. The process of generalizing a linear model is through ‚Äònon-linear link functions‚Äô which link the regression coefficients to the distribution and allow the linear model to be generalised. We will learn more about this in the later sections.\nThe syntax for the glm() function is as follows;\n\nglm(formula, data, family = \"\")\n\nFor the Poisson regression model we can specify family = \"poisson\" and for the binomial regression model we can specify family = \"binomial\".\nSince the lm() function is only applicable for normal or Gaussian distributions;\nglm(formula, data, family = \"gaussian\") = lm(formula, data)\nTherefore a linear model is a generalized linear model with the Gaussian family."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#poisson-regression-model",
    "href": "tutorials/stat_model/glm_stat_model.html#poisson-regression-model",
    "title": "Generalized linear models in R",
    "section": "\n5 Poisson regression model",
    "text": "5 Poisson regression model\nAs mentioned earlier, the Poisson model is used if the dataset contains count data. Count data can be no of things sold in a shop, no people with cancer in a city or no of visitors in a shop. Intuitively, we can say that count data ranges from 0 to infinity. There is no negative or decimal count data. Thus your dataset should only contain 0 or positive integer values for the Poisson model to work. Also, the values in the dataset should have been obtained in the same manner. For example: let‚Äôs say your dataset contains count data for the no. of people coming to the shop per day, then the same dataset should not contain values for no. of people coming to the shop per week, as they are sampled differently in terms of time. Lastly, if your dataset has a variance greater than the mean or if you have lots of zeros in your data then some modifications have to be done to be able to use the Poisson regression model.\nIn short, you should not use the Poisson regression model if you have the following conditions;\n\nDataset contains negative and/or non-integer values\nDataset is sampled across in a different manner (no. of people per day vs.¬†no. of people per week)\nDataset has a variance greater than its mean (Over-dispersed data)\nDataset has lots of zero values (Zero-inflated data)\n\nLet us build a Poisson model. We will use the rwm1984 dataset from the {COUNT} package in R. The dataset is a German health registry for the year 1984. We are interested in seeing whether the number of visits to the doctor during a year (docvis) is associated with age (age). Since we have count data we will build a Poisson model to see this association.\n\nif (!require(COUNT)) install.packages('COUNT')\nlibrary(COUNT)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson &lt;- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Printing model output\nsummary(model_poisson)\n\n\nCall:\nglm(formula = docvis ~ age, family = \"poisson\", data = rwm1984)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-3.229  -2.263  -1.271   0.326  26.383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.0983153  0.0399295  -2.462   0.0138 *  \nage          0.0273395  0.0008204  33.325   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 25791  on 3873  degrees of freedom\nResidual deviance: 24655  on 3872  degrees of freedom\nAIC: 31742\n\nNumber of Fisher Scoring iterations: 6\n\n\nApart from the summary() function, we also use the tidy() function from the broom package in R to print the outputs of the model as a data frame.\n\nif (!require(broom)) install.packages('broom')\nlibrary(COUNT)\nlibrary(broom)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson &lt;- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Printing model output using tidy()\ntidy(model_poisson)\n\n\n\n  \n\n\n\nSometimes we are interested in extracting the coefficients or finding out the confidence intervals from the model.\n\nlibrary(COUNT)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson &lt;- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Extracting the coefficients\ncoef(model_poisson)\n\n(Intercept)         age \n-0.09831529  0.02733953 \n\n# Estimating the confidence interval\nconfint(model_poisson)\n\n                 2.5 %      97.5 %\n(Intercept) -0.1767936 -0.02027057\nage          0.0257330  0.02894895"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#logistic-regression-model",
    "href": "tutorials/stat_model/glm_stat_model.html#logistic-regression-model",
    "title": "Generalized linear models in R",
    "section": "\n6 Logistic regression model",
    "text": "6 Logistic regression model\nIn tutorial 1, we briefly saw what logistic regression was. We use the logistic model when;\n\nData is binary (0/1)\nData has only two mutually exclusive values (Alive/Dead or Yes/No or Win/Lose or Pass/Fail)\nData has only two values choices or behaviour (Non-veg/Veg, Wet/Dry)\n\nIn short, for a logistic regression model, the model outputs probabilities (p) for binary outcomes in the response variable (y).\ny = Binomial(p)\nLet us build a logistic model using run17 dataset from the cherryblossom package in R. We have seen this dataset in the first tutorial. The dataset contains details for all 19,961 runners in the 2017 Cherry Blossom Run, which is an annual road race that takes place in Washington, DC, USA. The Cherry Blossom Run has two events; a 10 Mile marathon and a 5 Km run. We will be building a logistic model to whether event choice is predicted by the participant‚Äôs age.\n\nif (!require(cherryblossom)) install.packages('cherryblossom')\nlibrary(dplyr)\nlibrary(cherryblossom)\n\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy &lt;- run17\nrun17_tidy$event &lt;- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event &lt;- as.factor(run17_tidy$event)\n\n# Building a logistic model\nmodel_binomial &lt;- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Printing model output\nsummary(model_binomial)\n\n\nCall:\nglm(formula = event ~ age, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1471   0.4963   0.5102   0.5285   0.6241  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.250155   0.074991  30.006  &lt; 2e-16 ***\nage         -0.008390   0.001897  -4.423 9.74e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15130  on 19959  degrees of freedom\nResidual deviance: 15111  on 19958  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 15115\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n6.1 Bernouli and binomial distributions\nBoth Bernoulli and binomial distributions form the basis for logistic regression. The main difference between these two distributions is that Bernoulli is associated with probabilities of a result in a single event whereas binomial is associated with probabilities of a result in multiple events. For example; the chance of getting a head in a single coin toss follows a Bernoulli distribution and the chance of getting a head in 10 coin tosses follows a binomial distribution.\nIn R we have the option to either use binomial or Bernoulli. So how do we know when to use which? The short answer is that it depends on the data structure. For wide format data, the binomial distribution is used and for long format data, the Bernoulli distribution is used. You can learn more about long format and wide format data here.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy &lt;- run17 %&gt;% dplyr::select(event, age)\nrun17_tidy$event &lt;- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event &lt;- as.factor(run17_tidy$event)\n\n# Long format\nhead(run17_tidy)\n\n\n\n  \n\n\n# Converting the dataset to wide format\nrun17_wide &lt;- run17_tidy %&gt;% group_by(age, event) %&gt;%\n  summarise(count = n())\n\nrun17_wide &lt;- run17_wide %&gt;% pivot_wider(names_from = \"event\", values_from = \"count\")\nrun17_wide[is.na(run17_wide)] &lt;- 0\n\n# Wide format\nhead(run17_wide)\n\n\n\n  \n\n\n\nIn the above case, for long format data, we have a single entry in each row corresponding to individual data but in the wide format, we have each row entry corresponding to a single age group. Another way of expressing the data in a wide format is to calculate the percentage of choices, which is by dividing the total number of choices for the first category by the total number of choices for both categories.\nSo for wide format data; If our data structure has the absolute value of the no. of choices made then we use cbind() in the model formula (also called as the Wilkinson-Rogers format). If we have percentages, then we use weights = () in the glm() function.\nSo in summary we choose between Bernoulli and binomial by checking the following questions;\n\nIs the data in long or wide format?\nDo we have individual or group data?\nAre we interested in individuals or groups?\n\nLet us build logistic models for both long format and wide format data.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy &lt;- run17 %&gt;% dplyr::select(event, age)\nrun17_tidy$event &lt;- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event &lt;- as.factor(run17_tidy$event)\n\n# Building a logistic model with long format\nmodel_logistic_long &lt;- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Converting the dataset to wide format\nrun17_tidy_1 &lt;- run17_tidy %&gt;% mutate(event_choice = ifelse(event == \"Ten_Mile\", 1, 0))\n\nrun17_wide &lt;- run17_tidy_1 %&gt;% group_by(age, event) %&gt;%\n  summarise(count = n())\n\nrun17_wide &lt;- run17_wide %&gt;% pivot_wider(names_from = \"event\", values_from = \"count\")\nrun17_wide[is.na(run17_wide)] &lt;- 0\n\n# Building a logistic model with wide format\n# We use cbind() to bind the two choices\nmodel_logistic_wide_1 &lt;- glm(cbind(Five_Km, Ten_Mile) ~ age,\n                             data = run17_wide, family = \"binomial\")\n\n# Building a logistic model with wide format\n# We use percentage of choice and the weight or number of observations per group\nrun17_wide$prect_ten_mile &lt;- run17_wide$Ten_Mile / (run17_wide$Ten_Mile + run17_wide$Five_Km)\n\nmodel_logistic_wide_2 &lt;- glm(prect_ten_mile ~ age, \n                             data = run17_wide, family = \"binomial\",\n                             weights = (run17_wide$Ten_Mile + run17_wide$Five_Km))\n\n# Printing model outputs\nsummary(model_logistic_long)\n\n\nCall:\nglm(formula = event ~ age, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1471   0.4963   0.5102   0.5285   0.6241  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.250155   0.074991  30.006  &lt; 2e-16 ***\nage         -0.008390   0.001897  -4.423 9.74e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15130  on 19959  degrees of freedom\nResidual deviance: 15111  on 19958  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 15115\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(model_logistic_wide_1)\n\n\nCall:\nglm(formula = cbind(Five_Km, Ten_Mile) ~ age, family = \"binomial\", \n    data = run17_wide)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.9381  -0.6116   0.4472   1.7556   7.5007  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.245070   0.074961 -29.950  &lt; 2e-16 ***\nage          0.008267   0.001897   4.358 1.31e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 411.90  on 79  degrees of freedom\nResidual deviance: 393.17  on 78  degrees of freedom\nAIC: 711.65\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(model_logistic_wide_2)\n\n\nCall:\nglm(formula = prect_ten_mile ~ age, family = \"binomial\", data = run17_wide, \n    weights = (run17_wide$Ten_Mile + run17_wide$Five_Km))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-7.5007  -1.7556  -0.4472   0.6116   2.9381  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.245070   0.074961  29.950  &lt; 2e-16 ***\nage         -0.008267   0.001897  -4.358 1.31e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 411.90  on 79  degrees of freedom\nResidual deviance: 393.17  on 78  degrees of freedom\nAIC: 711.65\n\nNumber of Fisher Scoring iterations: 4\n\n\nIn summary we have three data formats for logistic regression\n\nBinary format\n\n\n# Binary: y = 0 or 1\nmodel_logistic_long &lt;- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n\nWilkinson-Rogers format (the ‚Äòcbind‚Äô format)\n\n\n# Wilkinson-Rogers: cbind(success, failure)\nmodel_logistic_wide_1 &lt;- glm(cbind(Five_Km, Ten_Mile) ~ age,\n                             data = run17_wide, family = \"binomial\")\n\n\nWeighted format\n\n\n# Weighted format: y = 'proportions', weights = 'totals'\nmodel_logistic_wide_2 &lt;- glm(prect_ten_mile ~ age, \n                             data = run17_wide, family = \"binomial\",\n                             weights = (run17_wide$Ten_Mile + run17_wide$Five_Km))\n\nPlease note that all three models give the same coefficients but the model with the long format structure has higher degrees of freedom as compared to the other two models with the wide format."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#link-functions",
    "href": "tutorials/stat_model/glm_stat_model.html#link-functions",
    "title": "Generalized linear models in R",
    "section": "\n7 Link functions",
    "text": "7 Link functions\nThe ‚Äòlink function‚Äô is what enables us to generalize a linear model so that we can use it in datasets which do not meet the assumptions of a linear model. In a way, it is a function which transforms the coefficients of the explanatory variables to form a linear combination with each other like in the case of a linear regression model.\n\nFor poisson family models, the link function is log and thus the coefficients of the model are in log-scale.\nFor ‚Äòbinomial‚Äô family models, there are two link functions; logit and probit\n\n\nThe logit link function transforms the probabilities to ‚Äòlog-odds values‚Äô which are real numbers ranging from -\\infty to +\\infty and gives rise to a linear equation similar to what is seen in a linear regression model. In logistic regression, by default, the logit link function is used.\nlogit(p) = m_1x_1 + c_1 = log(\\frac{p}{1-p}) The probit link function is also similar to the logit link but is based on the cumulative standard normal distribution function. By default, the logistic regression in the glm() function uses the logit link function.\nTo use another link function, we should specify the link function in the family;\nfamily = \"binomial\" = family = binomial(link = \"probit\"): By default, logit link function is used\nfamily = binomial(link = \"probit\"): Using the probit link function\nLook at the graphs given below. Use the slider to compare them. You can see that the model with the logit link function has a slightly longer or fatter tail, as both the tail ends reaches the limit slower as compared to the model with the probit link function. Therefore, in general, the logit link function is better at modelling outliers or rare events as compared to probit.\n\nCodelibrary(datasets)\nlibrary(statisticalModeling)\n\n# Building a logistic model with logit link which is the default\nmtcars_logit &lt;- glm(vs ~ mpg, data=mtcars, family= \"binomial\")\n\n# Building a logistic model with probit link which is the default\nmtcars_probit &lt;- glm(vs ~ mpg, data=mtcars, family=binomial(link=\"probit\"))\n\n# Plotting logistic model with logit link\nfmodel(mtcars_logit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with logit link)\")\n\n# Plotting logistic model with probit link\nfmodel(mtcars_probit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with probit link)\")\n\n\n\n\nlibrary(datasets)\nlibrary(statisticalModeling)\n\n# Building a logistic model with logit link which is the default\nmtcars_logit &lt;- glm(vs ~ mpg, data=mtcars, family= \"binomial\")\n\n# Building a logistic model with probit link which is the default\nmtcars_probit &lt;- glm(vs ~ mpg, data=mtcars, family=binomial(link=\"probit\"))\n\n# Plotting logistic model with logit link\nfmodel(mtcars_logit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with logit link)\")\n\n\n\n# Plotting logistic model with probit link\nfmodel(mtcars_probit) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"VS ~ mpg (logistic model with probit link)\")"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#simulating-logit-and-probit",
    "href": "tutorials/stat_model/glm_stat_model.html#simulating-logit-and-probit",
    "title": "Generalized linear models in R",
    "section": "\n8 Simulating logit and probit",
    "text": "8 Simulating logit and probit\nWe can simulate a logit distribution using the plogis() and the rbinom() functions together. The rbinom() is a random number generator function which has additional input parameters;\nn = Number of random numbers to generate size = Number of trails p = Probability of success\n\n# Converting a logit scale value to probability value\np &lt;- plogis(2)\n\n# Simulating a logit distribution\nrbinom(n = 10, size = 1, prob = p)\n\n [1] 0 1 1 1 1 1 1 1 1 1\n\n\nA probit distribution can be made using the pnrom() and the rbinom() functions together.\n\n# Converting a probit scale value to probability value\np &lt;- pnorm(2)\n\n# Simulating a probit distribution\nrbinom(n = 10, size = 1, prob = p)\n\n [1] 1 1 1 1 1 1 1 1 1 1"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#understanding-the-coefficients-in-a-poisson-model",
    "href": "tutorials/stat_model/glm_stat_model.html#understanding-the-coefficients-in-a-poisson-model",
    "title": "Generalized linear models in R",
    "section": "\n9 Understanding the coefficients in a Poisson model",
    "text": "9 Understanding the coefficients in a Poisson model\nThe link function for the Poisson regression model is the log function. Now we will see how to interpret the coefficients/effect sizes of a Poisson regression model. Let us quickly recall how we interpreted the coefficients/effect sizes of a linear regression model. We will use our earlier example but with only ‚ÄòSex‚Äô as the exploratory variable.\n\nlibrary(Stat2Data)\n\ndata(\"HorsePrices\")\n\n# Building a linear model\nmodel_lm &lt;- lm(Price ~ Sex, data = HorsePrices)\n\n# Getting the summary of the linear model\nsummary(model_lm)\n\n\nCall:\nlm(formula = Price ~ Sex, data = HorsePrices)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-23730 -10061  -1255   8495  28495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    16505       2783   5.931 3.20e-07 ***\nSexm           17225       3593   4.794 1.62e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12450 on 48 degrees of freedom\nMultiple R-squared:  0.3238,    Adjusted R-squared:  0.3097 \nF-statistic: 22.98 on 1 and 48 DF,  p-value: 1.619e-05\n\n\nIn algebraic form, the model formula for a linear model is;\ny \\sim \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .. + \\epsilon From the model summary, we have;\ny = Price\\\\ \\beta_0 = 16505 = Reference\\,intercept\\,(female)\\\\ \\beta_1 = 17225 = Difference\\,from\\,the\\,reference\\,intercept\\,(for\\,male)\\\\ \\epsilon = Error\\,value\\,of\\,the\\,model\nWe also have;\n\\beta_0 + \\beta_1 = Average\\,price\\,of\\,male\\,horses This shows that the coefficient in a linear model is additive. But when we come to the Poisson model, the link function is a log function which takes in the parameter \\lambda of the Poisson distribution. We have;\nParameter\\,of\\,poisson\\,distribution:\\lambda = e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .. + \\epsilon}\\\\ Link\\,function: ln(\\lambda) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + .. + \\epsilon\nHere the coefficients are no longer additive as they are in log scale. Therefore the coefficients are multiplicative. Let us look at the Poisson regression model we created earlier;\n\nlibrary(COUNT)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson &lt;- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Printing model outputs\nsummary(model_poisson)\n\n\nCall:\nglm(formula = docvis ~ age, family = \"poisson\", data = rwm1984)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-3.229  -2.263  -1.271   0.326  26.383  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.0983153  0.0399295  -2.462   0.0138 *  \nage          0.0273395  0.0008204  33.325   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 25791  on 3873  degrees of freedom\nResidual deviance: 24655  on 3872  degrees of freedom\nAIC: 31742\n\nNumber of Fisher Scoring iterations: 6\n\n\nHere we have;\ny = No.\\,of\\,days\\,of\\,doctor\\,visits\\\\ \\beta_0 = -0.0983153 = Reference\\,intercept\\\\ \\beta_1 = 0.0273395 = Difference\\,from\\,the\\,reference\\,intercept\\\\ \\epsilon = Error\\,value\\,of\\,the\\,model\nNow to find the average effect of age on the no. of days of doctor visits, we have to multiply \\beta_0 and \\beta_1 instead adding the values like in the case of a linear model.\n\\beta_0 * \\beta_1 = ln(average\\,effect\\,of\\,age\\,on\\,the\\,no.\\,of\\,days\\,of\\,doctor\\,visits)\\\\ e^{\\beta_0 + \\beta_1} = average\\,effect\\,of\\,age\\,on\\,the\\,no.\\,of\\,days\\,of\\,doctor\\,visits\nBut if we exponentiate the coefficients, then we get the raw value, which we can add like in the case of a linear model."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#plotting-a-poisson-model",
    "href": "tutorials/stat_model/glm_stat_model.html#plotting-a-poisson-model",
    "title": "Generalized linear models in R",
    "section": "\n10 Plotting a Poisson model",
    "text": "10 Plotting a Poisson model\nYou can either use the fmodel() function from the {statisticalModeling} package or use the ggplot() function from the ggplot2 package.\nFor the ggplot() function, we specify ‚Äòpoisson‚Äô model in the method.args = list(family = \" \") parameter inside geom_smooth() argument.\n\nlibrary(COUNT)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\ndata(\"rwm1984\")\n\n# Building a poisson model\nmodel_poisson &lt;- glm(docvis ~ age, data = rwm1984, family = \"poisson\")\n\n# Plotting using the fmodel() function\nfmodel(model_poisson) + geom_point(data = rwm1984) +\n  theme_bw() +\n  labs(title = \"docvis ~ age (poisson model)\")\n\n\n\n# Plotting using the ggplot() function\nggplot(data = rwm1984, aes(age, docvis)) +\n  geom_jitter(width = 0.05, height = 0.05) +\n  geom_smooth(method = 'glm', method.args = list(family = 'poisson')) +\n  theme_bw() +\n  labs(title = \"docvis ~ age (poisson model)\")"
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#understanding-the-coefficients-in-a-logistic-model",
    "href": "tutorials/stat_model/glm_stat_model.html#understanding-the-coefficients-in-a-logistic-model",
    "title": "Generalized linear models in R",
    "section": "\n11 Understanding the coefficients in a logistic model",
    "text": "11 Understanding the coefficients in a logistic model\nIn a linear model, the coefficients were straight forward and in the Poisson model, after exponentiating the coefficients, the values were similar to that of the linear model. But for a logistic model transformation won‚Äôt help and it‚Äôs not as straightforward as compared to the other models. So instead, the coefficient values are converted to odds ratio. The odds ratio calculates the relative odds of two events occurring. For example, let us imagine a football team that is expected to win 10 games for every game they lose. Then the odds of winning the odds of losing would be 10 to 1. And the odds ratio would be 10/1 which is 10.\nAs mentioned earlier, the default link function for a logistic model is logit, so our coefficients start as ‚Äòlog-odds‚Äô which when exponentiated will give us the odds ratio.\nIn general, odds-ratio values are interpreted in the following way;\n\nOR = 1: Coefficient has no effect\nOR &lt; 1: Coefficient decreased the odds\nOR &gt; 1: Coefficient increases the odds\n\nImagine a football team is singing two players; A and B. Odds-ratio of winning is 3 if player A joins the team, which means, in that season, if player A joins, the team is bound to win 3 games for every game they lose. Likewise, if player B joins let‚Äôs say the odds ratio is 0.5, which means they will win 1 game for every 2 games they lose. The odds ratio is often reported along the 95% confidence intervals.\nLet us calculate the odds ratio and the confidence interval of the model we previously created. We will use the tidy() function from broom package in R.\n\nlibrary(dplyr)\nlibrary(broom)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy &lt;- run17\nrun17_tidy$event &lt;- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event &lt;- as.factor(run17_tidy$event)\n\n# Building a logistic model\nmodel_binomial &lt;- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Finding the odds ratio and confidence interval\ntidy(model_binomial, exponentiate = T, conf.int = T)\n\n\n\n  \n\n\n\nThe odds ratio of age to the event is ~ 0.9916 which is almost 1. This suggests that when age increases by 1 year, there 50% chance that the participant chooses 5Km over 10 Mile event run (the odds are 1 to 1). So basically age does not seem to have an association with event choice. Also note that for the 95% confidence interval, the upper bound almost includes 1 which further reinforces the notion that age is not associated with the event choice."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#plotting-a-logistic-model",
    "href": "tutorials/stat_model/glm_stat_model.html#plotting-a-logistic-model",
    "title": "Generalized linear models in R",
    "section": "\n12 Plotting a logistic model",
    "text": "12 Plotting a logistic model\nLike earlier, we can either use fmodel() function from the {statisticalModeling} package or use the ggplot() function from the ggplot2 package.\nFor the ggplot() function, we specify the ‚Äòbinomial‚Äô model in the method.args = list(family = \" \") parameter inside geom_smooth() argument. Converting the event to numeric form and subtracting 1 from it will make the choice values stay between zero and one. In the ggplot2 graph, the value 1 corresponds to the 10 Mile run and the value 0 corresponds to the 5 Km run.\n\nlibrary(dplyr)\nlibrary(cherryblossom)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy &lt;- run17 %&gt;% dplyr::select(event, age)\nrun17_tidy$event &lt;- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event &lt;- as.factor(run17_tidy$event)\n\n# Building a logistic model with logit link\nmodel_logistic &lt;- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the logistic model with link function using fmodel()\nfmodel(model_logistic) + theme_bw() +\n  labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n# Plotting the logistic model with link function using ggplot()\nggplot(run17_tidy, aes(age, as.numeric(event) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = 'glm', method.args = list(family = \"binomial\")) +\n  theme_bw() + labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n\nWe can also plot for different link functions in the logistic model.\n\nlibrary(dplyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy &lt;- run17 %&gt;% dplyr::select(event, age)\nrun17_tidy$event &lt;- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event &lt;- as.factor(run17_tidy$event)\n\n# Building a logistic model with logit link\nmodel_logistic_long &lt;- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the logistic model with logit and probit link function using ggplot()\n# Same graph as the earlier one\nggplot(run17_tidy, aes(age, as.numeric(event) - 1)) + geom_jitter(width = 0, height = 0.05) +\n  geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'logit')),\n              colour = \"red\") +\n  geom_smooth(method = 'glm', method.args = list(family = binomial(link = 'probit')),\n              colour = \"green\") + \n  theme_bw() + labs(title = \"event ~ age (logistic model; red: logit, green: probit)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n\nAs you can see the lines corresponding to logit and probit link functions are very similar."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#multiple-regression-with-glms",
    "href": "tutorials/stat_model/glm_stat_model.html#multiple-regression-with-glms",
    "title": "Generalized linear models in R",
    "section": "\n13 Multiple regression with GLMs",
    "text": "13 Multiple regression with GLMs\nAs seen with linear models, GLMs are also affected by collinearity. Therefore, the order of the explanatory variables in the model formula matters for GLMs, if there exists a correlation between the explanatory variables used. To check the correlation between two variables we can use the cor() function.\nLet us look at this in action using the logistic model we created earlier, but this time we will also add the pace_sec variable which tells the average time (in seconds) to complete a mile.\n\nlibrary(dplyr)\nlibrary(cherryblossom)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy &lt;- run17\nrun17_tidy$event &lt;- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event &lt;- as.factor(run17_tidy$event)\n\n# Finding the correlation between pace_sec and age\n# Remove any NAs prior to finding the correlation\nrun17_tidy &lt;- drop_na(run17_tidy)\ncor(run17_tidy$pace_sec, run17_tidy$age)\n\n[1] 0.1620674\n\n# Building a logistic model wiht pace_sec + age\nmodel_logistic_1 &lt;- glm(event ~ pace_sec + age, data = run17_tidy, family = \"binomial\")\n\n# Building a logistic model wiht age + pace_sec\nmodel_logistic_2 &lt;- glm(event ~ age + pace_sec, data = run17_tidy, family = \"binomial\")\n\n# Printing the results\nsummary(model_logistic_1)\n\n\nCall:\nglm(formula = event ~ pace_sec + age, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2235   0.1997   0.3203   0.4770   1.2549  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  8.4756604  0.1606570  52.756  &lt; 2e-16 ***\npace_sec    -0.0107900  0.0002218 -48.650  &lt; 2e-16 ***\nage          0.0154362  0.0022611   6.827 8.69e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15118  on 19956  degrees of freedom\nResidual deviance: 11404  on 19954  degrees of freedom\nAIC: 11410\n\nNumber of Fisher Scoring iterations: 6\n\nsummary(model_logistic_2)\n\n\nCall:\nglm(formula = event ~ age + pace_sec, family = \"binomial\", data = run17_tidy)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2235   0.1997   0.3203   0.4770   1.2549  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  8.4756604  0.1606570  52.756  &lt; 2e-16 ***\nage          0.0154362  0.0022611   6.827 8.69e-12 ***\npace_sec    -0.0107900  0.0002218 -48.650  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 15118  on 19956  degrees of freedom\nResidual deviance: 11404  on 19954  degrees of freedom\nAIC: 11410\n\nNumber of Fisher Scoring iterations: 6\n\n\nYou can see that the model summary is identical, even with different ordering of the exploratory variables. This is because the correlation coefficient is ~ 0.17 which means there is a weak correlation between the used exploratory variables. So before building a model it‚Äôs a good practice to check if the exploratory variables exhibit collinearity."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#assumptions-of-glms",
    "href": "tutorials/stat_model/glm_stat_model.html#assumptions-of-glms",
    "title": "Generalized linear models in R",
    "section": "\n14 Assumptions of GLMs",
    "text": "14 Assumptions of GLMs\nGLMs extend the linear model via link function which transforms the coefficient into a linear combination which can be interpreted in a similar way to that seen in linear models. Nevertheless, there are important assumptions which are to be met by the data to apply any GLM.\n\nSimpson‚Äôs paradox does not occur\n\nSimpson‚Äôs paradox occurs when we fail to include an exploratory variable which significantly changes the model output after its addition. Consider the graphs given below. Please use the slider to compare them.\n\nCodelibrary(dplyr)\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy &lt;- run17\nrun17_tidy$event &lt;- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event &lt;- as.factor(run17_tidy$event)\n\n# Building a logistic model with age\nmodel_logistic_1 &lt;- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_1) + theme_bw() +\n  labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n# Building a logistic model with age + pace_sec\nmodel_logistic_2 &lt;- glm(event ~ age + pace_sec, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_2) + theme_bw() +\n  labs(title = \"event ~ age + pace_sec (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n\nlibrary(dplyr)\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\n\ndata(\"run17\")\n\n# Tidying the data\nrun17_tidy &lt;- run17\nrun17_tidy$event &lt;- recode(run17_tidy$event, \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_Km\")\nrun17_tidy$event &lt;- as.factor(run17_tidy$event)\n\n# Building a logistic model with age\nmodel_logistic_1 &lt;- glm(event ~ age, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_1) + theme_bw() +\n  labs(title = \"event ~ age (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n# Building a logistic model with age + pace_sec\nmodel_logistic_2 &lt;- glm(event ~ age + pace_sec, data = run17_tidy, family = \"binomial\")\n\n# Plotting the model\nfmodel(model_logistic_2) + theme_bw() +\n  labs(title = \"event ~ age + pace_sec (logistic model)\",\n                    y = \"Probability to choose the 10 Mile marathon over the 5 Km run\")\n\n\n\n\n\nIn the first model, predicting event choice with just age suggests that with increasing age, there is less chance that people will participate in the 10 Mile marathon. But when you also add the pace of participants into the model formula then you get completely different results (and absurd too!). The second model indicates that participants with a high pace (shorter y-axis values in the graph) irrespective of age will almost always choose the 10 Mile marathon. So our interpretation is completely changed and this is an example of Simpson‚Äôs paradox.\n\nVariables follow linear relationship and are monotonic\n\nLike in the case of linear models, GLMs also require that the variables follow a linear change. Monotonic means that the change is either always increasing or decreasing.\n\nVariables should independent\n\nThe effect sizes of one variable should not influence the effect size of another variable. We have seen this in detail for linear models. If they are independent, then we have to represent them as interaction terms. In this case, the order also matters while inputting them into the model formula.\n\nOver dispersion\n\nOverdispersion occurs when the datasets have a lot of zeros or ones in the case of the binomial distribution and a lot of zeros in the case of the Poisson distribution. Also if the variance is greater than the mean of the data, then again the model is over-dispersed."
  },
  {
    "objectID": "tutorials/stat_model/glm_stat_model.html#conclusion",
    "href": "tutorials/stat_model/glm_stat_model.html#conclusion",
    "title": "Generalized linear models in R",
    "section": "\n15 Conclusion",
    "text": "15 Conclusion\nThis marks the end of the introduction to generalised linear models. In summary, we learned;\n\nUnderstanding the coefficients in a linear model\nWhy do linear models fail for some datasets and what are the limitations of a linear model\nTwo types of GLMs: Poisson and Logistic\nLink functions\nUnderstanding the coefficients in a Poisson and logistic models\nPlotting Poisson and logistic models\nBrief introduction to multiple regression using GLMs\nAssumptions of GLMs: When to use GLMs\n\nIn the next tutorial, we will take our understanding of GLMs to the next level and learn about ‚ÄòHierarchical and Mixed Effects Models in R‚Äô. See you then!"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html",
    "href": "tutorials/stat_model/inter_stat_model.html",
    "title": "Intermediate statistical modelling in R",
    "section": "",
    "text": "require(\"https://cdn.jsdelivr.net/npm/juxtaposejs@1.1.6/build/js/juxtapose.min.js\")\n  .catch(() =&gt; null)"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#prologue",
    "href": "tutorials/stat_model/inter_stat_model.html#prologue",
    "title": "Intermediate statistical modelling in R",
    "section": "\n1 Prologue",
    "text": "1 Prologue\nThis tutorial serves as a sequel to the tutorial post: Introduction to statistical modelling in R. In this tutorial, you will gain knowledge in intermediate statistical modelling using R. You will learn more about effect sizes, interaction terms, total change and partial change, bootstrapping and collinearity."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#making-life-easier",
    "href": "tutorials/stat_model/inter_stat_model.html#making-life-easier",
    "title": "Intermediate statistical modelling in R",
    "section": "\n2 Making life easier",
    "text": "2 Making life easier\nPlease install and load the necessary packages and datasets which are listed below for a seamless tutorial session. (Not required but can be very helpful if you are following this tutorial code by code)\n\n# Libraries used in this tutorial\ninstall.packages('NHANES')\ninstall.packages('devtools')\ndevtools::install_github(\"dtkaplan/statisticalModeling\")\ninstall.packages('mosaicData')\ninstall.packages('mosaicModel')\ninstall.packages('Stat2Data')\ninstall.packages('cherryblossom')\ninstall.packages(\"tidyverse\")\ninstall.packages(\"mosaic\")\n\n# Loading the libraries\ntutorial_packages &lt;- c(\"cherryblossom\", \"mosaicData\", \"NHANES\",\n                 \"Stat2Data\", \"tidyverse\", \"mosaic\", \"mosaicModel\", \"statisticalModeling\")\n\nlapply(tutorial_packages, library, character.only = TRUE)\n\n# Datasets used in this tutorial\ndata(\"NHANES\") # From NHANES\ndata(\"SwimRecords\") # From mosaicData\ndata(\"Tadpoles\") # From mosaicModel\ndata(\"HorsePrices\") # From Stat2Data\ndata(\"run17\") # From cherryblossom\ndata(\"Oil_history\") # From statisticalModeling\ndata(\"SAT\") # From mosaicData"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#effect-size-when-response-variable-is-categorical",
    "href": "tutorials/stat_model/inter_stat_model.html#effect-size-when-response-variable-is-categorical",
    "title": "Intermediate statistical modelling in R",
    "section": "\n3 Effect size when response variable is categorical",
    "text": "3 Effect size when response variable is categorical\nIn the last tutorial, we learned how to calculate the effect size for an explanatory variable. To recall, we learned that;\n\n\n\n\n\n\n\nExploratory variable type\nEffect size\nError\n\n\n\nNumerical\nRate of change (slope)\nMean of the square of the prediction error (m.s.e)\n\n\nCategorical\nDifference (change)\nLikelihood value\n\n\n\nSo depending on the nature of the exploratory variable, the nature of the effect size will also change. For a numerical exploratory variable, the effect size is represented as the slope of the line, which represents the rate of change. For a categorical response variable, the effect size is represented as the difference in model output values, when the variable changes from one category to the other.\nWe also learned that, for a categorical response variable, it‚Äôs helpful to represent the model output as probability values rather than class values. Therefore, for calculating errors, if we have a numerical response variable then the error metric is the mean of the square of the prediction error and for a categorical response variable, the error would be the likelihood value. We saw in detail how to find each of them.\nBut what we did not calculate back then was the effect size, if we had a categorical response variable. So what intuition would effect size make here for this case? Let us find out.\nWe will be using the NHANES dataset from the {NHANES} package in R, the same one we saw last time while learning to interpret the recursive partitioning model plots. Back then, our exploratory analysis found that depression is having a strong association with the interest in doing things on a given day. Let us build that model using the rpart() function. Here the model will have Depressed as the response variable and LittleInterest as the explanatory variable. Please note that here both the variables are categorical.\nThe levels for Depressed are; ‚ÄúNone‚Äù = No sign of depression, ‚ÄúSeveral‚Äù = Individual was depressed for less than half of the survey period days, and ‚ÄúMost‚Äù = Individual was depressed more than half of the days.\nThe levels for LittleInterest are also similar to that of Depressed; ‚ÄúNone‚Äù, ‚ÄúSeveral‚Äù and ‚ÄúMost‚Äù, and are analogous to levels of Depressed.\n\nif (!require(NHANES)) install.packages('NHANES')\nif (!require(rpart)) install.packages('rpart')\nif (!require(devtools)) install.packages('devtools')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(rpart)\nlibrary(statisticalModeling)\n\n# Building the recursive partitioning model\nmodel_rpart &lt;- rpart(Depressed ~ LittleInterest, data = NHANES)\n\n# Calculating the effect size\neffect_size_rpart &lt;- effect_size(model_rpart, ~ LittleInterest)\n\n# Printing the effect size\neffect_size_rpart\n\n\n\n  \n\n\n\nAll right, we got the effect size values and they are given in two columns; ‚Äòchange.None‚Äô and ‚Äòchange.Several‚Äô. Since our explanatory variable is categorical, effect size values with a categorical response variable give the change in probability values. Here, notice that the effect size is calculated as the difference when the response variable is changing its category from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô. That difference is the probability difference from the base value, which is zero.\nTo make more sense, imagine we get an effect size value of zero, in both the columns, in the above case. This means that when the level of little interest changes from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô, there is no increase or decrease in the probability of depression is ‚ÄòNone‚Äô or ‚ÄòSeveral‚Äô. This means, that if a person is having no depression, changing little interest from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô will not evoke a change in that person‚Äôs depressed state. Thus zero is taken as the base value.\nNow let us look at the actual effect size values we got. As the level of little interest is changing from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô, we get a probability difference of -0.4466272 = ~ 0.45 for depression is ‚ÄòNone‚Äô. This means that there is a reduction of 45% in the base value of probability. In other words, the person is 45% less likely to have depression set to ‚ÄòNone‚Äô. Likewise, in the same notion, the person is 36% more likely to have a depressed state of ‚ÄòSeveral‚Äô, when little interest changes from ‚ÄòNone‚Äô to ‚ÄòSeveral‚Äô. I hope this was clear to you.\nNow, let us look at the case when, again, our response variable is categorical but this time, our explanatory variable is numerical.\nLet us make another recursive partitioning model. We will have SmokeNow as the response variable and our explanatory variable will be BMI, which denotes the body mass index of the participants. The SmokeNow variable denotes smoking habit and has two levels; ‚ÄúYes‚Äù = smoked 100 or more cigarettes in their lifetime., and ‚ÄúNo‚Äù = did not smoke 100 or more cigarettes.\n\nif (!require(NHANES)) install.packages('NHANES')\nif (!require(rpart)) install.packages('rpart')\nif (!require(devtools)) install.packages('devtools')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(rpart)\nlibrary(statisticalModeling)\n\n# Building the recursive partitioning model\nmodel_rpart &lt;- rpart(SmokeNow ~ BMI, data = NHANES)\n\n# Calculating the effect size\neffect_size_rpart &lt;- effect_size(model_rpart, ~ BMI)\n\n# Printing the effect size\neffect_size_rpart\n\n\n\n  \n\n\n\nLooks like we got the effect size values as zero. Going by our previous notion, this means that when BMI is changed from 26 to 33, there is no change in smoking habit. Does this mean that BMI has no relationship with the smoking habit?\nBefore we conclude, let us try a different model architecture. We learned about logistic modelling last time. Let us build a model using it for the same question.\n\nif (!require(NHANES)) install.packages('NHANES')\nif (!require(devtools)) install.packages('devtools')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model\nmodel_logistic &lt;- glm(SmokeNow == \"No\" ~ BMI, data = NHANES, family = \"binomial\")\n\n# Calculating the effect size\neffect_size_logistic &lt;- effect_size(model_logistic, ~ BMI)\n\n# Printing the effect size\neffect_size_logistic\n\n\n\n  \n\n\n\nWe got a very small effect size value but a non-zero value. Let us try plotting both models. We will use the fmodel() function in the {statisticalModeling} package to easily plot the model. Use the slider to view between the plots.\n\n# Plotting both recursive partitioning model\nfmodel(model_rpart) + ggplot2::theme_bw() + \n  ggplot2::labs(title = \"SmokeNow ~ Age (recursive partitioning model)\")\n\n# Plotting both logistic model\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"SmokeNow ~ Age (logistic model)\")\n\n\n\n# Plotting both recursive partitioning model\nfmodel(model_rpart) + ggplot2::theme_bw() + \n  ggplot2::labs(title = \"SmokeNow ~ Age (recursive partitioning model)\")\n\n\n\n# Plotting both logistic model\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"SmokeNow ~ Age (logistic model)\")\n\n\n\n\n\nPlease note that the fmodel() function, while plotting for the recursive partitioning model takes in the ‚Äòfirst‚Äô level in the variable. You can easily check which level is represented in the y-axis by checking the level order.\n\nlibrary(NHANES)\n\n# Checking levels\nlevels(NHANES$SmokeNow)\n\n[1] \"No\"  \"Yes\"\n\n\nThe level ‚ÄúNo‚Äù will be represented in the y-axis for the recursive partitioning model plot. Therefore for increasing y-axis values, the probability of not smoking will increase.\nAfter comparing both the plots you can quickly notice that the plot for the recursive partitioning model shows step-wise change or sudden change and for the logistic model plot, the increase is gradual. Interestingly the plot shows that in higher BMI groups, i.e.¬†in groups of obese people, a smoking habit is uncommon. Anyway, in the recursive partitioning model plot, there is a sudden increase in y-axis values between 20 and 30 in the axis. Let us calculate the effect size of both models at this range.\n\n# Calculating the effect size for recursive partitioning model\neffect_size_rpart &lt;- effect_size(model_rpart, ~ BMI, BMI = 20, to = 10)\n\n# Printing the effect size for recursive partitioning \neffect_size_rpart\n\n\n\n  \n\n\n# Calculating the effect size for logistic model\neffect_size_logistic &lt;- effect_size(model_logistic, ~ BMI, BMI = 20, to = 10)\n\n# Printing the effect size for logistic\neffect_size_logistic\n\n\n\n  \n\n\n\nNote how the effect size for the recursive partitioning model is higher as compared to the logistic model. This is evident from the graph as the slope changes much faster at the given range of x-axis values for the recursive partitioning model as compared to the logistic model. Thus, recursive partitioning works best for sharp, discontinuous changes, whereas logistic regression can capture smooth, gradual changes.\nNow coming back to interpreting the values, let‚Äôs take the case we just discussed. Let us take the effect sizes in the recursive partitioning model. Here, as BMI changes from 20 to 30, there is a 1.8% increased chance that the smoking habit drops. Also ass the response variable is dichotomous, the opposite happens for the other case; 1.8% reduced chance that smoking habit is seen. With such low chances, BMI might not have any association with smoking habit at that particular range of the x-axis. values."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#plotting-the-model-output",
    "href": "tutorials/stat_model/inter_stat_model.html#plotting-the-model-output",
    "title": "Intermediate statistical modelling in R",
    "section": "\n4 Plotting the model output",
    "text": "4 Plotting the model output\nWith the help of the fmodel() function in the {statisticalModeling} package, we can easily plot the model outputs as a plot. Let us plot some plots.\nWe will again use the NHANES dataset from the {NHANES} package in R. We will DiabetesAge as the response variable and see if it has any association with Age. The Diabetes describes the participant‚Äôs age at which they were declared diabetic.\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model\nmodel_logistic &lt;- glm(Diabetes ~ Age, data = NHANES, family = \"binomial\")\n\n# Plotting the graph\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"Diabetes ~ Age (logistic model)\")\n\n\n\n\nWe get a rather unsurprising graph. As age increases, the risk of getting diabetes increases. Let us add more variables to the model and plot them one by one. Let us add the following variables; Gender, Depressed and Education.\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model (adding gender)\nmodel_logistic &lt;- glm(Diabetes ~ Age + Gender, data = NHANES, family = \"binomial\")\n\n# Plotting the graph\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"DiabetesAge ~ Age + Gender (logistic model)\")\n\n\n\n\nThe addition of the third variable introduced the colour aesthetic to the plot. Let us add another variable.\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model (adding depressed state)\nmodel_logistic &lt;- glm(Diabetes ~ Age + Gender + Depressed, data = NHANES, family = \"binomial\")\n\n# Plotting the graph\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"DiabetesAge ~ Age + Gender + Depressed (logistic model)\")\n\n\n\n\nAdding a fourth variable faceted the plot into the levels of the fourth variable. Let us add one last variable.\n\n# Loading libraries\nlibrary(NHANES)\nlibrary(statisticalModeling)\n\n# Building the logistic model (adding Education)\nmodel_logistic &lt;- glm(Diabetes ~ Age + Gender + Depressed + Education,\n                      data = NHANES, family = \"binomial\")\n\n# Plotting the graph\nfmodel(model_logistic) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"DiabetesAge ~ Age + Gender + Depressed + Education (logistic model)\")\n\n\n\n\nThe plot undergoes another layer of faceting. Now we get a rather complex plot as compared to the plot with just one explanatory variable. The plot shows some interesting info. Males are more prone to become diabetic as compared to females in all the groups. For college graduates who have no depression, both sexes have a reduced chance of having diabetes at younger ages as compared to all other groups. People with high school level of education and college level education have the most chance of becoming diabetic out of all groups. I leave the rest of the interpretations to the readers.\nIn summary, the fmodel() function syntax with four explanatory variables would be;\n\nfmodel(response ~ var1 + var2 + var3 + var4, data = dataset)\n\nHere, the response variable will always be plotted on the y-axis. The var1 will be plotted on the x-axis and will be the main exploratory variable that we are interested in. The var2 will assume the colour aesthetic. The var3 and var4 will invoke faceting. Therefore we have;\n\nfmodel() variables as plot components\n\nVariable\nPlot component\n\n\n\nvar1\ny-axis\n\n\nvar2\ncolour\n\n\nvar3\nfacet\n\n\nvar4\nfacet"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#interactions-among-explanatory-variables",
    "href": "tutorials/stat_model/inter_stat_model.html#interactions-among-explanatory-variables",
    "title": "Intermediate statistical modelling in R",
    "section": "\n5 Interactions among explanatory variables",
    "text": "5 Interactions among explanatory variables\nTill now, when making models, we assumed that our explanatory variables are independent of each other, i.e.¬†their effect sizes are independent of each other. But often this is not the case in real life, we can have instances where the effect size of one variable changes with the other explanatory variables. When this happens, we say that both of those variables are interacting with each other.\nTo denote interacting terms in the formula, instead of a + sign, we use a *.\nLet us see it with an example. We will use the SwimRecords dataset from the mosaicData package in R. The dataset contains world records for 100m swimming for men and women over time from 1905 through 2004. We will build a linear model, using time as the response variable and, year and sex as the explanatory variable. Here time denotes the time of the world record.\n\nif (!require(mosaicData)) install.packages('mosaicData')\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building the linear model\nmodel_lm &lt;- lm(time ~ year + sex, data = SwimRecords)\n\n# Plotting the model\nfmodel(model_lm) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"time ~ year + sex (linear model)\")\n\n\n\n\nFrom the graph, you can see that the slopes for both the lines are decreasing. This means that as years progressed, the world record times for both the sexes reduced. Also, men‚Äôs record time is faster compared to women‚Äôs. What happens to this plot if we introduce an interaction between year and sex?\n\nif (!require(mosaicData)) install.packages('mosaicData')\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building the linear model\nmodel_lm_int &lt;- lm(time ~ year * sex, data = SwimRecords)\n\n# Plotting the model\nfmodel(model_lm_int) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"time ~ year * sex (linear model)\")\n\n\n\n\nThe introduction of interacting terms seems to converge the two lines. For better comparison, use the slider to compare the plots as given below.\n\n\nif (!require(mosaicData)) install.packages('mosaicData')\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building the linear model\nmodel_lm &lt;- lm(time ~ year + sex, data = SwimRecords)\n\n# Plotting the model\nfmodel(model_lm) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"time ~ year + sex (linear model without interaction)\")\n\n\n\n# Building the linear model with interaction term\nmodel_lm_int &lt;- lm(time ~ year * sex, data = SwimRecords)\n\n# Plotting the model\nfmodel(model_lm_int) + ggplot2::theme_bw() +\n  ggplot2::labs(title = \"time ~ year * sex (linear model with interaction)\")\n\n\n\n\n\nWe get some interesting results. First of all, compared to the plot without the interaction term, the slope changes for both the sexes, in the plot with interaction terms. Also, the lines are not parallel when interaction terms are introduced, this is direct evidence showing that both year and sex variables are truly interacting with each other. This is why with increasing years, the gap between the lines decreased, indicating a reduced effect size of sex with increasing years.\nTo see if this interaction term makes the model better, we can calculate the mean of the square of prediction errors (m.s.e) for each model and do a t.test using them (something we learned in the previous tutorial).\n\nif (!require(mosaicData)) install.packages('mosaicData')\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building the linear model\nmodel_lm &lt;- lm(time ~ year + sex, data = SwimRecords)\n\n# Building the linear model with interaction term\nmodel_lm_int &lt;- lm(time ~ year * sex, data = SwimRecords)\n\n# Calculating the m.s.e\nt.test(mse ~ model, data = cv_pred_error(model_lm, model_lm_int))\n\n\n    Welch Two Sample t-test\n\ndata:  mse by model\nt = 13.854, df = 7.9205, p-value = 7.829e-07\nalternative hypothesis: true difference in means between group model_lm and group model_lm_int is not equal to 0\n95 percent confidence interval:\n 3.992455 5.590253\nsample estimates:\n    mean in group model_lm mean in group model_lm_int \n                  17.04400                   12.25265 \n\n\nFor \\alpha = 0.05 level of significance, we have a p-value &lt; 0.05. Thus we conclude that the m.s.e values between the two models are significantly different from each other. Also, from the t.test summary, we can see that the m.s.e value is lowest for the model with the interaction term as compared to the model without the interaction term. This means interaction made the model better.\nSome takeaway points about effect sizes, from analysing the plots are;\n\n\nPoint 1\nPoint 2\nPoint 3\nPoint 4\nPoint 5\n\n\n\n The slope of the line is the effect size of the x-axis exploratory variable on the y-axis response variable.\n\n\n The difference in y-intercepts of both the lines for a given x-axis exploratory variable value gives the effect size of the colour aesthetic on the y-axis response variable.\n\n\n If lines are parallel, then there is no interaction between the x-axis variable and the colour aesthetic variable.\n\n\n\nThe difference between the values of the slopes tells how the colour aesthetic variable is affecting the effect size of the x-axis variable.\n\n\n\nThe rate of change of the y-intercepts tells how the x-axis variable is affecting the effect size of the colour aesthetic variable.\n\n\n\nNow we are equipped with a strong intuition of how the interaction between variables affects the effect size of those variables and how they can be visualized from the plot."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#polynomial-regression",
    "href": "tutorials/stat_model/inter_stat_model.html#polynomial-regression",
    "title": "Intermediate statistical modelling in R",
    "section": "\n6 Polynomial regression",
    "text": "6 Polynomial regression\nLet us look at the Tadpoles dataset from the {mosaicModel} package in R. We saw this dataset while we learned about covariates in the previous tutorial. The dataset contains the swimming speed of tadpoles as a function of the water temperature and the water temperature at which the tadpoles had been raised. Let us plot the temperature at which the tadpoles were allowed to swim versus the maximum swimming speed.\n\nif (!require(mosaicModel)) install.packages('mosaicModel')\nlibrary(mosaicModel)\nlibrary(ggplot2)\n\ndata(Tadpoles)\n\n# Building a plot\nTadpoles |&gt; ggplot(aes(rtemp, vmax)) + geom_point() + theme_bw()\n\n\n\n\nNow let us try building a linear model using vmax as the response variable and rtemp as the exploratory variable.\n\nlibrary(mosaicModel)\nlibrary(statisticalModeling)\n\ndata(Tadpoles)\n\n# Building the linear model\nmodel_lm &lt;- lm(vmax ~ rtemp, data = Tadpoles)\n\n# Plotting the model\nfmodel(model_lm) + ggplot2::geom_point(data = Tadpoles) +\n  ggplot2::theme_bw()\n\n\n\n\nWe see that the model out shows a weak relationship between the temperature at which the tadpoles were allowed to swim and the maximum swimming speed. But our intuition tells us that most of the maximum values for vmax peaked at temperatures of 15¬∞C and 25¬∞C hence, the line should curve upwards at these temperature regions. Essentially, instead of a straight line relationship, a parabolic relationship would be better for defining the relationship between rtemp and vmax. To tell the model to consider rtemp as a second-order variable, we use the I() function. So here, including I(rtemp^2) in the formula will inform the model to regress vmax against rtemp^2. You might be tempted to ask, why not just use rtemp^2 in the model formula? If we use vmax ~ rtemp^2 as the model formula, then notation wise it is equivalent to using vmax ~ rtemp * rtemp. If we run the model using this formula, the model will omit one of the rtemp variables as it is redundant to regress against a variable interacting with itself.\nOn a more technical note, in R, the symbols \"+\", \"-\", \"*\" and \"^\" are interpreted as formula operators. Therefore if rtemp^2 is used inside the formula, it will consider it as rtemp * rtemp. Thus to make things right, we use the I() function, so that R will interpret rtemp^2 as a new predictor with squared values.\nLet us plot a polynomial model for the same question as above.\n\nlibrary(mosaicModel)\nlibrary(statisticalModeling)\n\ndata(Tadpoles)\n\n# Building the linear model with rtemp in second order using I()\nmodel_lm_2 &lt;- lm(vmax ~ rtemp + I(rtemp^2), data = Tadpoles)\n\n# Plotting the model\nfmodel(model_lm_2) + ggplot2::geom_point(data = Tadpoles) +\n  ggplot2::theme_bw()\n\n\n\n\nMuch better plot than the earlier one. The same can also be achieved by using the poly() function. The syntax for it is; poly(variable, the order)\n\nlibrary(mosaicModel)\nlibrary(statisticalModeling)\n\ndata(Tadpoles)\n\n# Building the linear model with rtemp in second order using poly()\nmodel_lm_2 &lt;- lm(vmax ~ poly(rtemp, 2), data = Tadpoles)\n\n# Plotting the model\nfmodel(model_lm_2) + ggplot2::geom_point(data = Tadpoles) +\n  ggplot2::theme_bw()"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#total-and-partial-change",
    "href": "tutorials/stat_model/inter_stat_model.html#total-and-partial-change",
    "title": "Intermediate statistical modelling in R",
    "section": "\n7 Total and partial change",
    "text": "7 Total and partial change\nFrom the previous tutorial and current tutorial, we built our notion of effect size bit by bit. In a gist, the flowchart below shows our efforts till now;\n\n\n\n\n%%{init: {'securityLevel': 'loose', 'theme':'base'}}%%\ngraph TB\n  A[Understanding effect sizes] --o B(In the context of numerical explanatory variable)\n  B --&gt; C(In the context of categorical explanatory variable)\n  C --&gt; D(Magnitude and the sign of effect size value)\n  D --&gt; E(Units of effect size)\n  E --&gt; F(In the context of categorical response variable)\n  F --&gt; G(How interaction terms affect effect sizes:&lt;br&gt;Understanding it from graphs)\n  G -.-&gt; H(How interaction terms affect effect sizes:&lt;br&gt;Understanding it from calculations)\n  style H fill:#f96\n  subgraph Flowchart on current understanding of effect size\n  A\n  B\n  C\n  D\n  E\n  F\n  G\n  H\n  end\n\n\n\n\n\nFrom the flowchart, we can appreciate the growing complexity in the understanding of effect size. We have already begun to understand effect sizes in the context of interaction terms as seen from previous graphs. Now let us try to calculate effect sizes with interaction terms.\nWe will use the HorsePrices dataset from the Stat2Data package in R. The dataset contains the price and related characteristics of horses listed for sale on the internet. The variables in the dataset include; Price = Price of the horse ($), HorseID = ID of the horse, Age = Age of the horse, Height = Height of the horse and Sex = Sex of the horse.\nSuppose we are interested in the following questions\n\nHow does the price of the horse change between sexes?\nHow does the price of the horse change between sexes for the same height and age?\n\nIn the first question, we are checking for the price difference between the sexes. Here, when sex changes, the covariate variables; height and age of the horse will also change with sex. Therefore, here we are measuring the change in price by allowing all other variables in the model to change along with the exploratory variable sex. This is termed the ‚Äòtotal change in price as sex changes‚Äô.\nIn the second question, unlike the first, we want to specifically see what is the price difference between sexes. To see this change, we keep height and age constant. Therefore, the only change seen in this case would be coming from the sex change. This is termed the ‚Äòpartial change of price as sex changes‚Äô.\nFor the first question, we will calculate the effect size of sex on price using the following lines of code. Since our question focuses on the total change in price with sex, we make a model by excluding all the other covariates that we want to change as sex changes.\n\n# Loading libraries\nif (!require(Stat2Data)) install.packages('Stat2Data')\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building the linear model\nmodel_lm &lt;- lm(Price ~ Sex, data = HorsePrices)\n\n\n# Calculating the effect size\neffect_size(model_lm, ~ Sex)\n\n\n\n  \n\n\n\nThe effect size value tells us that, changing from a male horse to a female, the price depreciates by 17225 dollars. Now to answer the second question, to calculate the partial change in price because of a change in sex, we build a model by including all the other covariates that we want to keep constant when we change sex.\n\n# Loading libraries\nif (!require(Stat2Data)) install.packages('Stat2Data')\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building the linear model\nmodel_lm_partial &lt;- lm(Price ~ Sex + Height + Age, data = HorsePrices)\n\n# Calculating the effect size\neffect_size(model_lm_partial, ~ Sex)\n\n\n\n  \n\n\n\nThe effect size value, in this case, tells us that, a male horse of age 6 and 16.5m in height will be 9928 dollars more costly than a female horse of the same physical attributes. Here it‚Äôs clear that for the given height and age, there is a price difference between male and female horses, something which we were trying to answer."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#r-squared-in-terms-of-model-output",
    "href": "tutorials/stat_model/inter_stat_model.html#r-squared-in-terms-of-model-output",
    "title": "Intermediate statistical modelling in R",
    "section": "\n8 R-squared in terms of model output",
    "text": "8 R-squared in terms of model output\nSo far we have seen metrics like the mean of the square of prediction error and likelihood values to evaluate a model‚Äôs predictive abilities. In terms of variables used in the model formula, we know how to interpret them using effect sizes. There is also another metric called R-squared (R^2).\nOne of the implications of a model is to account for variations seen within the response variable to the model output values. The R-squared value tells us how well this has been done. By definition;\nR^2 = \\frac{variance\\,of\\,model\\,output\\,values}{variance\\,of\\,response\\,variable\\,values} The R-squared value is always positive and is between 0 and 1. R-squared value of 1 means that the model accounted for all the variance seen within the actual dataset values of the response variable. R-squared value of 0 means that the model does not account for any variance seen in the response variable.\nBut the R-squared value has its flaws which we will see soon.\nLet us calculate the R-squared value for some models. We will again use the HorsePrices dataset from the Stat2Data package in R. Let us build the earlier model again and calculate the R-squared value for the model. We will use the evaluate_model() function from the statisticalModeling package to get the predicted values. For now, we will use the training dataset itself as the testing dataset. We will use the var() formula to calculate the variance.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Building the linear model\nmodel_lm &lt;- lm(Price ~ Sex, data = HorsePrices)\n\n# Getting the predicted values\nmodel_lm_output &lt;- evaluate_model(model_lm, data = HorsePrices)\n\n# Calculating the R-squared value\nr_squared &lt;- with(data = model_lm_output, var(model_output)/var(Price))\n\n# Printing R-squared value\nr_squared\n\n[1] 0.3237929\n\n\nWe get an R-squared value of 0.32. What this means is that 32% of the variability seen in the price of the horses is explained by the sex of the horses.\nNow let us add in more explanatory variables and calculate the R-squared value. We will also remove any NA values in the model variables.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Removing NA values\nHorsePrices_new &lt;- tidyr::drop_na(HorsePrices)\n\n# Building the linear model\nmodel_lm &lt;- lm(Price ~ Sex + Height + Age, data = HorsePrices_new)\n\n# Getting the predicted values\nmodel_lm_output &lt;- evaluate_model(model_lm, data = HorsePrices_new)\n\n# Calculating the R-squared value\nr_squared &lt;- with(data = model_lm_output, var(model_output)/var(Price))\n\n# Printing R-squared value\nr_squared\n\n[1] 0.4328061\n\n\nWe got a higher R-squared value than before. Instead of 32%, we now can account for 43% of variability seen in price by all the other variables.\nDoes this mean that this model is better than the previous one? Before we jump in, let us do a fun little experiment. Let us see what happens if we add in random variables with no predicted power whatsoever to the model and run a model using them.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Removing NA values\nHorsePrices_new &lt;- tidyr::drop_na(HorsePrices)\n\n# Adding random variable\nset.seed(56)\nHorsePrices_new$Random &lt;- rnorm(nrow(HorsePrices_new)) &gt; 0\n\n# Building the linear model\nmodel_lm_random &lt;- lm(Price ~ Sex + Height + Age + Random, data = HorsePrices_new)\n\n# Getting the predicted values\nmodel_lm_random_output &lt;- evaluate_model(model_lm_random, data = HorsePrices_new)\n\n# Calculating the R-squared value\nr_squared &lt;- with(data = model_lm_random_output, var(model_output)/var(Price))\n\n# Printing R-squared value\nr_squared\n\n[1] 0.4437072\n\n\nWe got an R-squared value of 0.44. The random variable should throw off the model output, then why did we get a higher R-squared value?\nThe R-squared value of a model will increase with increasing explanatory variables. To remind you again, the R-squared value comes partly from the model output and our model output comes from the design of the model. The model design includes selecting the appropriate explanatory variables which are up to the user. Therefore, stupidly adding variables which have no relationship whatsoever with the response variable can lead us to the wrong conclusion. Let us calculate the mean square of the prediction error for our last two models and compare them.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Removing NA values\nHorsePrices_new &lt;- tidyr::drop_na(HorsePrices)\n\n# Adding random variable\nset.seed(56)\nHorsePrices_new$Random &lt;- rnorm(nrow(HorsePrices_new)) &gt; 0\n\n# Building the linear model\nmodel_lm &lt;- lm(Price ~ Sex, data = HorsePrices_new)\n\n# Building the linear model with random variable\nmodel_lm_random &lt;- lm(Price ~ Sex + Random, data = HorsePrices_new)\n\n# Calculating the mean of square of prediction errors for trials\nmodel_errors &lt;- cv_pred_error(model_lm, model_lm_random)\n\n# Calculating the mean of square of prediction errors\nboxplot(mse ~ model, model_errors)\n\n\n\n# Conducting t-test\nt.test(mse ~ model, model_errors)\n\n\n    Welch Two Sample t-test\n\ndata:  mse by model\nt = -2.9348, df = 6.9143, p-value = 0.02218\nalternative hypothesis: true difference in means between group model_lm and group model_lm_random is not equal to 0\n95 percent confidence interval:\n -11481425  -1221092\nsample estimates:\n       mean in group model_lm mean in group model_lm_random \n                    153536030                     159887288 \n\n\nThe plot and the t.test results can be used to check if the model with the random variable is indeed a poor one compared to the model without any random variable, something the R-squared value failed to report. Therefore R-squared value has its flaw, but it is widely used by people. Therefore, we should be very careful in interpreting the R-squared values of different models. Different models made from the same data can have different R-squared values."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#r-squared-in-terms-of-residuals",
    "href": "tutorials/stat_model/inter_stat_model.html#r-squared-in-terms-of-residuals",
    "title": "Intermediate statistical modelling in R",
    "section": "\n9 R-squared in terms of residuals",
    "text": "9 R-squared in terms of residuals\nFrom the above example, I mentioned that the R-squared value tells us how much percentage of the variability seen within the response variable is captured by the predicted model output values. Another way of seeing the same thing is through residuals. The residual is the distance between the regression line and the response variable value, or in other words, the difference between the fitted value and the corresponding response variable value. We can find R-squared using the following formula also;\nR^2 = \\frac{variance\\,of\\,response\\,variable\\,values - variance\\,of\\,residuals}{variance\\,of\\,response\\,variable\\,values}\nWe can use the following code to calculate the R-squared value via the above formula.\n\n# Loading libraries\nlibrary(Stat2Data)\nlibrary(statisticalModeling)\n\ndata(\"HorsePrices\")\n\n# Removing NA values\nHorsePrices_new &lt;- tidyr::drop_na(HorsePrices)\n\n# Building the linear model\nmodel_lm &lt;- lm(Price ~ Sex + Height + Age, data = HorsePrices_new)\n\n# Getting the predicted values\nmodel_lm_output &lt;- evaluate_model(model_lm, data = HorsePrices_new)\n\n# Calculating the R-squared value using model output values\nr_squared_mo &lt;- with(data = model_lm_output, var(model_output)/var(Price))\n\n# Calculating the R-squared value using residuals\nr_squared_res &lt;- (var(HorsePrices_new$Price) - var(model_lm$residuals)) / var(HorsePrices_new$Price)\n\n# Checking if both R-squared values are the same\nr_squared_mo\n\n[1] 0.4328061\n\nr_squared_res\n\n[1] 0.4328061\n\nround(r_squared_mo) == round(r_squared_res)\n\n[1] TRUE\n\n\nAs you can see, they are both the same. Therefore, the R-squared value of 1 means that the model has zero residuals, which means that the model is a perfect fit; all data points lie on the regression line. R-squared value of 0 means that the variance of residuals is the same as that of the variance of the response variable. This means the explanatory variables we chose do not account for the variation seen within the response variable."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#bootstrapping-and-precision",
    "href": "tutorials/stat_model/inter_stat_model.html#bootstrapping-and-precision",
    "title": "Intermediate statistical modelling in R",
    "section": "\n10 Bootstrapping and precision",
    "text": "10 Bootstrapping and precision\nAll the datasets that we have worked on till now have been collected from a large population. Therefore, from these samples, we calculated different test statistics like the mean prediction error and effect sizes. Suppose we sample data again from the same population and calculate these test statistics, if they are close to earlier calculated values, then we can say that our test statistics are precise. But re-sampling the data is often costly and tedious to do. Then how can we check for precision? We can do that using a technique called bootstrapping. Similar to resampling, instead of collecting new samples from the population, we resample the data from the already collected data itself. Essentially, we are treating our existing dataset as a population dataset from which resampling occurs. Needless to say, this would only work effectively with large datasets.\n\n\n\n\n%%{init: {'securityLevel': 'loose', 'theme':'base'}}%%\ngraph LR\n  A[Population] --&gt; B(Random sample)\n  B --&gt; C(Calculating sample statistics)\n  A --&gt; D(Random sample)\n  D --&gt; E(Calculating sample statistics)\n  A --&gt; F(\"---\")\n  F --&gt; G(\"---\")\n  A --&gt; H(Random sample)\n  H --&gt; I(Calculating sample statistics)\n  J[Population] --&gt; K(Random sample)\n  J --&gt; L(Random sample)\n  J --&gt; M(Collected random sample)\n  M --&gt; N(Resample 1)\n  M --&gt; O(Resample 2)\n  M --&gt; P(Resample 3)\n  J --&gt; Q(Random sample)\n  subgraph Bootstrapping\n  J\n  K\n  L\n  M\n  N\n  O\n  P\n  Q\n  end\n  subgraph Resampling from the population\n  A\n  B\n  C\n  D\n  E\n  F\n  G\n  H\n  I\n  end\n\n\n\n\n\nLet us perform a bootstrap trial code by code. We will be using the run17 dataset from the cherryblossom package in R. It‚Äôs a relatively large dataset that we can use to study how to do a bootstrap.\nWe will first build a model and then calculate a sample statistic, which in this case would be the effect size.\n\n# Loading libraries\nif (!require(cherryblossom)) install.packages('cherryblossom')\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon &lt;- run17 %&gt;% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_lm &lt;- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Calculating the effect size\neffect_size(model_lm, ~ age)\n\n\n\n  \n\n\n\nUsing the sample() function in R, we will sample the row indices in the original dataset. Then using these row indices, we build a resampled dataset. Then using this resampled dataset, we will build a new model and then calculate the effect size.\n\n# Loading libraries\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon &lt;- run17 %&gt;% filter(event == \"10 Mile\")\n\n# Collecting the row indices \nrow_indices &lt;- sample(1:nrow(run17_marathon), replace = T)\n\n# Resampling data using the rwo indices\nresample_data &lt;- run17_marathon[row_indices, ]\n\n# Building a linear model\nmodel_lm_resample &lt;- lm(net_sec ~ age + sex, data = resample_data)\n\n# Calculating the effect size\neffect_size(model_lm_resample, ~ age)\n\n\n\n  \n\n\n\nWe get a slightly different effect size value in each case. If we repeat this process, we can get multiple effect size values that we can plot to get the sampling distribution of the effect size. doing this code by code is tedious and therefore we will automate this process using the ensemble() function in the statisticalModeling package in R. Using nreps inside the ensemble() function, we can specify how many times we want to resample the data. Normally, 100 resampling trials are performed.\n\n# Loading libraries\nlibrary(cherryblossom)\nlibrary(statisticalModeling)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon &lt;- run17 %&gt;% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_lm &lt;- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Resampling 100 times\nmodel_trials &lt;- ensemble(model_lm, nreps = 100)\n\n# Calculating the effect size for each resampled data\nresampled_effect_sizes &lt;- effect_size(model_trials, ~ age)\n\n# Printing effect sizes\nresampled_effect_sizes\n\n\n\n  \n\n\n\nNow to estimate the precision in the effect size, we can calculate its standard deviation. This is the bootstrapped estimate of the standard error of the effect size, a measure of the precision of the quantity calculated on the original model. We can also plot a histogram of the effect size values we got from each of the trails to get the sampling distribution of the effect size.\n\n# Calculating the standard error of effect size of age\nsd(resampled_effect_sizes$slope)\n\n[1] 0.652797\n\n# Plotting a histogram\nhist(resampled_effect_sizes$slope)"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#scales-and-transformations",
    "href": "tutorials/stat_model/inter_stat_model.html#scales-and-transformations",
    "title": "Intermediate statistical modelling in R",
    "section": "\n11 Scales and transformations",
    "text": "11 Scales and transformations\nDepending on the class of the response variables, we can apply relevant transformations to change the scale of the variables to make the model better. We have seen how to use appropriate model architecture depending on the response variable, now we will see how to use relevant transformations.\n\n11.1 Logarithmic transformation\nIn this exercise, we will see log-transformation which is mainly used when the response variable varies in proportion to its current size. For example, variables denoting population growth, exponential growth, prices or other money-related variables.\nWe will be using the Oil_history dataset from the statisticalModeling package in R. It denotes the historical production of crude oil, worldwide from 1880-2014. Let us first plot the data points and see how they are distributed. Here I am filtering the data till the year 1980 to get a nice looking exponential growth curve for the exercise.\n\nlibrary(statisticalModeling)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"Oil_history\")\n\n# Plotting the data points\nOil_history %&gt;% filter(year &lt; 1980) %&gt;%\n  ggplot(aes(year, mbbl)) + geom_point() +\n  labs(y = \"mbbl (oil production in millions of barrels)\") +\n  theme_bw()\n\n\n\n\nYou can see exponential growth in oil barrel production with increasing years. First, we will build a linear model looking at how years influenced change in barrel production. Here, mbbl would be our response variable and year our explanatory variable. Secondly, we will build another linear model but here our response variable; mbbl will be log-transformed. You can compare the plots using the slider.\n\nlibrary(statisticalModeling)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"Oil_history\")\n\n# Filtering values till 1980\nOil_1980 &lt;- Oil_history %&gt;% filter(year &lt; 1980)\n\n# Building a linear model\nmodel_lm &lt;- lm(mbbl ~ year, data = Oil_1980)\n\n# Plotting the linear model\nfmodel(model_lm) + ggplot2::geom_point(data = Oil_1980) +\n  ggplot2::labs(y = \"mbbl (oil production in millions of barrels)\",\n                title = \"Oil production in millions of barrels ~ Year\") +\n  theme_bw()\n\n# Transforming mbbl values to logarithmic scale\nOil_1980$log_mbbl &lt;- log(Oil_1980$mbbl)\n\n# Building a linear model with log transformed response variable\nmodel_lm_log &lt;- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Plotting the linear model with log transformed response variable\nfmodel(model_lm_log) + ggplot2::geom_point(data = Oil_1980) +\n  ggplot2::labs(y = \"log(mbbl) [oil production in millions of barrels]\",\n                title = \"log(Oil production in millions of barrels) ~ Year\") +\n  theme_bw()\n\n\n\nlibrary(statisticalModeling)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata(\"Oil_history\")\n\n# Filtering values till 1980\nOil_1980 &lt;- Oil_history %&gt;% filter(year &lt; 1980)\n\n# Building a linear model\nmodel_lm &lt;- lm(mbbl ~ year, data = Oil_1980)\n\n# Plotting the linear model\nfmodel(model_lm) + ggplot2::geom_point(data = Oil_1980) +\n  ggplot2::labs(y = \"mbbl (oil production in millions of barrels)\",\n                title = \"Oil production in millions of barrels ~ Year\") +\n  theme_bw()\n\n\n\n# Transforming mbbl values to logarithmic scale\nOil_1980$log_mbbl &lt;- log(Oil_1980$mbbl)\n\n# Building a linear model with log transformed response variable\nmodel_lm_log &lt;- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Plotting the linear model with log transformed response variable\nfmodel(model_lm_log) + ggplot2::geom_point(data = Oil_1980) +\n  ggplot2::labs(y = \"log(mbbl) [oil production in millions of barrels]\",\n                title = \"log(Oil production in millions of barrels) ~ Year\") +\n  theme_bw()\n\n\n\n\n\nIn the first graph, our linear model does not fit the values perfectly as opposed to the model which used the log transformation of the response variable. Why the second model fits better because initially mbbl and Year had an exponential relationship. But after log transformation, the relationship became a linear one and thus the model performs better. We can calculate the mean predictive error between these two models and see which one does a better job at predicting.\n\nlibrary(statisticalModeling)\nlibrary(dplyr)\n\ndata(\"Oil_history\")\n\n# Filtering values till 1980\nOil_1980 &lt;- Oil_history %&gt;% filter(year &lt; 1980)\n\n# Transforming mbbl values to logarithmic scale\nOil_1980$log_mbbl &lt;- log(Oil_1980$mbbl)\n\n# Building a linear model\nmodel_lm &lt;- lm(mbbl ~ year, data = Oil_1980)\n\n# Building a linear model with log transformed response variable\nmodel_lm_log &lt;- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Evaluating the model\npredict_lm &lt;- evaluate_model(model_lm, data = Oil_1980)\npredict_lm_log &lt;- evaluate_model(model_lm_log, data = Oil_1980)\n\n# Transforming the model output values back to normal\npredict_lm_log$model_output_nonlog &lt;- exp(predict_lm_log$model_output)\n\n# Calculating the mean square errors\nmean((predict_lm$mbbl - predict_lm$model_output)^2, na.rm = TRUE)\n\n[1] 17671912\n\nmean((predict_lm_log$mbbl - predict_lm_log$model_output_nonlog)^2, na.rm = TRUE)\n\n[1] 1861877\n\n\nWe get a much smaller mean predictive error for the model with log transformation as compared to the model which does not have log transformation.\nLet us calculate the effect size for these two models\n\nlibrary(statisticalModeling)\n\ndata(\"Oil_history\")\n\n# Building a linear model\nmodel_lm &lt;- lm(mbbl ~ year, data = Oil_1980)\n\n# Building a linear model with log transformed response variable\nmodel_lm_log &lt;- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Calculating the effect sizes\neffect_size(model_lm, ~ year)\n\n\n\n  \n\n\neffect_size(model_lm_log, ~ year)\n\n\n\n  \n\n\n\nFor the model without a log-transformed response variable, we have slope = 254. This means, that from the year 1958 to 1988, the oil barrels production increased by 254 million barrels per year. What about the effect size of the model with a log-transformed response variable?\nThe effect size for a log-transformed value is in terms of the change of logarithm per unit of the explanatory variable. It‚Äôs generally easier to interpret this as a percentage change per unit of the explanatory variable, which also involves an exponential transformation: 100 * (exp(__effect_size__) - 1)\n\nlibrary(statisticalModeling)\n\ndata(\"Oil_history\")\n\n# Building a linear model with log transformed response variable\nmodel_lm_log &lt;- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Calculating the effect size\neffect_lm_log &lt;- effect_size(model_lm_log, ~ year)\n\n# Converting effect size to percentage change\n100 * (exp(effect_lm_log$slope) - 1)\n\n[1] 6.751782\n\n\nWe get a value of 6.75. This means that barrel production increased by 6.75% each year from 1958 to 1988.\nWe can take this one step further by using the bootstrapping method and thereby calculate the effect size. Then, after converting effect size to percentage change, we can calculate the 95% confidence interval on the percentage change in oil barrel production per year.\n\nlibrary(statisticalModeling)\n\ndata(\"Oil_history\")\n\n# Building a linear model with log transformed response variable\nmodel_lm_log &lt;- lm(log_mbbl ~ year, data = Oil_1980)\n\n# Bootstrap replications: 100 trials\nbootstrap_trials &lt;- ensemble(model_lm_log, nreps = 100, data = Oil_1980)\n\n# Calculating the effect size\nbootstrap_effect_sizes &lt;- effect_size(bootstrap_trials, ~ year)\n\n# Converting effect size to percentage change\nbootstrap_effect_sizes$precentage_change &lt;- 100 * (exp(bootstrap_effect_sizes$slope) - 1)\n\n# Calculating 95% confidence interval\nwith(bootstrap_effect_sizes, mean(precentage_change) + c(-2, 2) * sd(precentage_change))\n\n[1] 6.496748 7.003000\n\n\nNice! We got a narrow 95% confidence interval of [6.50, 7.00].\n\n11.2 Rank transformation\nWe found that log transformation worked best for datasets showcasing exponential growth or where depicting percentage change of response variables makes more sense. Likewise, there is another transformation called rank transformation which works best with a dataset that deviates from normality and has outliers.\nConsider the dataset HorsePrices from the Stat2Data package in R. We will plot the prices for female horses with their age.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\n\n# Plotting the graph\nHorsePrices %&gt;% filter(Sex == \"f\") %&gt;%\n  ggplot(aes(Age, Price)) + geom_point() + theme_bw()\n\n\n\n\nYou can see that there are two outliers in the plot for y &gt; 30000, let us label them.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\n\n# Making a function to identify outliers\nis_outlier &lt;- function(x) {\n  x &gt; 30000\n}\n\n# Filtering female horses and labelling the outliers\nfemale_horses &lt;- HorsePrices %&gt;% filter(Sex == \"f\") %&gt;%\n  mutate(outliers = if_else(is_outlier(Price), Price, rlang::na_int))\n\n# Plotting the graph\nfemale_horses %&gt;% filter(Sex == \"f\") %&gt;%\n  ggplot(aes(Age, Price)) + geom_point() +\n  geom_text(aes(label = outliers), na.rm = TRUE, vjust = 1, col = \"red\") +\n  theme_bw()\n\n\n\n\nNow let us build a linear model with Price as the response variable and Age as the explanatory variable and then plot the model.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\n# Making a function to identify outliers\nis_outlier &lt;- function(x) {\n  x &gt; 30000\n}\n\n# Filtering female horses and labelling the outliers\nfemale_horses &lt;- HorsePrices %&gt;% filter(Sex == \"f\") %&gt;%\n  mutate(outliers = if_else(is_outlier(Price), Price, rlang::na_int))\n\n# Building the linear model\nmodel_lm &lt;- lm(Price ~ Age, data = female_horses)\n\n# Plotting the model\nfmodel(model_lm) + geom_point(data = female_horses) +\n  geom_text(aes(label = outliers), na.rm = TRUE, vjust = 1,\n            col = \"red\", data = female_horses) +\n  scale_y_continuous(breaks = seq(0,50000,10000), limits = c(0, 50000)) +\n  labs(title = \"Linear model\") +\n  theme_bw()\n\n\n\n\nThe outliers might be causing an effect on the slope of the regression line. This is where rank transformation comes into place. Using the rank() in R, we replace the actual value with the rank which that value occupies when arranged from ascending to descending order. Let us see how the plot will be after rank transformation.\n\nlibrary(Stat2Data)\nlibrary(ggplot2)\nlibrary(statisticalModeling)\n\n# Making a function to identify outliers via ranks\nis_outlier &lt;- function(x) {\n  x &gt; 18\n}\n\n# Filtering female horses\nfemale_horses &lt;- HorsePrices %&gt;% filter(Sex == \"f\")\n\n# Assigning ranks\nfemale_horses$price_rank &lt;- rank(female_horses$Price)\n\n# Labelling outliers via ranks\nfemale_horses&lt;- female_horses %&gt;%\n  mutate(outliers = if_else(is_outlier(price_rank), Price, rlang::na_int))\n\n# Building the linear model\nmodel_lm_rank &lt;- lm(price_rank ~ Age, data = female_horses)\n\n# Plotting the model\nfmodel(model_lm_rank) + geom_point(data = female_horses) +\n  geom_text(aes(label = outliers), na.rm = TRUE, vjust = 1,\n            col = \"red\", data = female_horses) +\n  labs(y = \"Ranked Price\", title = \"Linear model with ranked price\") +\n  theme_bw()\n\n\n\n\nA quick comparison of both the graphs shows that the slope of the regression line has changed. For now, let us not worry if this made the model better. We will see in greater detail rank transformation in the coming tutorials. For now, keep in mind that rank transformation works best for data with outliers."
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#collinearity-also-called-multicollinearity",
    "href": "tutorials/stat_model/inter_stat_model.html#collinearity-also-called-multicollinearity",
    "title": "Intermediate statistical modelling in R",
    "section": "\n12 Collinearity (also called Multicollinearity)",
    "text": "12 Collinearity (also called Multicollinearity)\nCollinearity is a phenomenon that occurs when two or more explanatory variables used in the model are in a linear relationship with each other. Consider a dataset with ‚Äòpoverty‚Äô as a variable, let‚Äôs say we are looking at the association between ‚Äòpoverty‚Äô and other variables such as ‚Äòeducation‚Äô and ‚Äòincome‚Äô. Now common sense tells us that, most often, education and income have a linear relationship with each other. Highly educated people often will have a high income. Therefore, changes seen in poverty status explained by education might often be a result of income rather than education itself and vice versa. In this case, we say that the variables ‚Äòeducation‚Äô and ‚Äòincome‚Äô are colinear.\nLet us look at a real-life example. Let us use the SAT dataset from the mosaicData package in R. The SAT data looks at the link between SAT scores and measures of educational expenditures.\nWe will build a linear model using sat as the response variable. The sat variable denotes the average total SAT score. We will also use expend as the explanatory variable. The expend variable denotes expenditure per pupil in average daily attendance in public elementary and secondary schools, 1994-95 (in thousands of US dollars). We will also do bootstrapping and find the 95% confidence interval.\n\nlibrary(mosaicData)\nlibrary(statisticalModeling)\nlibrary(ggplot2)\n\n# Building a linear model\nmodel_lm &lt;- lm(sat ~ expend, data = SAT)\n\n# Plotting the model\nfmodel(model_lm) + theme_bw()\n\n\n\n# Bootstrap replications: 100 trials\nbootstrap_trials &lt;- ensemble(model_lm, nreps = 100)\n\n# Calculating the effect sizes of salary from the bootstrap samples\nbootstrap_effect_sizes &lt;- effect_size(bootstrap_trials, ~ expend)\n\n# Calculating the 95% confidence interval\nwith(bootstrap_effect_sizes, mean(slope) + c(-2, 2) * sd(slope))\n\n[1] -33.765958  -8.135405\n\n\nWe get a rather surprising plot and a 95% confidence interval value. The plot suggests that with increasing college expenditure, sat scores reduce. The confidence interval is adding evidence to it by showing that the mean of the effect size lies within a negative interval. So what‚Äôs happening? Should we believe our model?\nBefore we decide on anything, let us add a covariate to the model. We will add the variable frac to the model which denotes the percentage of all eligible students taking the SAT. Let us build the model.\n\nlibrary(mosaicData)\nlibrary(statisticalModeling)\nlibrary(ggplot2)\n\n# Building a linear model\nmodel_lm_cov &lt;- lm(sat ~ expend + frac, data = SAT)\n\n# Plotting the model\nfmodel(model_lm_cov) + theme_bw()\n\n\n\n# Bootstrap replications: 100 trials\nbootstrap_trials_cov &lt;- ensemble(model_lm_cov, nreps = 100)\n\n# Calculating the effect sizes of salary from the bootstrap samples\nbootstrap_effect_sizes_cov &lt;- effect_size(bootstrap_trials, ~ expend)\n\n# Calculating the 95% confidence interval\nwith(bootstrap_effect_sizes_cov, mean(slope) + c(-2, 2) * sd(slope))\n\n[1] -33.765958  -8.135405\n\n\nGuess we got quite the opposite result now. Let us calculate the mean prediction error using the cross-validation technique and see which of these models is better.\n\nlibrary(mosaicData)\nlibrary(statisticalModeling)\n\n# Building a linear model\nmodel_lm &lt;- lm(sat ~ expend, data = SAT)\n\n# Building a linear model with covariate\nmodel_lm_cov &lt;- lm(sat ~ expend + frac, data = SAT)\n\n# Calculating the m.s.e values\ntrials_mse &lt;- cv_pred_error(model_lm, model_lm_cov)\n\n# Printing mse\ntrials_mse\n\n\n\n  \n\n\n# Doing a t.test\nt.test(mse ~ model, trials_mse)\n\n\n    Welch Two Sample t-test\n\ndata:  mse by model\nt = 142.26, df = 5.3575, p-value = 8.603e-11\nalternative hypothesis: true difference in means between group model_lm and group model_lm_cov is not equal to 0\n95 percent confidence interval:\n 3704.738 3838.352\nsample estimates:\n    mean in group model_lm mean in group model_lm_cov \n                  4921.496                   1149.951 \n\n\nFrom the t.test results, the model with the covariate has a lower mean prediction error value as compared to the model without the covariate. Adding frac as the covariate seems to significantly improve our model.\nThe reason why I emphasized covariates is because collinearity is introduced to the model as a result of our choice of covariates. To check for collinearity, we build a linear model with our group of explanatory variables that we want to check. Then we will find the R-squared value from the model, the greater the R-squared value, the greater the collinearity between these variables. Let us find the collinearity between expend and frac. We can either use the summary() function to get the model summary and then get the R-squared value from the summary or use the rsquared() function from the mosaic package in R.\n\nif (!require(mosaic)) install.packages('mosaic')\nlibrary(mosaicData)\nlibrary(mosaic)\n\n# Building a linear model\nmodel_cov &lt;- lm(expend ~ frac, data = SAT)\n\n# Getting the summary of the model to get Rsquared value\nsummary(model_cov)\n\n\nCall:\nlm(formula = expend ~ frac, data = SAT)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7951 -0.7441 -0.2177  0.5983  2.8197 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.84179    0.26101  18.550  &lt; 2e-16 ***\nfrac         0.03018    0.00592   5.097 5.78e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.109 on 48 degrees of freedom\nMultiple R-squared:  0.3512,    Adjusted R-squared:  0.3377 \nF-statistic: 25.98 on 1 and 48 DF,  p-value: 5.781e-06\n\n# Getting the Rsquared value\nrsquared(model_cov)\n\n[1] 0.3512072\n\n\nFrom the model summary, the value that we are interested in is given in the ‚ÄòMultiple R-squared‚Äô section, which is 0.35, which is the same as the output given by the rsquared() function. The value of 0.35 means that 35% of the variation seen in the expend variable is explained by frac. This suggests that there is some level of redundancy between the expend and frac. Greater the R-squared value, the greater the redundancy between the variables.\nR-squared is also represented in a different way called ‚Äòvariance inflation factor (VIF)‚Äô which measures the factor by which the correlations amongst the predictor variables inflate the variance of the standard error of the effect size. Okay, that was a handful, let us see what this means. First, let us see the formula for VIF;\nVIF = \\frac{1}{1-R^2}\nHere, if we have an R-squared value of 0, which means there is no collinearity between the variables, then VIF would be 1. Let us assume we get a VIF value of 2. In the current context, this means that the variance of the effect sizes calculated from bootstrap trials is increased by a factor of 2. If variance increases, then the standard error associated with the effect sizes increases and thereby it will lead to reduced precision. A result of collinearity is often getting a wider confidence interval thereby giving us an unprecise estimation of; for example effect sizes in this case.\nIf we take the square root of VIF, then we get the ‚Äòstandard error inflation factor‚Äô which tells us by how much factor the standard error of the effect sizes calculated from bootstrap trails inflate.\nLet us see all this in action from the codes below;\nWe will add another covariate to our earlier model; salary. The variable salary denotes the estimated average annual salary of teachers in public elementary and secondary schools.\nLet us first check for collinearity between the salary and expend.\n\nlibrary(mosaic)\nlibrary(mosaicData)\n\n# Building a linear model to check collinearity\nmodel_collinear_salary &lt;- lm(expend ~ salary, data = SAT)\n\n# Getting the Rsquared value\nrsquared(model_collinear_salary)\n\n[1] 0.7565547\n\n\nLooks like we opened Pandora‚Äôs box. Now let us build two models, one without salary and one with salary and, see how the prediction error of the effect sizes changes. The two models will have the SAT scores as the response variable. We will calculate the standard error of effect sizes within the effect_size() formula by assigning bootstrap = TRUE.\n\nlibrary(statisticalModeling)\nlibrary(mosaicData)\n\n# Model 1 without salary\nmodel_1 &lt;- lm(sat ~ expend, data = SAT)\n\n# Model 2 with salary\nmodel_2 &lt;- lm(sat ~ expend + salary, data = SAT)\n\n# Calculating effect sizes and standard error via bootstrapping\nhead(effect_size(model_1, ~ expend, bootstrap = TRUE), n = 1)\n\n\n\n  \n\n\nhead(effect_size(model_2, ~ expend, bootstrap = TRUE), n = 1)\n\n\n\n  \n\n\n\nYou can see that the standard error has increased which is due to the effect of collinearity between expend and salary.\nThe statisticalModeling package comes with the collinearity() function which can be used to calculate how much the effect size might (at a maximum) be influenced by collinearity with the other explanatory variables. Essentially, the collinearity() function calculates the square root of VIF which denotes the inflation of standard errors. Let us check the collinearity between expend and salary variables.\n\nlibrary(statisticalModeling)\nlibrary(mosaicData)\n\n# Calculating the collinearity\ncollinearity(~ expend + salary, data = SAT)\n\n\n\n  \n\n\n\nInteraction between collinear variables can also increase the standard error of effect size.\n\nlibrary(statisticalModeling)\nlibrary(mosaicData)\n\n# Model 3 with salary interaction\nmodel_3 &lt;- lm(sat ~ expend * salary, data = SAT)\n\n# Calculating effect sizes and standard error via bootstrapping\nhead(effect_size(model_3, ~ expend, bootstrap = TRUE), n = 1)\n\n\n\n  \n\n\n\nAs you can see, we got a higher standard error value as compared to model_1. Let us also calculate the mean prediction error for the three models we created.\n\nlibrary(statisticalModeling)\nlibrary(mosaicData)\nlibrary(dplyr)\n\n# Building the models\nmodel_1 &lt;- lm(sat ~ expend, data = SAT)\nmodel_2 &lt;- lm(sat ~ expend + salary, data = SAT)\nmodel_3 &lt;- lm(sat ~ expend * salary, data = SAT)\n\n# Calculating the mean prediction error for 100 trials\nmodel_mse &lt;- cv_pred_error(model_1, model_2, model_3, ntrials = 100)\n\n# Calculating the mean and sd of mse\nmodel_mse_mean &lt;- model_mse %&gt;% group_by(model) %&gt;%\n  summarise(mean = mean(mse),\n            sd = sd(mse))\n\n# Printing the mean and sd of mse\nmodel_mse_mean\n\n\n\n  \n\n\n\nSince I did not set seed, you might not get the same results as I got. But essentially we can use the mean prediction error values to also choose the better model out of the three.\nIn the end, from these two tutorials, we can sum up our criteria for model comparison. We should check for the following while doing the comparison;\n\nCross-validated prediction error\nInflation due to collinearity\nThe standard error of effect size"
  },
  {
    "objectID": "tutorials/stat_model/inter_stat_model.html#conclusion",
    "href": "tutorials/stat_model/inter_stat_model.html#conclusion",
    "title": "Intermediate statistical modelling in R",
    "section": "\n13 Conclusion",
    "text": "13 Conclusion\nFirst of all, congratulations on completing the intermediate statistical modelling tutorial (bonus points if you completed the introduction tutorial also). In a nutshell, this is what we have learned from this tutorial;\n\nInterpreting effect size when the response variable is categorical\nPlotting model output using the fmodel() function from the {statisticalModeling} package\nInteraction terms\nPolynomial regression\nTotal and partial change\nInterpreting the R-squared value in terms of model output and residuals\nBootstrapping technique to measure the precision of model statistics\n\nScales and transformation\n\nLog transformation\nRank transformation\n\n\nCollinearity\n\nYou have mastered almost everything that is there in linear modelling. Congratulations! But, the journey is not over. The next stop is ‚ÄôGeneralized Linear Models in R`. This is a realm where we deal with complex datasets that cannot be analysed by our trusty linear regression models. Anyway, that‚Äôs a problem for tomorrow. Practice the concepts you gained from both the tutorials and come ready for the next tutorial ‚úåÔ∏è"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html",
    "href": "tutorials/stat_model/intro_stat_model.html",
    "title": "Introduction to statistical modelling in R",
    "section": "",
    "text": "require(\"https://cdn.jsdelivr.net/npm/juxtaposejs@1.1.6/build/js/juxtapose.min.js\")\n  .catch(() =&gt; null)"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#definition-of-statistical-model",
    "href": "tutorials/stat_model/intro_stat_model.html#definition-of-statistical-model",
    "title": "Introduction to statistical modelling in R",
    "section": "\n1 Definition of statistical model",
    "text": "1 Definition of statistical model\nBefore we get to know the definition of what a statistical model is, let us first try to understand what a model means. All of us have an intuitive idea of what a ‚Äòmodel‚Äô is. A model can be a fashion model posing in pretty clothes and makeup or it can be a blueprint which represents the overall layout of a building. In all these understandings of a model, we can summarise that it is representing a relationship.\nA statistical model also belongs to the same suite but is made using data. It is also representing ‚Äòrelationships‚Äô, but between variables within the data. Understanding a statistical model can lead us to understand how the variables within a dataset are related to each other.\nLet us consider the graph given below;\n\n\n\n\n\nThe x-axis shows speed and the y-axis shows stopping distance. From the graph, you can see that, as speed increases, the stopping distance also increases. Therefore we can say that the stopping distance linearly increases with speed. To showcase this linear relationship, we can plot a regression line (green line) as shown below.\n\n\n\n\n\nThe regression line can be extended to values outside the dataset and doing so would enable us to predict values outside our dataset, like predicting what the stopping distance will be if the speed was 35 mph. You can also see that many points are away from the regression line and do not lie on it, therefore the linear relationship we predicted for the variables is not a perfect one. Suppose instead of a linear relationship, we fit a polynomial regression line.\n\n\n\n\n\nIn this graph, especially at the extreme ends, most of the points are closer to the regression line than in the earlier case. Therefore, the residual distance, which is the distance between the actual data points and the fitted values by regression lines would be smaller indicating a better fit for the polynomial regression. Therefore a good fit can more accurately represent the relationship between the variables and therefore would have better predictive power. But how do we exactly compare the linear regression and the polynomial regression ‚Äòmodels‚Äô? Is there a metric which would allow this comparison easier? We will learn all about this later in this article.\nGoing back to the linear regression plot, suppose, instead of speed, we use the colour of the car and see if it affects the stopping distance at a constant speed. For obvious reasons, there should be no effect of colour on the stopping distances (duh!). Nevertheless, now we also know that some variables form relationships and some don‚Äôt.\nIn short, what we essentially were doing was linear modelling, a type of statistical modelling. And this exercise helped us realise that a model can inform us about;\n\nThe relationship between the variables in the data\nWhich variables form relationships\nPredict values outside our dataset\nModel comparisons (linear vs polynomial)\n\nThe textbook definition for statistical modelling is;\n\nA statistical model is a set of probability distributions P(S) on a sample space (S) (1).\n\nIn our previous modelling exercise, we saw how speed affects stopping distance. Here the variable ‚Äòspeed‚Äô is called the parameter or otherwise, better called the explanatory variable, which tries to explain the variations seen in the response variable, which is the stopping distance. The model we created was not perfect as there were many data points which did not lie on the regression line. One potential source of this error can arrive from the selection of the explanatory variables. The dataset had stopping distances of a particular vehicle at different speeds. The stopping distance can also be affected by the road terrain (friction) and the efficiency of the brakes, all of which were missing from the dataset. Therefore these parameters that we have not accounted for can also potentially affect our model. So why weren‚Äôt those included in the dataset? Often it is time-consuming to keep note of every parameter that can affect the variable we are interested in and therefore, they are often excluded from data collection. Thus, the model created using this data would only be a close approximation of what is happening in the real world. This particular drawback is emphasised in the following aphorism;\n\n‚ÄúAll models are wrong, but some are useful‚Äù - George Box (2)\n\nThus even though our model is not the perfect one1, if it reliably predicts values then the model is considered useful. Thus a statistical model is stochastic.\nFrom a mathematical point of view, an ideal model would contain all the parameters that affect the response variable. Thus the sample space (S) will contain all possible combinations of [explanatory variables, stopping distance] values. Therefore, the set of probability distributions corresponding to all these combinations will be P(S). But in real life, we cannot afford to measure all the parameters concerning the response variable and are only interested in a small set of variables. Thus we often have a subset of the total possible combinations of [explanatory variables, stopping distance] values (\\theta), and the corresponding probability distribution for these combinations will be P(\\theta). The model thus created with a small set of parameters is called a parametrised statistical model.\nFor all such models; P(\\theta) \\subset P(S).\nThis definition is not exactly needed to do modelling but it‚Äôs good to know the actual mathematical definition of what a statistical model is.\nNow that we have a rough idea of what a model is, let us know learn how to build models in R."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#making-life-easier",
    "href": "tutorials/stat_model/intro_stat_model.html#making-life-easier",
    "title": "Introduction to statistical modelling in R",
    "section": "\n2 Making life easier",
    "text": "2 Making life easier\nPlease install and load the necessary packages and datasets which are listed below for a seamless tutorial session. (Not required but can be very helpful if you are following this tutorial code by code)\n\n# Libraries used in this tutorial\ninstall.packages('cherryblossom')\ninstall.packages('rpart')\ninstall.packages('datasets')\ninstall.packages('NHANES')\ninstall.packages('rpart.plot')\ninstall.packages('mosaicModel')\ninstall.packages('devtools')\ndevtools::install_github(\"dtkaplan/statisticalModeling\")\n\n# Loading the libraries\ntutorial_packages &lt;- c(\"cherryblossom\", \"rpart\", \"datasets\", \"NHANES\",\n                 \"rpart.plot\", \"mosaicModel\", \"statisticalModeling\")\n\nlapply(tutorial_packages, library, character.only = TRUE)\n\n# Datasets used in this tutorial\ndata(\"run17\") # From cherryblossom\ndata(\"ToothGrowth\") # From datasets\ndata(\"chickwts\") # From datasets\ndata(\"NHANES\") # From NHANES\ndata(\"Tadpoles\") # From mosaicModel\ndata(\"Used_Fords\") # From mosaicModel"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#building-a-model",
    "href": "tutorials/stat_model/intro_stat_model.html#building-a-model",
    "title": "Introduction to statistical modelling in R",
    "section": "\n3 Building a model",
    "text": "3 Building a model\nThe pipeline for building a model is as follows;\n\n\n\n\ngraph LR\nsubgraph Pipeline for making a statistical model\nA(Collect data) --&gt; B(Select explanatory&lt;br&gt; and response variables) --&gt; C(Select model architecture) --&gt; D(Build the model)\nend\n\n\n\n\n\nThe first step in building a model is by acquiring data. Then we need to select the appropriate response variable (dependent variable) and the explanatory variables (independent variable). In a cancer drug trial experiment data, tumour size can be the response variable and the drug type and patient age can be the explanatory variables. After selecting the variables, the model architecture is chosen. In our earlier example, we used a linear model to explain the changes seen in stopping distances with speed. Choosing a model architecture depends on the nature of the data. For now, we will mostly be using the linear model lm(). But throughout the tutorial, we will also see other model architectures. The final step is to build the model, which is done by the computer.\nThe syntax for building a model in R is as follows;\n\nfunction(response ~ explanaotry1 + explanatory2, data = dataset_name)\n\n\n3.1 Linear model\nLet us try to plot some models using the linear model architecture using the lm()function in R. We will be using the run17 dataset from the cherryblossom package in R. The run17 dataset contains details for all 19,961 runners in the 2017 Cherry Blossom Run, which is an annual road race that takes place in Washington, DC, USA. Also, the Cherry Blossom Run has two events; a 10 Mile marathon and a 5 Km run. For now, we will be concerned with participants that participated in the 10 Mile marathon only.\nIn the dataset, we are interested to check whether the net time to complete the 10 Mile marathon (net_sec) is affected by the age and sex of the participant. So for making the model, we use net_sec as the response variable and, age and sex as the explanatory variables.\nWe can use the summary() function to summarise the model. For now, we will not worry about the summary details. Then, using the ggplot2 package, we will plot the model using the stat_smooth() function. Please not that there is another function geom_smooth() which is an alias of stat_smooth(). Both do the same thing.\n\nif (!require(cherryblossom)) install.packages('cherryblossom')\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Checking headers in the data\nhead(run17)\n\n\n\n  \n\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon &lt;- run17 %&gt;% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_1 &lt;- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Get the summary of the model\n# For now don't worry about the details\nsummary(model_1)\n\n\nCall:\nlm(formula = net_sec ~ age + sex, data = run17_marathon)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2814.89  -672.65   -41.43   625.34  3112.81 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5475.8377    25.8841  211.55   &lt;2e-16 ***\nage           17.9157     0.6789   26.39   &lt;2e-16 ***\nsexM        -674.3765    14.8290  -45.48   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 947.5 on 17439 degrees of freedom\nMultiple R-squared:  0.1229,    Adjusted R-squared:  0.1228 \nF-statistic:  1222 on 2 and 17439 DF,  p-value: &lt; 2.2e-16\n\n# Plotting the model using ggplot2\n# Use stat_smooth() function and specify \"lm\"\n\nrun17_marathon %&gt;% ggplot(aes(age, net_sec, col = sex)) +\n  stat_smooth(method = \"lm\") + \n  labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex\") + \n  theme_bw()\n\n\n\n\nFrom the graph, it seems that female participants take more time to complete the run compared to their male counterparts.\n\n3.2 Logistic model\nNow let us try to see whether the participant‚Äôs choice in choosing between the events is affected by their age and sex. We can hypothesize that older participants of both sexes would prefer the 5 km run over 10 Mile marathon. Thus, we use the variable event as the response variable and, age and sex as the explanatory variables. We are more interested to see the effect of age as compared to sex, therefore the variable sex is considered a covariate. We will learn later what covariate mean\nWe will use the lm() function to make a linear model. Here we will convert the event variable to boolean values for it to work with the formula. The corresponding values are; 1 = 10 Mile event and 0 = 5 Km event.\n\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Converting event variable values into boolean \nrun17_boolean &lt;- run17\nrun17_boolean$event &lt;- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Building the linear model\nmodel_2 &lt;- lm(event ~ age + sex, data = run17_boolean)\n\n# Plotting the model using ggplot2\nrun17_boolean %&gt;% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"lm\") +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n\n\n\nFrom the graph, it seems like, for male participants, age doesn‚Äôt affect their event choice selection. But for females, compared to males, age does seem to affect it. Therefore older female participants prefer 5 km runs as compared to older male participants.\nLet us look at the dataset more closely, especially the response variable.\n\n# Checking the data type of the response variable\nclass(run17$event)\n\n[1] \"character\"\n\n# Printing the unique strings from the response variable\nunique(run17$event)\n\n[1] \"10 Mile\" \"5K\"     \n\n\nOur response variable is dichotomous and thus not continuous. The lm() function we used works best for continuous numerical data. So the model architecture we used was not an appropriate one here. So in these situations, we can use the logistic modelling architecture. Both logistic modelling and linear modelling are part of generalised linear modelling. We can use the glm() function in R and specify family = binomial to have a logistic model.\nThe syntax for a logistic model in R is;\n\nglm(response_variable ~ explanatory_variable, data = dataset_name, family = \"binomial\")\n\nNow let us try the glm() function and make a logistic model for the earlier case.\n\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Converting event variable values into boolean \nrun17_boolean &lt;- run17\nrun17_boolean$event &lt;- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Building the logistic model\nmodel_3 &lt;- glm(event ~ sex + age, data = run17_boolean, family = \"binomial\")\n\n# Plotting the model using ggplot2\n# Use stat_smooth() function and specify \"glm\"\nrun17_boolean %&gt;% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (logistic model)\") + \n  theme_bw()\n\n\n\n\nThe graph looks very similar to the earlier one. For easy comparison, both the graphs are shown next to each other below. Please use the slider to compare between the graphs.\n\nCodelibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cherryblossom)\n\nrun17_boolean &lt;- run17\nrun17_boolean$event &lt;- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Linear model\nrun17_boolean %&gt;% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"lm\") +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n# Logistic model\nrun17_boolean %&gt;% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (logistic model)\") + \n  theme_bw()\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(cherryblossom)\n\nrun17_boolean &lt;- run17\nrun17_boolean$event &lt;- recode(run17_boolean$event, \"10 Mile\" = 1, \"5K\" = 0)\n\n# Linear model\nrun17_boolean %&gt;% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"lm\") +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n\n\n# Logistic model\nrun17_boolean %&gt;% ggplot(aes(age, event, col = sex)) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(x = \"Age\",\n       y = \"Event choice\",\n       title = \"Event ~ Age + Sex (logistic model)\") + \n  theme_bw()"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#model-evaluation",
    "href": "tutorials/stat_model/intro_stat_model.html#model-evaluation",
    "title": "Introduction to statistical modelling in R",
    "section": "\n4 Model evaluation",
    "text": "4 Model evaluation\nAfter building the model, we can evaluate it by providing new inputs to the model to get corresponding predicted output values. The predicted output values can then be cross-checked against the original output values and see how far off they are in terms of prediction (i.e.¬†the prediction error). Using the predict() function, we can predict for values outside the dataset or evaluate the model using the prediction error.\n\n4.1 Prediciting values\nWe will go back to the first example where we checked if the net time to complete the 10 Mile marathon (net_sec) is affected by the age and sex of the participants. We will create dummy data of participants with random age and sex values and use the model to predict their net time to complete the race.\n\nlibrary(cherryblossom)\n\n# Creating dummy data of different ages\nmale &lt;- data.frame(\"age\" = c(seq(1, 100, 2)),\n                   \"sex\" = c(replicate(50, \"M\")))\nfemale &lt;- data.frame(\"age\" = c(seq(1, 100, 2)),\n                   \"sex\" = c(replicate(50, \"F\")))\n\ndummy_data &lt;- rbind(male, female)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon &lt;- run17 %&gt;% filter(event == \"10 Mile\")\n\n# Building the linear model\nmodel_lm &lt;- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Predicting values\ndummy_data$net_sec &lt;- predict(model_lm, newdata = dummy_data)\n\n# Plotting the predicted values\ndummy_data %&gt;% ggplot(aes(age, net_sec, col = sex)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 100, by = 5)) +\n  labs(x = \"Age\",\n       y = \"Time to complete the marathon (sec)\",\n       title = \"Predicted values\") + \n  theme_bw()\n\n\n\n\nIn the plot, we can see that we have babies as young as 1 year old, who have been predicted to have completed the 10 Mile marathon faster that their older participants. So what on earth did we do to get these results?\n\n\nMy reaction after the model predictions\n\n\nLet us look the linear model plot that we had earlier.\n\n\n\n\n\nYou can see that there is a general trend of increasing ‚Äònet time to complete the race‚Äô as the ‚Äòage‚Äô value increases and decreasing net time as age decreases. Therefore our linear model favours reduced net times for lesser values of age, which is what we see as predicted values.\nSo our model has its limitations in predicting values for a certain range of ageüòÖ.\nThe moral of the story here is that models trained on data can be a bit wild when evaluated outside the range of the data. So we have to be mindful of its prediction abilities, otherwise, we can end up with superhuman babies who can run marathons faster than anyone.\n\n\nIdeal relationship between age and the time to complete the marathon\n\n\n\n4.2 Evaluating a model\nInstead of predicting new values outside the dataset, we can use the same dataset used for model training to predict values. Utilizing these predicted values, we can compare them back to the original values and calculate the prediction error. This is one way to compare different models and see which models predict values closer to the original values in the dataset.\nWe will use our earlier made linear model, where we looked at whether the total time to complete the race is affected by age and sex. We will also make a new model using the ‚Äòrecursive partitioning model architecture‚Äô using the same dataset so that we can have a model to compare with. Using the rpart() function in the rpart package in R, we can build a recursive partitioning model. The rpart() model works for both numerical (dichotomous and discontinuous) and categorical data. We will learn more about rpart models later in this article.\n\nif (!require(rpart)) install.packages('rpart')\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(rpart)\n\n# Checking headers in the data\nhead(run17)\n\n\n\n  \n\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon &lt;- run17 %&gt;% filter(event == \"10 Mile\")\n\n# Building a linear model\nmodel_lm &lt;- lm(net_sec ~ age + sex, data = run17_marathon)\n\n# Building a recursive partitioning model\nmodel_rpart &lt;- rpart(net_sec ~ age + sex, data = run17_marathon)\n\n# Predicting values\nlm_predict &lt;- predict(model_lm, newdata = run17_marathon)\nrpart_predict &lt;- predict(model_rpart, newdata = run17_marathon)\n\n# Calculating error values\nlm_error &lt;- with(run17_marathon, net_sec - lm_predict)\nrpart_error &lt;- with(run17_marathon, net_sec - rpart_predict)\n\n# Printing few error values\nhead(as.data.frame(lm_error))\n\n\n\n  \n\n\nhead(as.data.frame(rpart_error))\n\n\n\n  \n\n\n\nNow we have data frames containing error values calculated between each of the original values in the dataset to the predicted values from the model. You can see that it‚Äôs tedious to compare the error values of the linear model to the logistic model, also some of these error values are negative, which makes the comparison even harder. So how can we know which model is better? Calculating the mean of the square of the prediction errors (m.s.e) would be a great way to start. The m.s.e will reflect the magnitude and not the sign of the errors.\n\n# Calculate the mean of the square of the prediction errors (m.s.e)\nmean(lm_error ^ 2, na.rm = T)\n\n[1] 897630\n\nmean(rpart_error ^ 2, na.rm = T)\n\n[1] 905886.9\n\n\nThe linear model has a lower error value compared to the recursive partitioning model. Therefore the fitted values in the linear model are closer to the actual value in the dataset compare to the other model.\nWe can also plot the predicted values and see how they look. Please use the slider to compare the graphs.\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon &lt;- run17 %&gt;% filter(event == \"10 Mile\")\n\n# Plotting the linear model\nrun17_marathon %&gt;% ggplot(aes(age, net_sec, col = sex)) +\n  stat_smooth(method = \"lm\") + \n  labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n# Adding rpart predicted values to the dataset\nrpart_predict &lt;- predict(model_rpart, newdata = run17_marathon)\nrun17_marathon$rpart_fitted_values &lt;- rpart_predict\n\n# Plotting the recursive partitioning model\nrun17_marathon %&gt;% ggplot(aes(age, rpart_predict, col = sex)) +\n  geom_line() +\n    labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (recursive partitioning model)\") + \n  theme_bw()\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Filtering the data for 10 Mile marathon participants\nrun17_marathon &lt;- run17 %&gt;% filter(event == \"10 Mile\")\n\n# Plotting the linear model\nrun17_marathon %&gt;% ggplot(aes(age, net_sec, col = sex)) +\n  stat_smooth(method = \"lm\") + \n  labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (linear model)\") + \n  theme_bw()\n\n\n\n# Adding rpart predicted values to the dataset\nrpart_predict &lt;- predict(model_rpart, newdata = run17_marathon)\nrun17_marathon$rpart_fitted_values &lt;- rpart_predict\n\n# Plotting the recursive partitioning model\nrun17_marathon %&gt;% ggplot(aes(age, rpart_predict, col = sex)) +\n  geom_line() +\n    labs(x = \"Age\",\n       y = \"Time to complete 10 Mile marathon (sec)\",\n       title = \"Net time ~ Age + Sex (recursive partitioning model)\") + \n  theme_bw()\n\n\n\n\n\n\n4.3 Choosing the explanatory variables\nIn this exercise, we will use the ToothGrowth dataset from the {datasets} package in R. The dataset is from an experiment that looked at the effect of vitamin C on tooth growth in guinea pigs. In the experiment, three different doses of vitamin C were administered to 60 guinea pigs. Vitamin C was administered either through orange juice or ascorbic acid. The length of odontoblasts (cells responsible for tooth growth) was measured to check for tooth growth.\nWe will be creating a linear model to check if tooth growth is affected by vitamin C dosages. At the same time, there is also a chance that tooth growth is affected by the method of vitamin C administration. Therefore, we have the choice of choosing between ‚Äòvitamin C dosage‚Äô and the ‚Äòmethod of vitamin C administration‚Äô as the explanatory variables. So how do we know which of these variables is better at predicting tooth growth values? To find the best predictor, we can first build a model using ‚Äòvitamin C dosage‚Äô as the only explanatory variable. Then later, we can build yet another model using both ‚Äòvitamin C dosage‚Äô and the ‚Äòmethod of vitamin C administration‚Äô as the explanatory variables. Then similiar to the ealrier case, we can use the mean of the square of the prediction errors can see which model is better.\nIn the ToothGrowth dataset, the variable len contains the length of the odontoblasts, supp contains the method of vitamin C administration (VC - ascorbic acid, OJ - Orange juice) and dose contains the dosage of vitamin C administered in mg/day units.\n\nif (!require(datasets)) install.packages('datasets')\nlibrary(datasets)\n\ndata(\"ToothGrowth\")\n\n# Building a linear model with only dose\nmodel_dose &lt;- lm(len ~ dose, data = ToothGrowth)\n\n# Building a linear model with dose + supp\nmodel_dose_supp &lt;- lm(len ~ dose + supp, data = ToothGrowth)\n\n# Predicting values using the trained dataset\npredict_dose &lt;- predict(model_dose, newdata = ToothGrowth)\npredict_dose_supp &lt;- predict(model_dose_supp, newdata = ToothGrowth)\n\n# Calculating error values\nerror_dose &lt;- with(ToothGrowth, dose - predict_dose)\nerror_dose_supp &lt;- with(ToothGrowth, dose - predict_dose_supp)\n\n# Calculate the mean of the square of the prediction errors\nmean(error_dose ^ 2, na.rm = T)\n\n[1] 341.2716\n\nmean(error_dose_supp ^ 2, na.rm = T)\n\n[1] 344.6941\n\n\nYou can see that the model with both vitamin C dosage and method of vitamin C administration as the explanatory variables has a greater error value compared to the model with only vitamin C dosage. Therefore adding the method of vitamin C administration as the explanatory variable did not improve the model and therefore can be excluded from the analysis.\n\n4.4 Cross validation\nSo far we have been using the training dataset for predicting values. But there is a problem in using it for our analysis as it allows models with the additional explanatory variable to have smaller prediction errors than the base model. Let‚Äôs see what this means in the following code.\nHere we use the chickwts dataset from the {datasets} package in R. In this dataset, there are two variables, weight which tells the weight (g) of the chick measured after 6 weeks and the feed variable, which tells the type of feed that was given to the chicks for those 6 weeks. There were 6 different types of feed used. The experiment was done to check whether the feed type has any effect on the weights of the chick.\nHere we will build a linear model with weight as the response variable and feed type as the explanatory variable. Then we will make a random variable within the dataset which contains values which have no explanatory power. It will be filled with random numbers which have no relationship with the original data. We will build yet another linear model like earlier but this time we shall use both the feed type and the random variable as the explanatory variable. Adding a random variable to the model should cause the model to perform poorly and therefore should lead to an increased m.s.e value when compared to the model with just the feed type variable as the explanatory variable. Let us see if that‚Äôs the case here.\n\nlibrary(datasets)\n\ndata(\"chickwts\")\n\n# Creating a random variable\n# The variable contains random numbers\nset.seed(231)\nchickwts$random &lt;- rnorm(nrow(chickwts))\n\n# Building a linear model with only feed\nmodel_feed &lt;- lm(weight ~ feed, data = chickwts)\n\n# Building a linear model with dose + random\nmodel_feed_random &lt;- lm(weight ~ feed + random, data = chickwts)\n\n# Predicting values using the trained dataset\npredict_feed &lt;- predict(model_feed, newdata = chickwts)\npredict_feed_random &lt;- predict(model_feed_random, newdata = chickwts)\n\n# Calculating error values\nerror_feed &lt;- with(chickwts, weight - predict_feed)\nerror_feed_random &lt;- with(chickwts, weight - predict_feed_random)\n\n# Calculate mean of the square of the prediction errors (m.s.e)\nmean(error_feed ^ 2, na.rm = T)\n\n[1] 2754.31\n\nmean(error_feed_random ^ 2, na.rm = T)\n\n[1] 2725.115\n\n\nWell, quite a surprise, right? The model with the random variable is having a lower error value as compared to the model with the correct explanatory variable. We will see later (not in this article though) why the error value was lower. But for now, keep in mind that when you use the same dataset to do both training and prediction in a model, the model with the additional explanatory variable will have smaller prediction errors than the base model, as seen here. Therefore this mistake can throw off our analysis and gives us a false positive that some of the explanatory variables form a relationship with the response variable when in reality there is no effect.\nTo mitigate this problem, we use a technique called ‚Äòcross-validation‚Äô. In this technique, we split the original data into two parts; a training set and a testing set. Both sets will have data points that are chosen at random from the original dataset. We train our model using the training set and then test our model using the testing set and then calculate the m.s.e. value. Thus the explanatory variable values in the testing set will be novel to the model. Let us see how we can do this.\nWe will be reusing our earlier example of chick weight and feed type.\nIn the code given below, the rnorm(nrow(chickwts)) &gt; 0 function will assign TRUE and FALSE values at random to the row values. Looking at this function more closely, the rnorm() function will choose random numbers up to the number of rows in the dataset, and since there is an inequality (greater than zero), it will assign TRUE if the random number is greater than zero and vice versa. The row values with TRUE will go to the training set and rows with FALSE will form the testing set.\n\nlibrary(datasets)\n\ndata(\"chickwts\")\n\n# Creating training set\nset.seed(231)\nchickwts$training_set &lt;- rnorm(nrow(chickwts)) &gt; 0\nchickwts$random &lt;- rnorm(nrow(chickwts)) &gt; 0\n\n# Building the linear model using the training set\nmodel_feed &lt;- lm(weight ~ feed, data = subset(chickwts, training_set))\n\n# Building the linear model using the training set but with random variable\nmodel_feed_random &lt;- lm(weight ~ feed + random, data = subset(chickwts, training_set))\n\n# Predicting values using the testing set\n# !training_set means row values with FALSE value\npredict_feed &lt;- predict(model_feed, newdata = subset(chickwts, !training_set))\npredict_feed_random &lt;- predict(model_feed_random, newdata = subset(chickwts, !training_set))\n\n# Calculating error values using the testing data\nerror_feed &lt;- with(subset(chickwts, !training_set), weight - predict_feed)\nerror_feed_random &lt;- with(subset(chickwts, !training_set), weight - predict_feed_random)\n\n# Calculate the mean of the square of the prediction errors (m.s.e)\nmean(error_feed ^ 2, na.rm = T)\n\n[1] 2830.79\n\nmean(error_feed_random ^ 2, na.rm = T)\n\n[1] 2732.715\n\n\nFor the seed I have set, using the cross-validation method, we seem to not solve the problem we had earlier. The model with the random variable as the explanatory variable still has a lower error value as compared to the model without the random variable. But this was just an opportunistic case, as the training and testing sets are chosen at random. You might not get the same result as mine if you run this code (provided that the set.seed() is changed). We can deal with this randomness by rerunning the calculation many times to get a more consistent measure of the error value.\nWe will use the cv_pred_error() function from the statisticalModeling package to rerun the calculations many times. The function automatically makes the training and testing sets using the original dataset and also calculates the m.s.e for each trial. In the code given below, we store the results from the cv_pred_error() function into a variable called ‚Äòtrials‚Äô. The variable ‚Äòtrials‚Äô will have two columns in it; mse which denotes the mean of the square of the prediction errors (m.s.e) and model which denotes the name of the model given as input. Then in the final step, we compare the m.s.e values between the model using a simple t-test.\n\nif (!require(devtools)) install.packages('devtools')\nif (!require(statisticalModeling)) devtools::install_github(\"dtkaplan/statisticalModeling\")\nlibrary(statisticalModeling)\nlibrary(datasets)\n\ndata(\"chickwts\")\n\n# Creating a random variable\n# The variable contains random numbers\nset.seed(231)\nchickwts$random &lt;- rnorm(nrow(chickwts)) &gt; 0\n\n# Building a linear model with only feed\nmodel_feed &lt;- lm(weight ~ feed, data = chickwts)\n\n# Building a linear model with dose + random\nmodel_feed_random &lt;- lm(weight ~ feed + random, data = chickwts)\n\n# Rerunning the models (100 times for each model)\ntrials &lt;- cv_pred_error(model_feed, model_feed_random, ntrials = 100)\n\n# Compare the two sets of cross-validated errors\nt.test(mse ~ model, data = trials)\n\n\n    Welch Two Sample t-test\n\ndata:  mse by model\nt = -5.9019, df = 197.95, p-value = 1.537e-08\nalternative hypothesis: true difference in means between group model_feed and group model_feed_random is not equal to 0\n95 percent confidence interval:\n -118.53296  -59.16003\nsample estimates:\n       mean in group model_feed mean in group model_feed_random \n                       3309.232                        3398.079 \n\n\nFor \\alpha = 0.05 level of significance, we have a p-value &lt; 0.05, which means that the mean error values between the models are not the same and are different from each other. From the t-test summary, we can see that the mean error value of the model without the random variable is lower than the model with the random variable (3309.232 &lt; 3398.079). Therefore we can conclude that the addition of the random variable does not improve the model.\nTherefore through the cross-validation technique iterated over many times, in conjunction with the m.s.e values, we can identify which of the variables in our data should be considered as the explanatory variables.\n\n4.5 Prediction error for categorical response variable\nSo far when we were calculating the predictive error values, the response variable we had was numerical. But what if our response variable was a categorical value, then how will we compare models using the predictive error values?\nLet us go back to the run17 dataset from the cherryblossom package in R. In this dataset, we looked at whether the participants‚Äô choice of event was influenced by their age and sex. We hypothesised that the older participants of both sexes will prefer the 5 Km run as compared to the 10 Mile marathon. In the earlier example, we have used the logistic model, but here let us use the recursive partitioning model made using the rpart() function in the rpart package in R. We had learned briefly that the recursive partitioning model is appropriate when the response variable is categorical, which is the case here.\nLike earlier, we will use event as the response variable and sex and age as the explanatory variable. The hypothesis remains the same, irrespective of sex, older participants will prefer the 5 Km run as compared to the 10 Mile marathon. We will build two models, one with only age as the explanatory variable and the other with both age and sex as the explanatory variables.\nWhile predicting for values, we use type = \"class\" so that the model gives prediction values, which are either ‚Äú10 Mile‚Äù or ‚Äú5 Km‚Äù (gives prediction values as categorical values).\nThen we will evaluate these two models by comparing the m.s.e values. This will tell us whether adding the variable sex improves the model or not.\n\nlibrary(cherryblossom)\nlibrary(dplyr)\nlibrary(rpart)\n\n# Creating training set\nset.seed(123)\nrun17$training_set &lt;- rnorm(nrow(run17)) &gt; 0\n\n# Training the model with the training set\n# Building the recursive partitioning model with just age\nmodel_rpart_age &lt;- rpart(event ~ age, data = subset(run17, training_set))\n\n# Building the recursive partitioning model with age + sex\nmodel_rpart_age_sex &lt;- rpart(event ~ age + sex, data = subset(run17, training_set))\n\n# Predicting values using the testing set\npredict_age &lt;- predict(model_rpart_age,\n                       newdata = subset(run17, !training_set), type = \"class\")\npredict_age_sex &lt;- predict(model_rpart_age_sex,\n                           newdata = subset(run17, !training_set), type = \"class\")\n\n# Printing a few row values\nhead(predict_age)\n\n      1       2       3       4       5       6 \n10 Mile 10 Mile 10 Mile 10 Mile 10 Mile 10 Mile \nLevels: 10 Mile 5K\n\nhead(predict_age_sex)\n\n      1       2       3       4       5       6 \n10 Mile 10 Mile 10 Mile 10 Mile 10 Mile 10 Mile \nLevels: 10 Mile 5K\n\nhead(run17$event)\n\n[1] \"10 Mile\" \"10 Mile\" \"10 Mile\" \"10 Mile\" \"10 Mile\" \"10 Mile\"\n\n\nIn the first 6 row values of the output, both models seem to agree with the values from the original dataset. But our dataset has 199961 rows of data. It would be crazy to even think that one can compare each of the row values between the models instead, we will try to quantify the error. In earlier cases, we could subtract the predicted values from the response variable values in the dataset to get the error value. But that is not possible here as the response variable is categorical.\nOne way to calculate the error values of these models is to see how many mistakes the model made. This can be calculated by checking how many times the predicted value by the model was not equal to the value in the dataset.\n\n# Calculating the sum of errors using the testing data\nwith(data = subset(run17, !training_set), sum(predict_age != event))\n\n[1] 1254\n\nwith(data = subset(run17, !training_set), sum(predict_age_sex != event))\n\n[1] 1254\n\n\nThe number of errors each model made is the same. Before we conclude anything let us try to see some more ways to quantify the error.\nInstead of sum, we can calculate the mean rate of errors also.\n\n# Calculating the mean of errors using the testing data\nwith(data = subset(run17, !training_set), mean(predict_age != event))\n\n[1] 0.1243061\n\nwith(data = subset(run17, !training_set), mean(predict_age_sex != event))\n\n[1] 0.1243061\n\n\nMean error values are the same (no surprises here).\nLet us go one step further. Till now our model predicted a deterministic value to the response variable. It can either be 10 Mile marathon or a 5 km run. Instead of this, we can use the model to predict the probability values to the categorical values present in the response variable. This means, our model will predict how likely a certain participant of a particular age and sex will choose between a 10 Mile marathon and a 5 Km run. To predict probability values, instead of type = \"class\", we will use type = \"prob\" within the predict() function.\n\n# Predicting probability values using the testing set\npredict_age_prob &lt;- predict(model_rpart_age,\n                       newdata = subset(run17, !training_set), type = \"prob\")\npredict_age_sex_prob &lt;- predict(model_rpart_age_sex,\n                           newdata = subset(run17, !training_set), type = \"prob\")\n\n# Comparing the predicted value to the actual value in the dataset\ntesting_data &lt;- run17 %&gt;% select(training_set, event) %&gt;% filter(training_set == F)\ncompare_values &lt;- data.frame(testing_data, predict_age_prob, predict_age_sex_prob)\n\n# Changing the column names for making sense of the column values\ncolnames(compare_values)[c(3:6)] &lt;- c(\"Ten_Mile_age\", \"Five_km_age\", \"Ten_Mile_age_sex\", \"Five_km_age_sex\")\n\n# Printing a few row values\nhead(compare_values)\n\n\n\n  \n\n\n\nThe columns Ten_Mile_age and Five_km_age corresponds to probability values from the model_rpart_age model with just ‚Äòage‚Äô as the explanatory variable. The last two columns Ten_Mile_age_sex and Five_km_age_sex are from the model_rpart_age_sex model with both ‚Äòage‚Äô and ‚Äòsex‚Äô as the explanatory variables. The first-row value in the 10Mile_age column indicates that the model predicts a nearly 88% chance for that particular participant to be choosing the 10 Mile marathon. And with no surprise, you can see that the probability values across the models are the same (because we found that the error values are the same earlier).\nWe can condense these probability values to a single value which is called the ‚Äòlikelihood value‚Äô, which can then be used as a figure of merit to compare the models. This is similar to the mean of the square of the prediction errors (m.s.e) we had when the response variable was numerical. The likelihood values are calculated by multiplying the individual probability values. But since the probability values are in decimal values and are less than 1, multiplying them will lead to a very small value which would be difficult to compare. Therefore we first log transform our probability values and add them, which is mathematically equivalent to multiplying them before the log transformation\nSince the dataset is not ‚Äòtidy‚Äô, some codes are used to tidy it. The ‚Äú10 Mile‚Äù has a space in-between. While doing analysis, R might register ‚Äú10 Mile‚Äù as ‚Äú10‚Äù and ‚ÄúMile‚Äù. So this needs to be reformatted.\n\n# Splitting the data frame into two other data frames\n# Newly made data frames have values corresponding to the respective model\ncompare_values_age &lt;- compare_values[,1:4]\ncompare_values_age_sex &lt;- compare_values[,c(1,2,5,6)]\n\n# Tidying the data\ncompare_values_age$event &lt;- recode(compare_values_age$event,\n                                   \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_km\")\ncompare_values_age_sex$event &lt;- recode(compare_values_age_sex$event,\n                                       \"10 Mile\" = \"Ten_Mile\", \"5K\" = \"Five_km\")\n\n# Calculating the likelihood values\n# Ten_Mile_age and Five_km_age are column names\nlikelihood_age &lt;- with(compare_values_age,\n                       ifelse(event == \"Ten_Mile\", Ten_Mile_age, Five_km_age))\nlikelihood_age_sex &lt;- with(compare_values_age_sex,\n                           ifelse(event == \"Ten_Mile\", Ten_Mile_age_sex, Five_km_age_sex))\n\n# Likelihood value of model with age\nsum(log(likelihood_age)) \n\n[1] -3785.999\n\n# Likelihood value of model with age + sex\nsum(log(likelihood_age_sex)) \n\n[1] -3785.999\n\n\nAs the likelihood values are one and the same we can conclude that the variable ‚Äòsex‚Äô does not improve the model.\n\n4.6 Creating a null model\nThe null model gives the best estimate of our data when there are no explanatory variables modelled to it. The predicted value from the null model will always be a constant, no matter what testing data is provided. Let us see how to make a null model.\nWe will again use the run17 dataset from the cherryblossom package in R. To this dataset we add a new column which contains a constant value. We will make a model where net_sec is the response variable and the constant variable is the explanatory variable. We will use the recursive partitioning architecture to model the data.\n\nlibrary(cherryblossom)\nlibrary(rpart)\n\n# Creating a constant variable\nrun17$constant &lt;- 10\n\n# Creating a null model\nnull_model &lt;- rpart(net_sec ~ constant, data = run17)\n\n# Predicting values\n# Notice how all the values are the same\npredict_null &lt;- predict(null_model, newdata = run17)\n\n# Prinitng a few predicted values\n# Notice how all the predicted values are the same\nhead(predict_null)\n\n[1] 5427.947 5427.947 5427.947 5427.947 5427.947 5427.947\n\n\nWe can calculate the mean of the square of the prediction errors of the null model. The null model essentially acts as a base of our model analysis, where we can compare the errors in the null model to the model of our interest.\n\nlibrary(cherryblossom)\nlibrary(rpart)\n\n# Creating a constant variable\nrun17$constant &lt;- 10\n\n# Creating training set\nset.seed(12)\nrun17$training_set &lt;- rnorm(nrow(run17)) &gt; 0\n\n# Creating a null model using the training set\nnull_model &lt;- rpart(net_sec ~ constant, data = subset(run17, training_set))\n\n# Predicting values using the testing set\npredict_null &lt;- predict(null_model,\n                       newdata = subset(run17, !training_set))\n\n# Prinitng a few predicted values\nhead(predict_null)\n\n[1] 5436.2 5436.2 5436.2 5436.2 5436.2 5436.2\n\n# Calculating error values using testing set\nerror_null &lt;- with(subset(run17, !training_set), net_sec - predict_null)\n\n# Calculate the mean of the square of the prediction errors (m.s.e)\nmean(error_null ^ 2, na.rm = T)\n\n[1] 2273954\n\n# Calculating m.s.e by iterating the model 100 times\ntrials &lt;- cv_pred_error(null_model)\n\n# Printing m.s.e values for each iteration\nhead(trials$mse)\n\n[1] 2289640 2289445 2289692 2289697 2289802"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#exploratory-modelling",
    "href": "tutorials/stat_model/intro_stat_model.html#exploratory-modelling",
    "title": "Introduction to statistical modelling in R",
    "section": "\n5 Exploratory modelling",
    "text": "5 Exploratory modelling\nTill now, we had a clear idea of the selection of the explanatory variables while making a model. But sometimes, you might just want to explore your dataset and see which variables affect your response variable without prior knowledge of it. Therefore, we can do exploratory modelling where we take a whole bunch of explanatory variables and see if any of them explain the changes seen in the response variable.\nFor this exercise, we will be using the NHANES dataset from the {NHANES} package in R. The dataset is a survey data collected by the US National Centre for Health Statistics (NCHS). A total of 75 variables concerning health are collected as data from around 7800 individuals in the US.\n\n5.1 Evaluating a recursive partitioning model\nFor our exploratory modelling, we will be using the recursive partitioning architecture. Using the dataset we will see what variables are related to depression.\nWe will use the formula Depressed ~ .\nThe single period on the right side of the Tilda indicates that we want to model using all the possible variables in the dataset. Finally, we will plot the model as a ‚Äòtree‚Äô using the rpart.plot() function from the rpart.plot package in R.\n\nif (!require(NHANES)) install.packages('NHANES')\nif (!require(rpart.plot)) install.packages('rpart.plot')\nlibrary(NHANES)\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Building the recursive partitioning model\nmodel_rpart &lt;- rpart(Depressed ~ . , data = NHANES)\n\n# Plotting the model with sample sizes\nrpart.plot(model_rpart, extra = 1, type = 4)\n\n\n\n\nWe got this nice-looking tree plot with lots of information. Let‚Äôs see how to interpret them.\nThe response variable that we used was the Depressed variable in the dataset. This variable has three levels.\n\nlevels(NHANES$Depressed)\n\n[1] \"None\"    \"Several\" \"Most\"   \n\n\nThe meaning of each of these levels is; ‚ÄúNone‚Äù = No sign of depression, ‚ÄúSeveral‚Äù = Individual was depressed for less than half of the survey period days, ‚ÄúMost‚Äù = Individual was depressed more than half of the days.\nThe colour code in the plot corresponds to the levels of the response variable. In the beginning node of the tree plot, you can see the label ‚ÄúNone‚Äù with three sets of numbers. The numbers correspond to the respective sample numbers of the levels, i.e.¬†at this node, 5246 individuals belong to ‚ÄúNone‚Äù, 1009 individuals belong to ‚ÄúSeveral‚Äù and 418 individuals belong to ‚ÄúMost‚Äù. Therefore we have data for a total of 6673 individuals (5256 + 1009 + 418 = 6673). Also, in this node, the majority of individuals belong to the level ‚ÄúNone‚Äù. Therefore the node is coloured by the respective colour code for ‚ÄúNone‚Äù, which is orange colour here. You can also see that the node colour changes its brightness to correspond to the difference between the majority value and the other values.\nWorking with numbers can be tricky, so let us represent the sample sizes in percentiles for easy comparisons.\n\n# Plotting the model with percentile values\nrpart.plot(model_rpart, extra = \"auto\", type = 4)\n\n\n\n\nNow instead of the actual sample sizes, we have percentile values. Now let us look at the plot. The beginning node which contains the whole set of depressed individuals in the dataset is further split into two groups and this split is caused by the variable LittleInterest. The variable LittleInterest in the dataset denotes the self-reported number of days where the participant had little interest in doing things. And like the Depressed variable, we have three levels for the variable LittleInterest.\n\nlevels(NHANES$LittleInterest)\n\n[1] \"None\"    \"Several\" \"Most\"   \n\n\nThe meaning of these levels is the same as explained for the Depressed variable.\nThe first group contains 77% of the total depressed individuals and they recorded zero days where they had reduced interest to do things. The rest of the 23% individuals belong to the second group which showed a severe reduction or most severe reduction in interest in doing things. This second group is further split into two by the variable DaysMentHlthBad which denotes the self-reported number of days the participant‚Äôs mental health was not good out of the past 30 days. Here, 13% of people in the second group with 23% of the total set had bad mental days less than 6. The remaining 11% of people (13% + 11% = ~ 23% of the second group) had bad mental days for more than 6 days, which further splits into two by their categories in the LittleInterest variable. There is no splitting of the group after this. This is because the recursive partitioning architecture stops at a point where further subdivisions don‚Äôt lead to a big change in predictive ability.\nSo as a summary, by this exploratory modelling exercise, we were able to determine potential variables that could act as the explanatory variables for a given response variable that we are interested in. Overall this led us to understand the relationships between the variables in the dataset that otherwise would not have been possible with a simple linear model. But also keep in mind that these potential variables are not indicating a cause and effect, but rather a simple relationship."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#covariate",
    "href": "tutorials/stat_model/intro_stat_model.html#covariate",
    "title": "Introduction to statistical modelling in R",
    "section": "\n6 Covariate",
    "text": "6 Covariate\nWe briefly learned what are covariates when we were looking at the run17 dataset from the cherryblossom package in R. In that exercise, we looked at whether the participant‚Äôs choice in choosing between the events is affected by their age and sex. We hypothesized that older participants of both sexes would prefer a 5 km run over the 10 Mile marathon. Therefore we considered age as the main explanatory variable and sex as a covariate.\nCovariates are those variables which are of no immediate interest to the user but hold the potential to explain changes seen in the response variable. There is no special distinction between a covariate and an explanatory variable in the formula syntax of the function R code. It‚Äôs just a mental label given by the user.\nLet us use the Tadpoles dataset from the {mosaicModel} package in R. The dataset contains the swimming speed of tadpoles as a function of the water temperature and the water temperature at which the tadpoles had been raised. Since size is a major determinant of speed, the tadpole‚Äôs length was measured as well. It was hypothesized that tadpoles would swim faster in the temperature of water close to that in which they had been raised.\nAs an exercise, let us see if the maximum swimming speed achieved by the tadpole is affected by the temperature at which they were raised. In addition, let also add the size of the tadpole as a covariate, as size could also affect the swimming speeds. The variable vmax denotes the maximum swimming speed (mm/sec) and therefore will be our response variable. The variable group denotes whether the tadpoles were raised in a cold environment (‚Äúc‚Äù) or warm environment (‚Äúw‚Äù) and length denotes the length of the tadpole (mm). Here both group and length will be our explanatory variables where length is also our covariate.\nWhile predicting values using the model, we can keep the covariate unchanged or constant, and predict different values for the other variable.\n\nif (!require(mosaicModel)) install.packages('mosaicModel')\nlibrary(mosaicModel)\n\ndata(Tadpoles)\n\n# Building a linear model\nmodel_vmax &lt;- lm(vmax ~ group + length, data = Tadpoles)\n\n# Predicting for vmax\npredict_vmax &lt;- predict(model_vmax, newdata = Tadpoles)\nhead(predict_vmax)\n\n       1        2        3        4        5        6 \n27.08018 27.11388 25.32803 27.31605 27.51822 26.57476 \n\n# Keeping length constant, predicting for vmax\npredict(model_vmax, newdata = data.frame(group = \"c\" ,length = 5))\n\n      1 \n26.2378 \n\npredict(model_vmax, newdata = data.frame(group = \"w\" ,length = 5))\n\n       1 \n25.25733 \n\n\nSo while keeping length constant, our model predicts change in swimming speeds between cold and warm environments the tadpoles were raised in."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#effect-sizes",
    "href": "tutorials/stat_model/intro_stat_model.html#effect-sizes",
    "title": "Introduction to statistical modelling in R",
    "section": "\n7 Effect sizes",
    "text": "7 Effect sizes\nEffect sizes are a great way to estimate the change in the output value, given the change in the input. Thus they are a great tool to analyse covariates in the model.\nWhile calculating the effect size, the units of it depend on both the response variable and the explanatory variable for which the effect size is measured.\nIf we have a response variable called ‚Äòsalary‚Äô (Dollars) and we have a numerical explanatory variable called the ‚Äòyears of education‚Äô (years), then the effect size of ‚Äòyears of education on the ‚Äôsalary‚Äô would be in the units of dollars/year. Suppose we get an effect size of 100 dollars/year, this means that for every unit increase in years of education, the salary will increase by 100 dollars.\nHere the effect size is calculated as a rate of change and its unit will be;\n\\begin{align*}\nUnit\\,of\\,effect\\,size = \\frac{Unit\\,of\\,response\\,variable}{Unit\\,of\\,explaratory\\,variable}\n\\end{align*}\nBut if our explanatory variable is categorical, then the effect size is calculated as a difference. As categorical values have no units, the units of effect sizes here would be the same as that of the response variable.\nSuppose we have a response variable called ‚Äòrent‚Äô (Dollars) and we have a categorical explanatory variable called ‚Äòcity‚Äô. Let us say the variable ‚Äòcity‚Äô has two levels; Chicago and New York. The meaning of effect size values in this context would be the numerical difference in the response variable when input is changed from one category to another. If our effect size in moving from Chicago to New York is 500 Dollars, then this means that the rent increases by 500 when we move from Chicago to New York.\nHere the calculated effect size is will have the same unit as the response variable\nLook at the following exercise, we will use the Used_Fords dataset in the {mosaicModel} package in R. The dataset contains details about used Ford cars. We will see if the price of the cars (Price) has any relationship with both, how old the car is (Age) and the colour of the car (Color). We will build two linear models using Price as the response variable. But one model will have Age and the other will have Color as the explanatory variable. The units are; Price = USD and Age = years\nWe will calculate the effect sizes using the effect_size() function from the {statisticalModeling } package. The effect_size() function takes in two arguments; the model and a formula indicating which variable to vary when looking at the model output. The effect size to Age will be represented in dollars/year and the effect size to Colour will be represented as a difference in dollars when changing from one colour to another.\n\nlibrary(mosaicModel)\nlibrary(statisticalModeling)\n\ndata(Used_Fords)\n\n# Building a linear model with only age\nmodel_car_age &lt;- lm(Price ~  Age, data = Used_Fords)\n\n# Calculating the effect sizes by varying age\neffect_size(model_car_age, ~ Age)\n\n\n\n  \n\n\n\nEffect size to Age is represented in the slope column. The value is -1124, which means for every unit increase in the age of the car (1-year increase), the price of the car depreciates by 1124 dollars.\n\n# Building a linear model with only colour\nmodel_car_age &lt;- lm(Price ~  Color, data = Used_Fords)\n\n# Calculating the effect sizes by varying colour\neffect_size(model_car_age, ~ Color, Color = \"blue\", to = \"red\")\n\n\n\n  \n\n\n\nFor categorical explanatory variables, the effect_size() function automatically takes appropriate levels. However, we can manually change this behaviour. From the given code, the effect size to colour is calculated when the colour of the car changes from blue to red. Here the effect size is represented in the change column. The value is -3290. This means that the price of the car reduces by 3290 dollars when the colour of the car changes from blue to red."
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#conclusion",
    "href": "tutorials/stat_model/intro_stat_model.html#conclusion",
    "title": "Introduction to statistical modelling in R",
    "section": "\n8 Conclusion",
    "text": "8 Conclusion\nI hope this article provided you with a good learning experience. In a nutshell, we learned;\n\n\nWhat is the meaning of a statistical model\n\nModel functions: lm(), glm() and rpart()\n\nModel formula: response ~ formula\n\nModel architectures: linear, logistic, recursive partitioning\n\n\n\nHow to build a model\n\nBuilding models using functions, formulas and model architectures\nPlotting the model output\n\n\n\nHow to evaluate a model\n\nUsing the model to predict values outside the data\nCalculating the mean of the square of the prediction errors\nUsing error to compare models to aid explanatory variable selection\nCross-validation technique and model iteration by cv_pred_error() function\nPrediction error for a categorical response variable\n\n\nHow to build a null model\n\nHow to do exploratory modelling\n\nEvaluation of a recursive partitioning model and plotting it\n\n\nWhat is a covariate\n\nHow to calculate effect size\n\nFor numerical explanatory variables\nFor categorical explanatory variables\n\n\n\nTake a break and practice the concepts learned from this tutorial. I will see you in the next tutorial where we will learn about ‚ÄòIntermediate statistical modelling using R‚Äô. See you there üëç"
  },
  {
    "objectID": "tutorials/stat_model/intro_stat_model.html#footnotes",
    "href": "tutorials/stat_model/intro_stat_model.html#footnotes",
    "title": "Introduction to statistical modelling in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nWhich is almost impossible to create under real-life conditions‚Ü©Ô∏é"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorial posts",
    "section": "",
    "text": "ggpubr\n\n\ndata visualization\n\n\n\nLearn how to make publication ready plots and visualize results of statistical tests directly on the plot using the ggpubr package\n\n\n\nJewel Johnson\n\n\nJan 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to customize the theme and the colour palette of a graph in ggplot2\n\n\n\nJewel Johnson\n\n\nDec 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to customize the aesthetics, labels and axes of a graph in ggplot2\n\n\n\nJewel Johnson\n\n\nDec 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to plot different types of graphs using the ggplot2 package\n\n\n\nJewel Johnson\n\n\nDec 2, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMaster the art of data visualization using the different packages in R"
  },
  {
    "objectID": "tutorials.html#data-visualization-using-r",
    "href": "tutorials.html#data-visualization-using-r",
    "title": "Tutorial posts",
    "section": "",
    "text": "ggpubr\n\n\ndata visualization\n\n\n\nLearn how to make publication ready plots and visualize results of statistical tests directly on the plot using the ggpubr package\n\n\n\nJewel Johnson\n\n\nJan 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to customize the theme and the colour palette of a graph in ggplot2\n\n\n\nJewel Johnson\n\n\nDec 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to customize the aesthetics, labels and axes of a graph in ggplot2\n\n\n\nJewel Johnson\n\n\nDec 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\n\ndata visualization\n\n\n\nLearn how to plot different types of graphs using the ggplot2 package\n\n\n\nJewel Johnson\n\n\nDec 2, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMaster the art of data visualization using the different packages in R"
  },
  {
    "objectID": "tutorials.html#data-manipulation-using-r",
    "href": "tutorials.html#data-manipulation-using-r",
    "title": "Tutorial posts",
    "section": "2 Data manipulation using R",
    "text": "2 Data manipulation using R\n\n\n\n\n\n\n\n\n\n\nChapter 2: Data manipulation using dplyr (part 1)\n\n\n\ndplyr\n\n\ndata wrangling\n\n\n\nLearn how to manipulate your data with the dplyr package\n\n\n\nJewel Johnson\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3: Data manipulation using dplyr (part 2)\n\n\n\ndplyr\n\n\ndata wrangling\n\n\n\nLearn how to manipulate your data with the dplyr package\n\n\n\nJewel Johnson\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1: Data tidying using tidyr\n\n\n\ntidyr\n\n\ndata wrangling\n\n\n\nLearn how to make your data tidy with the tidyr package\n\n\n\nJewel Johnson\n\n\nDec 11, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n\nTidy your data using powerful cleaning tools provided by the {tidyr} and {dplyr} packages in R"
  },
  {
    "objectID": "tutorials.html#introductory-statistics-using-r",
    "href": "tutorials.html#introductory-statistics-using-r",
    "title": "Tutorial posts",
    "section": "3 Introductory statistics using R",
    "text": "3 Introductory statistics using R\n\n\n\n\n\n\n\n\n\n\nIntermediate Regression in R\n\n\n\nregression\n\n\n\nLevel up your regression knowledge using multiple explanatory variables\n\n\n\nJewel Johnson\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Regression in R\n\n\n\nregression\n\n\n\nLearn the basics of regression in R\n\n\n\nJewel Johnson\n\n\nSep 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroductory statistics with R\n\n\n\nbasic statistics\n\n\n\nLearn the basics of statistics using R\n\n\n\nJewel Johnson\n\n\nAug 31, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\nBe confident in your foundations of statistics"
  },
  {
    "objectID": "tutorials.html#statistical-modelling-using-r",
    "href": "tutorials.html#statistical-modelling-using-r",
    "title": "Tutorial posts",
    "section": "4 Statistical modelling using R",
    "text": "4 Statistical modelling using R\n\n\n\n\n\n\n\n\n\n\nHierarchical and Mixed Effects Models in R\n\n\n\nstatistical modelling\n\n\n\nGo beyond linear models and extend your skills to analyse non-normal datasets with random effects\n\n\n\nJewel Johnson\n\n\nAug 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized linear models in R\n\n\n\nstatistical modelling\n\n\n\nGo beyond linear models and extend them to analyse non-normal datasets\n\n\n\nJewel Johnson\n\n\nAug 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntermediate statistical modelling in R\n\n\n\nstatistical modelling\n\n\n\nLearn about interaction terms, total and partial change, R-squared values, bootstrapping and collinearity\n\n\n\nJewel Johnson\n\n\nAug 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to statistical modelling in R\n\n\n\nstatistical modelling\n\n\n\nLearn how to build a model, predict values using it, evaluating it and plotting the model output in R\n\n\n\nJewel Johnson\n\n\nAug 4, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\nEquip yourselves with powerful modelling tools to analyse data in a more creative and complex way."
  },
  {
    "objectID": "tutorials.html#network-analysis",
    "href": "tutorials.html#network-analysis",
    "title": "Tutorial posts",
    "section": "5 Network analysis",
    "text": "5 Network analysis"
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html",
    "href": "docs/site_libs/d3v5-5.9.2/API.html",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "D3 is a collection of modules that are designed to work together; you can use the modules independently, or you can use them together as part of the default build. The source and documentation for each module is available in its repository. Follow the links below to learn more. For changes between major versions, see CHANGES; see also the release notes and the 3.x reference.\n\nArrays (Statistics, Search, Transformations, Histograms)\nAxes\nBrushes\nChords\nCollections (Objects, Maps, Sets, Nests)\nColors\nColor Schemes\nContours\nDispatches\nDragging\nDelimiter-Separated Values\nEasings\nFetches\nForces\nNumber Formats\nGeographies (Paths, Projections, Spherical Math, Spherical Shapes, Streams, Transforms)\nHierarchies\nInterpolators\nPaths\nPolygons\nQuadtrees\nRandom Numbers\nScales (Continuous, Sequential, Diverging, Quantize, Ordinal)\nSelections (Selecting, Modifying, Data, Events, Control, Local Variables, Namespaces)\nShapes (Arcs, Pies, Lines, Areas, Curves, Links, Symbols, Stacks)\nTime Formats\nTime Intervals\nTimers\nTransitions\nVoronoi Diagrams\nZooming\n\nD3 uses semantic versioning. The current version is exposed as d3.version.\n\n\nArray manipulation, ordering, searching, summarizing, etc.\n\n\nMethods for computing basic summary statistics.\n\nd3.min - compute the minimum value in an array.\nd3.max - compute the maximum value in an array.\nd3.extent - compute the minimum and maximum value in an array.\nd3.sum - compute the sum of an array of numbers.\nd3.mean - compute the arithmetic mean of an array of numbers.\nd3.median - compute the median of an array of numbers (the 0.5-quantile).\nd3.quantile - compute a quantile for a sorted array of numbers.\nd3.variance - compute the variance of an array of numbers.\nd3.deviation - compute the standard deviation of an array of numbers.\n\n\n\n\nMethods for searching arrays for a specific element.\n\nd3.scan - linear search for an element using a comparator.\nd3.bisect - binary search for a value in a sorted array.\nd3.bisectRight - binary search for a value in a sorted array.\nd3.bisectLeft - binary search for a value in a sorted array.\nd3.bisector - bisect using an accessor or comparator.\nbisector.left - bisectLeft, with the given comparator.\nbisector.right - bisectRight, with the given comparator.\nd3.ascending - compute the natural order of two values.\nd3.descending - compute the natural order of two values.\n\n\n\n\nMethods for transforming arrays and for generating new arrays.\n\nd3.cross - compute the Cartesian product of two arrays.\nd3.merge - merge multiple arrays into one array.\nd3.pairs - create an array of adjacent pairs of elements.\nd3.permute - reorder an array of elements according to an array of indexes.\nd3.shuffle - randomize the order of an array.\nd3.ticks - generate representative values from a numeric interval.\nd3.tickIncrement - generate representative values from a numeric interval.\nd3.tickStep - generate representative values from a numeric interval.\nd3.range - generate a range of numeric values.\nd3.transpose - transpose an array of arrays.\nd3.zip - transpose a variable number of arrays.\n\n\n\n\nBin discrete samples into continuous, non-overlapping intervals.\n\nd3.histogram - create a new histogram generator.\nhistogram - compute the histogram for the given array of samples.\nhistogram.value - specify a value accessor for each sample.\nhistogram.domain - specify the interval of observable values.\nhistogram.thresholds - specify how values are divided into bins.\nd3.thresholdFreedmanDiaconis - the Freedman‚ÄìDiaconis binning rule.\nd3.thresholdScott - Scott‚Äôs normal reference binning rule.\nd3.thresholdSturges - Sturges‚Äô binning formula.\n\n\n\n\n\nHuman-readable reference marks for scales.\n\nd3.axisTop - create a new top-oriented axis generator.\nd3.axisRight - create a new right-oriented axis generator.\nd3.axisBottom - create a new bottom-oriented axis generator.\nd3.axisLeft - create a new left-oriented axis generator.\naxis - generate an axis for the given selection.\naxis.scale - set the scale.\naxis.ticks - customize how ticks are generated and formatted.\naxis.tickArguments - customize how ticks are generated and formatted.\naxis.tickValues - set the tick values explicitly.\naxis.tickFormat - set the tick format explicitly.\naxis.tickSize - set the size of the ticks.\naxis.tickSizeInner - set the size of inner ticks.\naxis.tickSizeOuter - set the size of outer (extent) ticks.\naxis.tickPadding - set the padding between ticks and labels.\n\n\n\n\nSelect a one- or two-dimensional region using the mouse or touch.\n\nd3.brush - create a new two-dimensional brush.\nd3.brushX - create a brush along the x-dimension.\nd3.brushY - create a brush along the y-dimension.\nbrush - apply the brush to a selection.\nbrush.move - move the brush selection.\nbrush.extent - define the brushable region.\nbrush.filter - control which input events initiate brushing.\nbrush.handleSize - set the size of the brush handles.\nbrush.on - listen for brush events.\nd3.brushSelection - get the brush selection for a given node.\n\n\n\n\n\nd3.chord - create a new chord layout.\nchord - compute the layout for the given matrix.\nchord.padAngle - set the padding between adjacent groups.\nchord.sortGroups - define the group order.\nchord.sortSubgroups - define the source and target order within groups.\nchord.sortChords - define the chord order across groups.\nd3.ribbon - create a ribbon shape generator.\nribbon - generate a ribbon shape.\nribbon.source - set the source accessor.\nribbon.target - set the target accessor.\nribbon.radius - set the ribbon source or target radius.\nribbon.startAngle - set the ribbon source or target start angle.\nribbon.endAngle - set the ribbon source or target end angle.\nribbon.context - set the render context.\n\n\n\n\nHandy data structures for elements keyed by string.\n\n\nMethods for converting associative arrays (objects) to arrays.\n\nd3.keys - list the keys of an associative array.\nd3.values - list the values of an associated array.\nd3.entries - list the key-value entries of an associative array.\n\n\n\n\nLike ES6 Map, but with string keys and a few other differences.\n\nd3.map - create a new, empty map.\nmap.has - returns true if the map contains the given key.\nmap.get - get the value for the given key.\nmap.set - set the value for the given key.\nmap.remove - remove the entry for given key.\nmap.clear - remove all entries.\nmap.keys - get the array of keys.\nmap.values - get the array of values.\nmap.entries - get the array of entries (key-values objects).\nmap.each - call a function for each entry.\nmap.empty - returns false if the map has at least one entry.\nmap.size - compute the number of entries.\n\n\n\n\nLike ES6 Set, but with string keys and a few other differences.\n\nd3.set - create a new, empty set.\nset.has - returns true if the set contains the given value.\nset.add - add the given value.\nset.remove - remove the given value.\nset.clear - remove all values.\nset.values - get the array of values.\nset.each - call a function for each value.\nset.empty - returns true if the set has at least one value.\nset.size - compute the number of values.\n\n\n\n\nGroup data into arbitrary hierarchies.\n\nd3.nest - create a new nest generator.\nnest.key - add a level to the nest hierarchy.\nnest.sortKeys - sort the current nest level by key.\nnest.sortValues - sort the leaf nest level by value.\nnest.rollup - specify a rollup function for leaf values.\nnest.map - generate the nest, returning a map.\nnest.object - generate the nest, returning an associative array.\nnest.entries - generate the nest, returning an array of key-values tuples.\n\n\n\n\n\nColor manipulation and color space conversion.\n\nd3.color - parse the given CSS color specifier.\ncolor.rgb - compute the RGB equivalent of this color.\ncolor.brighter - create a brighter copy of this color.\ncolor.darker - create a darker copy of this color.\ncolor.displayable - returns true if the color is displayable on standard hardware.\ncolor.hex - returns the hexadecimal RGB string representation of this color.\ncolor.toString - returns the RGB string representation of this color.\nd3.rgb - create a new RGB color.\nd3.hsl - create a new HSL color.\nd3.lab - create a new Lab color.\nd3.hcl - create a new HCL color.\nd3.lch - create a new HCL color.\nd3.gray - create a new Lab gray.\nd3.cubehelix - create a new Cubehelix color.\n\n\n\n\nColor ramps and palettes for quantitative, ordinal and categorical scales.\n\n\n\nd3.schemeCategory10 -\nd3.schemeAccent -\nd3.schemeDark2 -\nd3.schemePaired -\nd3.schemePastel1 -\nd3.schemePastel2 -\nd3.schemeSet1 -\nd3.schemeSet2 -\nd3.schemeSet3 -\n\n\n\n\n\nd3.interpolateBrBG -\nd3.interpolatePiYG -\nd3.interpolatePRGn -\nd3.interpolatePuOr -\nd3.interpolateRdBu -\nd3.interpolateRdGy -\nd3.interpolateRdYlBu -\nd3.interpolateRdYlGn -\nd3.interpolateSpectral -\nd3.schemeBrBG -\nd3.schemePiYG -\nd3.schemePRGn -\nd3.schemePuOr -\nd3.schemeRdBu -\nd3.schemeRdGy -\nd3.schemeRdYlBu -\nd3.schemeRdYlGn -\nd3.schemeSpectral -\n\n\n\n\n\nd3.interpolateBlues -\nd3.interpolateGreens -\nd3.interpolateGreys -\nd3.interpolateOranges -\nd3.interpolatePurples -\nd3.interpolateReds -\nd3.schemeBlues -\nd3.schemeGreens -\nd3.schemeGreys -\nd3.schemeOranges -\nd3.schemePurples -\nd3.schemeReds -\n\n\n\n\n\nd3.interpolateBuGn -\nd3.interpolateBuPu -\nd3.interpolateCool -\nd3.interpolateCubehelixDefault -\nd3.interpolateGnBu -\nd3.interpolateInferno -\nd3.interpolateMagma -\nd3.interpolateOrRd -\nd3.interpolatePlasma -\nd3.interpolatePuBu -\nd3.interpolatePuBuGn -\nd3.interpolatePuRd -\nd3.interpolateRdPu -\nd3.interpolateViridis -\nd3.interpolateWarm -\nd3.interpolateYlGn -\nd3.interpolateYlGnBu -\nd3.interpolateYlOrBr -\nd3.interpolateYlOrRd -\nd3.schemeBuGn -\nd3.schemeBuPu -\nd3.schemeGnBu -\nd3.schemeOrRd -\nd3.schemePuBu -\nd3.schemePuBuGn -\nd3.schemePuRd -\nd3.schemeRdPu -\nd3.schemeYlGn -\nd3.schemeYlGnBu -\nd3.schemeYlOrBr -\nd3.schemeYlOrRd -\n\n\n\n\n\nd3.interpolateRainbow - the ‚Äúless-angry‚Äù rainbow\nd3.interpolateSinebow - the ‚Äúsinebow‚Äù smooth rainbow\n\n\n\n\n\nCompute contour polygons using marching squares.\n\nd3.contours - create a new contour generator.\ncontours - compute the contours for a given grid of values.\ncontours.contour -\ncontours.size -\ncontours.smooth -\ncontours.thresholds -\nd3.contourDensity - create a new density estimator.\ndensity - estimate the density of a given array of samples.\ndensity.x -\ndensity.y -\ndensity.size -\ndensity.cellSize -\ndensity.thresholds -\ndensity.bandwidth -\ndensity.weight -\n\n\n\n\nSeparate concerns using named callbacks.\n\nd3.dispatch - create a custom event dispatcher.\ndispatch.on - register or unregister an event listener.\ndispatch.copy - create a copy of a dispatcher.\ndispatch.call - dispatch an event to registered listeners.\ndispatch.apply - dispatch an event to registered listeners.\n\n\n\n\nDrag and drop SVG, HTML or Canvas using mouse or touch input.\n\nd3.drag - create a drag behavior.\ndrag - apply the drag behavior to a selection.\ndrag.container - set the coordinate system.\ndrag.filter - ignore some initiating input events.\ndrag.touchable - set the touch support detector.\ndrag.subject - set the thing being dragged.\ndrag.clickDistance - set the click distance threshold.\ndrag.on - listen for drag events.\nevent.on - listen for drag events on the current gesture.\nd3.dragDisable - prevent native drag-and-drop and text selection.\nd3.dragEnable - enable native drag-and-drop and text selection.\n\n\n\n\nParse and format delimiter-separated values, most commonly CSV and TSV.\n\nd3.dsvFormat - create a new parser and formatter for the given delimiter.\ndsv.parse - parse the given string, returning an array of objects.\ndsv.parseRows - parse the given string, returning an array of rows.\ndsv.format - format the given array of objects.\ndsv.formatBody - format the given array of objects.\ndsv.formatRows - format the given array of rows.\nd3.autoType - automatically infer value types for the given object.\nd3.csvParse - parse the given CSV string, returning an array of objects.\nd3.csvParseRows - parse the given CSV string, returning an array of rows.\nd3.csvFormat - format the given array of objects as CSV.\nd3.csvFormatBody - format the given array of objects as CSV.\nd3.csvFormatRows - format the given array of rows as CSV.\nd3.tsvParse - parse the given TSV string, returning an array of objects.\nd3.tsvParseRows - parse the given TSV string, returning an array of rows.\nd3.tsvFormat - format the given array of objects as TSV.\nd3.tsvFormatBody - format the given array of objects as TSV.\nd3.tsvFormatRows - format the given array of rows as TSV.\n\n\n\n\nEasing functions for smooth animation.\n\nease - ease the given normalized time.\nd3.easeLinear - linear easing; the identity function.\nd3.easePolyIn - polynomial easing; raises time to the given power.\nd3.easePolyOut - reverse polynomial easing.\nd3.easePolyInOut - symmetric polynomial easing.\npoly.exponent - specify the polynomial exponent.\nd3.easeQuad - an alias for easeQuadInOut.\nd3.easeQuadIn - quadratic easing; squares time.\nd3.easeQuadOut - reverse quadratic easing.\nd3.easeQuadInOut - symmetric quadratic easing.\nd3.easeCubic - an alias for easeCubicInOut.\nd3.easeCubicIn - cubic easing; cubes time.\nd3.easeCubicOut - reverse cubic easing.\nd3.easeCubicInOut - symmetric cubic easing.\nd3.easeSin - an alias for easeSinInOut.\nd3.easeSinIn - sinusoidal easing.\nd3.easeSinOut - reverse sinusoidal easing.\nd3.easeSinInOut - symmetric sinusoidal easing.\nd3.easeExp - an alias for easeExpInOut.\nd3.easeExpIn - exponential easing.\nd3.easeExpOut - reverse exponential easing.\nd3.easeExpInOut - symmetric exponential easing.\nd3.easeCircle - an alias for easeCircleInOut.\nd3.easeCircleIn - circular easing.\nd3.easeCircleOut - reverse circular easing.\nd3.easeCircleInOut - symmetric circular easing.\nd3.easeElastic - an alias for easeElasticOut.\nd3.easeElasticIn - elastic easing, like a rubber band.\nd3.easeElasticOut - reverse elastic easing.\nd3.easeElasticInOut - symmetric elastic easing.\nelastic.amplitude - specify the elastic amplitude.\nelastic.period - specify the elastic period.\nd3.easeBack - an alias for easeBackInOut.\nd3.easeBackIn - anticipatory easing, like a dancer bending his knees before jumping.\nd3.easeBackOut - reverse anticipatory easing.\nd3.easeBackInOut - symmetric anticipatory easing.\nback.overshoot - specify the amount of overshoot.\nd3.easeBounce - an alias for easeBounceOut.\nd3.easeBounceIn - bounce easing, like a rubber ball.\nd3.easeBounceOut - reverse bounce easing.\nd3.easeBounceInOut - symmetric bounce easing.\n\n\n\n\nConvenience methods on top of the Fetch API.\n\nd3.blob - get a file as a blob.\nd3.buffer - get a file as an array buffer.\nd3.csv - get a comma-separated values (CSV) file.\nd3.dsv - get a delimiter-separated values (CSV) file.\nd3.image - get an image.\nd3.json - get a JSON file.\nd3.text - get a plain text file.\nd3.tsv - get a tab-separated values (TSV) file.\n\n\n\n\nForce-directed graph layout using velocity Verlet integration.\n\nd3.forceSimulation - create a new force simulation.\nsimulation.restart - reheat and restart the simulation‚Äôs timer.\nsimulation.stop - stop the simulation‚Äôs timer.\nsimulation.tick - advance the simulation one step.\nsimulation.nodes - set the simulation‚Äôs nodes.\nsimulation.alpha - set the current alpha.\nsimulation.alphaMin - set the minimum alpha threshold.\nsimulation.alphaDecay - set the alpha exponential decay rate.\nsimulation.alphaTarget - set the target alpha.\nsimulation.velocityDecay - set the velocity decay rate.\nsimulation.force - add or remove a force.\nsimulation.find - find the closest node to the given position.\nsimulation.on - add or remove an event listener.\nforce - apply the force.\nforce.initialize - initialize the force with the given nodes.\nd3.forceCenter - create a centering force.\ncenter.x - set the center x-coordinate.\ncenter.y - set the center y-coordinate.\nd3.forceCollide - create a circle collision force.\ncollide.radius - set the circle radius.\ncollide.strength - set the collision resolution strength.\ncollide.iterations - set the number of iterations.\nd3.forceLink - create a link force.\nlink.links - set the array of links.\nlink.id - link nodes by numeric index or string identifier.\nlink.distance - set the link distance.\nlink.strength - set the link strength.\nlink.iterations - set the number of iterations.\nd3.forceManyBody - create a many-body force.\nmanyBody.strength - set the force strength.\nmanyBody.theta - set the Barnes‚ÄìHut approximation accuracy.\nmanyBody.distanceMin - limit the force when nodes are close.\nmanyBody.distanceMax - limit the force when nodes are far.\nd3.forceX - create an x-positioning force.\nx.strength - set the force strength.\nx.x - set the target x-coordinate.\nd3.forceY - create an y-positioning force.\ny.strength - set the force strength.\ny.y - set the target y-coordinate.\nd3.forceRadial - create a radial positioning force.\nradial.strength - set the force strength.\nradial.radius - set the target radius.\nradial.x - set the target center x-coordinate.\nradial.y - set the target center y-coordinate.\n\n\n\n\nFormat numbers for human consumption.\n\nd3.format - alias for locale.format on the default locale.\nd3.formatPrefix - alias for locale.formatPrefix on the default locale.\nd3.formatSpecifier - parse a number format specifier.\nd3.formatLocale - define a custom locale.\nd3.formatDefaultLocale - define the default locale.\nlocale.format - create a number format.\nlocale.formatPrefix - create a SI-prefix number format.\nd3.precisionFixed - compute decimal precision for fixed-point notation.\nd3.precisionPrefix - compute decimal precision for SI-prefix notation.\nd3.precisionRound - compute significant digits for rounded notation.\n\n\n\n\nGeographic projections, shapes and math.\n\n\n\nd3.geoPath - create a new geographic path generator.\npath - project and render the specified feature.\npath.area - compute the projected planar area of a given feature.\npath.bounds - compute the projected planar bounding box of a given feature.\npath.centroid - compute the projected planar centroid of a given feature.\npath.measure - compute the projected planar length of a given feature.\npath.projection - set the geographic projection.\npath.context - set the render context.\npath.pointRadius - set the radius to display point features.\n\n\n\n\n\nprojection - project the specified point from the sphere to the plane.\nprojection.invert - unproject the specified point from the plane to the sphere.\nprojection.stream - wrap the specified stream to project geometry.\nprojection.clipAngle - set the radius of the clip circle.\nprojection.clipExtent - set the viewport clip extent, in pixels.\nprojection.angle - set the post-projection rotation.\nprojection.scale - set the scale factor.\nprojection.translate - set the translation offset.\nprojection.fitExtent - set the scale and translate to fit a GeoJSON object.\nprojection.fitSize - set the scale and translate to fit a GeoJSON object.\nprojection.fitWidth - set the scale and translate to fit a GeoJSON object.\nprojection.fitHeight - set the scale and translate to fit a GeoJSON object.\nprojection.center - set the center point.\nprojection.rotate - set the three-axis spherical rotation angles.\nprojection.precision - set the precision threshold for adaptive sampling.\nprojection.preclip - set the spherical clipping stream transform.\nprojection.postclip - set the planar clipping stream transform.\nd3.geoClipAntimeridian - cuts spherical geometries that cross the antimeridian.\nd3.geoClipCircle - clips spherical geometries to a small circle.\nd3.geoClipRectangle - clips planar geometries to a rectangular viewport.\nd3.geoAlbers - the Albers equal-area conic projection.\nd3.geoAlbersUsa - a composite Albers projection for the United States.\nd3.geoAzimuthalEqualArea - the azimuthal equal-area projection.\nd3.geoAzimuthalEquidistant - the azimuthal equidistant projection.\nd3.geoConicConformal - the conic conformal projection.\nd3.geoConicEqualArea - the conic equal-area (Albers) projection.\nd3.geoConicEquidistant - the conic equidistant projection.\nconic.parallels - set the two standard parallels.\nd3.geoEqualEarth - the Equal Earth projection.\nd3.geoEquirectangular - the equirectangular (plate carre√©) projection.\nd3.geoGnomonic - the gnomonic projection.\nd3.geoMercator - the spherical Mercator projection.\nd3.geoOrthographic - the azimuthal orthographic projection.\nd3.geoStereographic - the azimuthal stereographic projection.\nd3.geoTransverseMercator - the transverse spherical Mercator projection.\nproject - project the specified point from the sphere to the plane.\nproject.invert - unproject the specified point from the plane to the sphere.\nd3.geoProjection - create a custom projection.\nd3.geoProjectionMutator - create a custom configurable projection.\nd3.geoAzimuthalEqualAreaRaw - the raw azimuthal equal-area projection.\nd3.geoAzimuthalEquidistantRaw - the raw azimuthal equidistant projection.\nd3.geoConicConformalRaw - the raw conic conformal projection.\nd3.geoConicEqualAreaRaw - the raw conic equal-area (Albers) projection.\nd3.geoConicEquidistantRaw - the raw conic equidistant projection.\nd3.geoEquirectangularRaw - the raw equirectangular (plate carre√©) projection.\nd3.geoGnomonicRaw - the raw gnomonic projection.\nd3.geoMercatorRaw - the raw Mercator projection.\nd3.geoOrthographicRaw - the raw azimuthal orthographic projection.\nd3.geoStereographicRaw - the raw azimuthal stereographic projection.\nd3.geoTransverseMercatorRaw - the raw transverse spherical Mercator projection.\n\n\n\n\n\nd3.geoArea - compute the spherical area of a given feature.\nd3.geoBounds - compute the latitude-longitude bounding box for a given feature.\nd3.geoCentroid - compute the spherical centroid of a given feature.\nd3.geoContains - test whether a point is inside a given feature.\nd3.geoDistance - compute the great-arc distance between two points.\nd3.geoLength - compute the length of a line string or the perimeter of a polygon.\nd3.geoInterpolate - interpolate between two points along a great arc.\nd3.geoRotation - create a rotation function for the specified angles.\nrotation - rotate the given point around the sphere.\nrotation.invert - unrotate the given point around the sphere.\n\n\n\n\n\nd3.geoCircle - create a circle generator.\ncircle - generate a piecewise circle as a Polygon.\ncircle.center - specify the circle center in latitude and longitude.\ncircle.radius - specify the angular radius in degrees.\ncircle.precision - specify the precision of the piecewise circle.\nd3.geoGraticule - create a graticule generator.\ngraticule - generate a MultiLineString of meridians and parallels.\ngraticule.lines - generate an array of LineStrings of meridians and parallels.\ngraticule.outline - generate a Polygon of the graticule‚Äôs extent.\ngraticule.extent - get or set the major & minor extents.\ngraticule.extentMajor - get or set the major extent.\ngraticule.extentMinor - get or set the minor extent.\ngraticule.step - get or set the major & minor step intervals.\ngraticule.stepMajor - get or set the major step intervals.\ngraticule.stepMinor - get or set the minor step intervals.\ngraticule.precision - get or set the latitudinal precision.\nd3.geoGraticule10 - generate the default 10¬∞ global graticule.\n\n\n\n\n\nd3.geoStream - convert a GeoJSON object to a geometry stream.\nstream.point - indicates a point with the specified coordinates.\nstream.lineStart - indicates the start of a line or ring.\nstream.lineEnd - indicates the end of a line or ring.\nstream.polygonStart - indicates the start of a polygon.\nstream.polygonEnd - indicates the end of a polygon.\nstream.sphere - indicates the sphere.\n\n\n\n\n\nd3.geoIdentity - scale, translate or clip planar geometry.\nidentity.reflectX - reflect the x-dimension.\nidentity.reflectY - reflect the y-dimension.\nd3.geoTransform - define a custom geometry transform.\n\n\n\n\n\nLayout algorithms for visualizing hierarchical data.\n\nd3.hierarchy - constructs a root node from hierarchical data.\nnode.ancestors - generate an array of ancestors.\nnode.descendants - generate an array of descendants.\nnode.leaves - generate an array of leaves.\nnode.path - generate the shortest path to another node.\nnode.links - generate an array of links.\nnode.sum - evaluate and aggregate quantitative values.\nnode.sort - sort all descendant siblings.\nnode.count - count the number of leaves.\nnode.each - breadth-first traversal.\nnode.eachAfter - post-order traversal.\nnode.eachBefore - pre-order traversal.\nnode.copy - copy a hierarchy.\nd3.stratify - create a new stratify operator.\nstratify - construct a root node from tabular data.\nstratify.id - set the node id accessor.\nstratify.parentId - set the parent node id accessor.\nd3.cluster - create a new cluster (dendrogram) layout.\ncluster - layout the specified hierarchy in a dendrogram.\ncluster.size - set the layout size.\ncluster.nodeSize - set the node size.\ncluster.separation - set the separation between leaves.\nd3.tree - create a new tidy tree layout.\ntree - layout the specified hierarchy in a tidy tree.\ntree.size - set the layout size.\ntree.nodeSize - set the node size.\ntree.separation - set the separation between nodes.\nd3.treemap - create a new treemap layout.\ntreemap - layout the specified hierarchy as a treemap.\ntreemap.tile - set the tiling method.\ntreemap.size - set the layout size.\ntreemap.round - set whether the output coordinates are rounded.\ntreemap.padding - set the padding.\ntreemap.paddingInner - set the padding between siblings.\ntreemap.paddingOuter - set the padding between parent and children.\ntreemap.paddingTop - set the padding between the parent‚Äôs top edge and children.\ntreemap.paddingRight - set the padding between the parent‚Äôs right edge and children.\ntreemap.paddingBottom - set the padding between the parent‚Äôs bottom edge and children.\ntreemap.paddingLeft - set the padding between the parent‚Äôs left edge and children.\nd3.treemapBinary - tile using a balanced binary tree.\nd3.treemapDice - tile into a horizontal row.\nd3.treemapSlice - tile into a vertical column.\nd3.treemapSliceDice - alternate between slicing and dicing.\nd3.treemapSquarify - tile using squarified rows per Bruls et. al.\nd3.treemapResquarify - like d3.treemapSquarify, but performs stable updates.\nsquarify.ratio - set the desired rectangle aspect ratio.\nd3.partition - create a new partition (icicle or sunburst) layout.\npartition - layout the specified hierarchy as a partition diagram.\npartition.size - set the layout size.\npartition.round - set whether the output coordinates are rounded.\npartition.padding - set the padding.\nd3.pack - create a new circle-packing layout.\npack - layout the specified hierarchy using circle-packing.\npack.radius - set the radius accessor.\npack.size - set the layout size.\npack.padding - set the padding.\nd3.packSiblings - pack the specified array of circles.\nd3.packEnclose - enclose the specified array of circles.\n\n\n\n\nInterpolate numbers, colors, strings, arrays, objects, whatever!\n\nd3.interpolate - interpolate arbitrary values.\nd3.interpolateArray - interpolate arrays of arbitrary values.\nd3.interpolateDate - interpolate dates.\nd3.interpolateNumber - interpolate numbers.\nd3.interpolateObject - interpolate arbitrary objects.\nd3.interpolateRound - interpolate integers.\nd3.interpolateString - interpolate strings with embedded numbers.\nd3.interpolateTransformCss - interpolate 2D CSS transforms.\nd3.interpolateTransformSvg - interpolate 2D SVG transforms.\nd3.interpolateZoom - zoom and pan between two views.\nd3.interpolateRgb - interpolate RGB colors.\nd3.interpolateRgbBasis - generate a B-spline through a set of colors.\nd3.interpolateRgbBasisClosed - generate a closed B-spline through a set of colors.\nd3.interpolateHsl - interpolate HSL colors.\nd3.interpolateHslLong - interpolate HSL colors, the long way.\nd3.interpolateLab - interpolate Lab colors.\nd3.interpolateHcl - interpolate HCL colors.\nd3.interpolateHclLong - interpolate HCL colors, the long way.\nd3.interpolateCubehelix - interpolate Cubehelix colors.\nd3.interpolateCubehelixLong - interpolate Cubehelix colors, the long way.\ninterpolate.gamma - apply gamma correction during interpolation.\nd3.interpolateHue - interpolate a hue angle.\nd3.interpolateDiscrete - generate a discrete interpolator from a set of values.\nd3.interpolateBasis - generate a B-spline through a set of values.\nd3.interpolateBasisClosed - generate a closed B-spline through a set of values.\nd3.piecewise - generate a piecewise linear interpolator from a set of values.\nd3.quantize - generate uniformly-spaced samples from an interpolator.\n\n\n\n\nSerialize Canvas path commands to SVG.\n\nd3.path - create a new path serializer.\npath.moveTo - move to the given point.\npath.closePath - close the current subpath.\npath.lineTo - draw a straight line segment.\npath.quadraticCurveTo - draw a quadratic B√©zier segment.\npath.bezierCurveTo - draw a cubic B√©zier segment.\npath.arcTo - draw a circular arc segment.\npath.arc - draw a circular arc segment.\npath.rect - draw a rectangle.\npath.toString - serialize to an SVG path data string.\n\n\n\n\nGeometric operations for two-dimensional polygons.\n\nd3.polygonArea - compute the area of the given polygon.\nd3.polygonCentroid - compute the centroid of the given polygon.\nd3.polygonHull - compute the convex hull of the given points.\nd3.polygonContains - test whether a point is inside a polygon.\nd3.polygonLength - compute the length of the given polygon‚Äôs perimeter.\n\n\n\n\nTwo-dimensional recursive spatial subdivision.\n\nd3.quadtree - create a new, empty quadtree.\nquadtree.x - set the x accessor.\nquadtree.y - set the y accessor.\nquadtree.add - add a datum to a quadtree.\nquadtree.addAll - add an array of data to a quadtree.\nquadtree.remove - remove a datum from a quadtree.\nquadtree.removeAll - remove an array of data from a quadtree.\nquadtree.copy - create a copy of a quadtree.\nquadtree.root - get the quadtree‚Äôs root node.\nquadtree.data - retrieve all data from the quadtree.\nquadtree.size - count the number of data in the quadtree.\nquadtree.find - quickly find the closest datum in a quadtree.\nquadtree.visit - selectively visit nodes in a quadtree.\nquadtree.visitAfter - visit all nodes in a quadtree.\nquadtree.cover - extend the quadtree to cover a point.\nquadtree.extent - extend the quadtree to cover an extent.\n\n\n\n\nGenerate random numbers from various distributions.\n\nd3.randomUniform - from a uniform distribution.\nd3.randomNormal - from a normal distribution.\nd3.randomLogNormal - from a log-normal distribution.\nd3.randomBates - from a Bates distribution.\nd3.randomIrwinHall - from an Irwin‚ÄìHall distribution.\nd3.randomExponential - from an exponential distribution.\nrandom.source - set the source of randomness.\n\n\n\n\nEncodings that map abstract data to visual representation.\n\n\nMap a continuous, quantitative domain to a continuous range.\n\ncontinuous - compute the range value corresponding to a given domain value.\ncontinuous.invert - compute the domain value corresponding to a given range value.\ncontinuous.domain - set the input domain.\ncontinuous.range - set the output range.\ncontinuous.rangeRound - set the output range and enable rounding.\ncontinuous.clamp - enable clamping to the domain or range.\ncontinuous.interpolate - set the output interpolator.\ncontinuous.unknown - set the output value for unknown inputs.\ncontinuous.ticks - compute representative values from the domain.\ncontinuous.tickFormat - format ticks for human consumption.\ncontinuous.nice - extend the domain to nice round numbers.\ncontinuous.copy - create a copy of this scale.\nd3.scaleLinear - create a quantitative linear scale.\nd3.scalePow - create a quantitative power scale.\npow - compute the range value corresponding to a given domain value.\npow.invert - compute the domain value corresponding to a given range value.\npow.exponent - set the power exponent.\npow.domain - set the input domain.\npow.range - set the output range.\npow.rangeRound - set the output range and enable rounding.\npow.clamp - enable clamping to the domain or range.\npow.interpolate - set the output interpolator.\npow.ticks - compute representative values from the domain.\npow.tickFormat - format ticks for human consumption.\npow.nice - extend the domain to nice round numbers.\npow.copy - create a copy of this scale.\nd3.scaleSqrt - create a quantitative power scale with exponent 0.5.\nd3.scaleLog - create a quantitative logarithmic scale.\nlog - compute the range value corresponding to a given domain value.\nlog.invert - compute the domain value corresponding to a given range value.\nlog.base - set the logarithm base.\nlog.domain - set the input domain.\nlog.range - set the output range.\nlog.rangeRound - set the output range and enable rounding.\nlog.clamp - enable clamping to the domain or range.\nlog.interpolate - set the output interpolator.\nlog.ticks - compute representative values from the domain.\nlog.tickFormat - format ticks for human consumption.\nlog.nice - extend the domain to nice round numbers.\nlog.copy - create a copy of this scale.\nd3.scaleSymlog - create a symmetric logarithmic scale.\nd3.scaleIdentity - create a quantitative identity scale.\nd3.scaleTime - create a linear scale for time.\ntime - compute the range value corresponding to a given domain value.\ntime.invert - compute the domain value corresponding to a given range value.\ntime.domain - set the input domain.\ntime.range - set the output range.\ntime.rangeRound - set the output range and enable rounding.\ntime.clamp - enable clamping to the domain or range.\ntime.interpolate - set the output interpolator.\ntime.ticks - compute representative values from the domain.\ntime.tickFormat - format ticks for human consumption.\ntime.nice - extend the domain to nice round times.\ntime.copy - create a copy of this scale.\nd3.scaleUtc - create a linear scale for UTC.\nd3.tickFormat - format ticks for human consumption.\n\n\n\n\nMap a continuous, quantitative domain to a continuous, fixed interpolator.\n\nd3.scaleSequential - create a sequential scale.\nsequential.interpolator - set the scale‚Äôs output interpolator.\nd3.scaleSequentialLog -\nd3.scaleSequentialPow -\nd3.scaleSequentialSqrt -\nd3.scaleSequentialSymlog -\nd3.scaleSequentialQuantile -\n\n\n\n\nMap a continuous, quantitative domain to a continuous, fixed interpolator.\n\nd3.scaleDiverging - create a diverging scale.\ndiverging.interpolator - set the scale‚Äôs output interpolator.\nd3.scaleDivergingLog -\nd3.scaleDivergingPow -\nd3.scaleDivergingSqrt -\nd3.scaleDivergingSymlog -\n\n\n\n\nMap a continuous, quantitative domain to a discrete range.\n\nd3.scaleQuantize - create a uniform quantizing linear scale.\nquantize - compute the range value corresponding to a given domain value.\nquantize.invertExtent - compute the domain values corresponding to a given range value.\nquantize.domain - set the input domain.\nquantize.range - set the output range.\nquantize.nice - extend the domain to nice round numbers.\nquantize.ticks - compute representative values from the domain.\nquantize.tickFormat - format ticks for human consumption.\nquantize.copy - create a copy of this scale.\nd3.scaleQuantile - create a quantile quantizing linear scale.\nquantile - compute the range value corresponding to a given domain value.\nquantile.invertExtent - compute the domain values corresponding to a given range value.\nquantile.domain - set the input domain.\nquantile.range - set the output range.\nquantile.quantiles - get the quantile thresholds.\nquantile.copy - create a copy of this scale.\nd3.scaleThreshold - create an arbitrary quantizing linear scale.\nthreshold - compute the range value corresponding to a given domain value.\nthreshold.invertExtent - compute the domain values corresponding to a given range value.\nthreshold.domain - set the input domain.\nthreshold.range - set the output range.\nthreshold.copy - create a copy of this scale.\n\n\n\n\nMap a discrete domain to a discrete range.\n\nd3.scaleOrdinal - create an ordinal scale.\nordinal - compute the range value corresponding to a given domain value.\nordinal.domain - set the input domain.\nordinal.range - set the output range.\nordinal.unknown - set the output value for unknown inputs.\nordinal.copy - create a copy of this scale.\nd3.scaleImplicit - a special unknown value for implicit domains.\nd3.scaleBand - create an ordinal band scale.\nband - compute the band start corresponding to a given domain value.\nband.domain - set the input domain.\nband.range - set the output range.\nband.rangeRound - set the output range and enable rounding.\nband.round - enable rounding.\nband.paddingInner - set padding between bands.\nband.paddingOuter - set padding outside the first and last bands.\nband.padding - set padding outside and between bands.\nband.align - set band alignment, if there is extra space.\nband.bandwidth - get the width of each band.\nband.step - get the distance between the starts of adjacent bands.\nband.copy - create a copy of this scale.\nd3.scalePoint - create an ordinal point scale.\npoint - compute the point corresponding to a given domain value.\npoint.domain - set the input domain.\npoint.range - set the output range.\npoint.rangeRound - set the output range and enable rounding.\npoint.round - enable rounding.\npoint.padding - set padding outside the first and last point.\npoint.align - set point alignment, if there is extra space.\npoint.bandwidth - returns zero.\npoint.step - get the distance between the starts of adjacent points.\npoint.copy - create a copy of this scale.\n\n\n\n\n\nTransform the DOM by selecting elements and joining to data.\n\n\n\nd3.selection - select the root document element.\nd3.select - select an element from the document.\nd3.selectAll - select multiple elements from the document.\nselection.select - select a descendant element for each selected element.\nselection.selectAll - select multiple descendants for each selected element.\nselection.filter - filter elements based on data.\nselection.merge - merge this selection with another.\nd3.matcher - test whether an element matches a selector.\nd3.selector - select an element.\nd3.selectorAll - select elements.\nd3.window - get a node‚Äôs owner window.\nd3.style - get a node‚Äôs current style value.\n\n\n\n\n\nselection.attr - get or set an attribute.\nselection.classed - get, add or remove CSS classes.\nselection.style - get or set a style property.\nselection.property - get or set a (raw) property.\nselection.text - get or set the text content.\nselection.html - get or set the inner HTML.\nselection.append - create, append and select new elements.\nselection.insert - create, insert and select new elements.\nselection.remove - remove elements from the document.\nselection.clone - insert clones of selected elements.\nselection.sort - sort elements in the document based on data.\nselection.order - reorders elements in the document to match the selection.\nselection.raise - reorders each element as the last child of its parent.\nselection.lower - reorders each element as the first child of its parent.\nd3.create - create and select a detached element.\nd3.creator - create an element by name.\n\n\n\n\n\nselection.data - bind elements to data.\nselection.join - enter, update or exit elements based on data.\nselection.enter - get the enter selection (data missing elements).\nselection.exit - get the exit selection (elements missing data).\nselection.datum - get or set element data (without joining).\n\n\n\n\n\nselection.on - add or remove event listeners.\nselection.dispatch - dispatch a custom event.\nd3.event - the current user event, during interaction.\nd3.customEvent - temporarily define a custom event.\nd3.mouse - get the mouse position relative to a given container.\nd3.touch - get a touch position relative to a given container.\nd3.touches - get the touch positions relative to a given container.\nd3.clientPoint - get a position relative to a given container.\n\n\n\n\n\nselection.each - call a function for each element.\nselection.call - call a function with this selection.\nselection.empty - returns true if this selection is empty.\nselection.nodes - returns an array of all selected elements.\nselection.node - returns the first (non-null) element.\nselection.size - returns the count of elements.\n\n\n\n\n\nd3.local - declares a new local variable.\nlocal.set - set a local variable‚Äôs value.\nlocal.get - get a local variable‚Äôs value.\nlocal.remove - delete a local variable.\nlocal.toString - get the property identifier of a local variable.\n\n\n\n\n\nd3.namespace - qualify a prefixed XML name, such as ‚Äúxlink:href‚Äù.\nd3.namespaces - the built-in XML namespaces.\n\n\n\n\n\nGraphical primitives for visualization.\n\n\nCircular or annular sectors, as in a pie or donut chart.\n\nd3.arc - create a new arc generator.\narc - generate an arc for the given datum.\narc.centroid - compute an arc‚Äôs midpoint.\narc.innerRadius - set the inner radius.\narc.outerRadius - set the outer radius.\narc.cornerRadius - set the corner radius, for rounded corners.\narc.startAngle - set the start angle.\narc.endAngle - set the end angle.\narc.padAngle - set the angle between adjacent arcs, for padded arcs.\narc.padRadius - set the radius at which to linearize padding.\narc.context - set the rendering context.\n\n\n\n\nCompute the necessary angles to represent a tabular dataset as a pie or donut chart.\n\nd3.pie - create a new pie generator.\npie - compute the arc angles for the given dataset.\npie.value - set the value accessor.\npie.sort - set the sort order comparator.\npie.sortValues - set the sort order comparator.\npie.startAngle - set the overall start angle.\npie.endAngle - set the overall end angle.\npie.padAngle - set the pad angle between adjacent arcs.\n\n\n\n\nA spline or polyline, as in a line chart.\n\nd3.line - create a new line generator.\nline - generate a line for the given dataset.\nline.x - set the x accessor.\nline.y - set the y accessor.\nline.defined - set the defined accessor.\nline.curve - set the curve interpolator.\nline.context - set the rendering context.\nd3.lineRadial - create a new radial line generator.\nlineRadial - generate a line for the given dataset.\nlineRadial.angle - set the angle accessor.\nlineRadial.radius - set the radius accessor.\nlineRadial.defined - set the defined accessor.\nlineRadial.curve - set the curve interpolator.\nlineRadial.context - set the rendering context.\n\n\n\n\nAn area, defined by a bounding topline and baseline, as in an area chart.\n\nd3.area - create a new area generator.\narea - generate an area for the given dataset.\narea.x - set the x0 and x1 accessors.\narea.x0 - set the baseline x accessor.\narea.x1 - set the topline x accessor.\narea.y - set the y0 and y1 accessors.\narea.y0 - set the baseline y accessor.\narea.y1 - set the topline y accessor.\narea.defined - set the defined accessor.\narea.curve - set the curve interpolator.\narea.context - set the rendering context.\narea.lineX0 - derive a line for the left edge of an area.\narea.lineX1 - derive a line for the right edge of an area.\narea.lineY0 - derive a line for the top edge of an area.\narea.lineY1 - derive a line for the bottom edge of an area.\nd3.radialArea - create a new radial area generator.\nradialArea - generate an area for the given dataset.\nradialArea.angle - set the start and end angle accessors.\nradialArea.startAngle - set the start angle accessor.\nradialArea.endAngle - set the end angle accessor.\nradialArea.radius - set the inner and outer radius accessors.\nradialArea.innerRadius - set the inner radius accessor.\nradialArea.outerRadius - set the outer radius accessor.\nradialArea.defined - set the defined accessor.\nradialArea.curve - set the curve interpolator.\nradialArea.context - set the rendering context.\nradialArea.lineStartAngle - derive a line for the start edge of an area.\nradialArea.lineEndAngle - derive a line for the end edge of an area.\nradialArea.lineInnerRadius - derive a line for the inner edge of an area.\nradialArea.lineOuterRadius - derive a line for the outer edge of an area.\n\n\n\n\nInterpolate between points to produce a continuous shape.\n\nd3.curveBasis - a cubic basis spline, repeating the end points.\nd3.curveBasisClosed - a closed cubic basis spline.\nd3.curveBasisOpen - a cubic basis spline.\nd3.curveBundle - a straightened cubic basis spline.\nbundle.beta - set the bundle tension beta.\nd3.curveCardinal - a cubic cardinal spline, with one-sided difference at each end.\nd3.curveCardinalClosed - a closed cubic cardinal spline.\nd3.curveCardinalOpen - a cubic cardinal spline.\ncardinal.tension - set the cardinal spline tension.\nd3.curveCatmullRom - a cubic Catmull‚ÄìRom spline, with one-sided difference at each end.\nd3.curveCatmullRomClosed - a closed cubic Catmull‚ÄìRom spline.\nd3.curveCatmullRomOpen - a cubic Catmull‚ÄìRom spline.\ncatmullRom.alpha - set the Catmull‚ÄìRom parameter alpha.\nd3.curveLinear - a polyline.\nd3.curveLinearClosed - a closed polyline.\nd3.curveMonotoneX - a cubic spline that, given monotonicity in x, preserves it in y.\nd3.curveMonotoneY - a cubic spline that, given monotonicity in y, preserves it in x.\nd3.curveNatural - a natural cubic spline.\nd3.curveStep - a piecewise constant function.\nd3.curveStepAfter - a piecewise constant function.\nd3.curveStepBefore - a piecewise constant function.\ncurve.areaStart - start a new area segment.\ncurve.areaEnd - end the current area segment.\ncurve.lineStart - start a new line segment.\ncurve.lineEnd - end the current line segment.\ncurve.point - add a point to the current line segment.\n\n\n\n\nA smooth cubic B√©zier curve from a source to a target.\n\nd3.linkVertical - create a new vertical link generator.\nd3.linkHorizontal - create a new horizontal link generator.\nlink - generate a link.\nlink.source - set the source accessor.\nlink.target - set the target accessor.\nlink.x - set the point x-accessor.\nlink.y - set the point y-accessor.\nlink.context - set the rendering context.\nd3.linkRadial - create a new radial link generator.\nradialLink.angle - set the point angle accessor.\nradialLink.radius - set the point radius accessor.\n\n\n\n\nA categorical shape encoding, as in a scatterplot.\n\nd3.symbol - create a new symbol generator.\nsymbol - generate a symbol for the given datum.\nsymbol.type - set the symbol type.\nsymbol.size - set the size of the symbol in square pixels.\nsymbol.context - set the rendering context.\nd3.symbols - the array of built-in symbol types.\nd3.symbolCircle - a circle.\nd3.symbolCross - a Greek cross with arms of equal length.\nd3.symbolDiamond - a rhombus.\nd3.symbolSquare - a square.\nd3.symbolStar - a pentagonal star (pentagram).\nd3.symbolTriangle - an up-pointing triangle.\nd3.symbolWye - a Y shape.\nd3.pointRadial -\nsymbolType.draw - draw this symbol to the given context.\n\n\n\n\nStack shapes, placing one adjacent to another, as in a stacked bar chart.\n\nd3.stack - create a new stack generator.\nstack - generate a stack for the given dataset.\nstack.keys - set the keys accessor.\nstack.value - set the value accessor.\nstack.order - set the order accessor.\nstack.offset - set the offset accessor.\nd3.stackOrderAppearance - put the earliest series on bottom.\nd3.stackOrderAscending - put the smallest series on bottom.\nd3.stackOrderDescending - put the largest series on bottom.\nd3.stackOrderInsideOut - put earlier series in the middle.\nd3.stackOrderNone - use the given series order.\nd3.stackOrderReverse - use the reverse of the given series order.\nd3.stackOffsetExpand - normalize the baseline to zero and topline to one.\nd3.stackOffsetDiverging - positive above zero; negative below zero.\nd3.stackOffsetNone - apply a zero baseline.\nd3.stackOffsetSilhouette - center the streamgraph around zero.\nd3.stackOffsetWiggle - minimize streamgraph wiggling.\n\n\n\n\n\nParse and format times, inspired by strptime and strftime.\n\nd3.timeFormat - alias for locale.format on the default locale.\nd3.timeParse - alias for locale.parse on the default locale.\nd3.utcFormat - alias for locale.utcFormat on the default locale.\nd3.utcParse - alias for locale.utcParse on the default locale.\nd3.isoFormat - an ISO 8601 UTC formatter.\nd3.isoParse - an ISO 8601 UTC parser.\nd3.timeFormatLocale - define a custom locale.\nd3.timeFormatDefaultLocale - define the default locale.\nlocale.format - create a time formatter.\nlocale.parse - create a time parser.\nlocale.utcFormat - create a UTC formatter.\nlocale.utcParse - create a UTC parser.\n\n\n\n\nA calculator for humanity‚Äôs peculiar conventions of time.\n\nd3.timeInterval - implement a new custom time interval.\ninterval - alias for interval.floor.\ninterval.floor - round down to the nearest boundary.\ninterval.round - round to the nearest boundary.\ninterval.ceil - round up to the nearest boundary.\ninterval.offset - offset a date by some number of intervals.\ninterval.range - generate a range of dates at interval boundaries.\ninterval.filter - create a filtered subset of this interval.\ninterval.every - create a filtered subset of this interval.\ninterval.count - count interval boundaries between two dates.\nd3.timeMillisecond, d3.utcMillisecond - the millisecond interval.\nd3.timeMilliseconds, d3.utcMilliseconds - aliases for millisecond.range.\nd3.timeSecond, d3.utcSecond - the second interval.\nd3.timeSeconds, d3.utcSeconds - aliases for second.range.\nd3.timeMinute, d3.utcMinute - the minute interval.\nd3.timeMinutes, d3.utcMinutes - aliases for minute.range.\nd3.timeHour, d3.utcHour - the hour interval.\nd3.timeHours, d3.utcHours - aliases for hour.range.\nd3.timeDay, d3.utcDay - the day interval.\nd3.timeDays, d3.utcDays - aliases for day.range.\nd3.timeWeek, d3.utcWeek - aliases for sunday.\nd3.timeWeeks, d3.utcWeeks - aliases for week.range.\nd3.timeSunday, d3.utcSunday - the week interval, starting on Sunday.\nd3.timeSundays, d3.utcSundays - aliases for sunday.range.\nd3.timeMonday, d3.utcMonday - the week interval, starting on Monday.\nd3.timeMondays, d3.utcMondays - aliases for monday.range.\nd3.timeTuesday, d3.utcTuesday - the week interval, starting on Tuesday.\nd3.timeTuesdays, d3.utcTuesdays - aliases for tuesday.range.\nd3.timeWednesday, d3.utcWednesday - the week interval, starting on Wednesday.\nd3.timeWednesdays, d3.utcWednesdays - aliases for wednesday.range.\nd3.timeThursday, d3.utcThursday - the week interval, starting on Thursday.\nd3.timeThursdays, d3.utcThursdays - aliases for thursday.range.\nd3.timeFriday, d3.utcFriday - the week interval, starting on Friday.\nd3.timeFridays, d3.utcFridays - aliases for friday.range.\nd3.timeSaturday, d3.utcSaturday - the week interval, starting on Saturday.\nd3.timeSaturdays, d3.utcSaturdays - aliases for saturday.range.\nd3.timeMonth, d3.utcMonth - the month interval.\nd3.timeMonths, d3.utcMonths - aliases for month.range.\nd3.timeYear, d3.utcYear - the year interval.\nd3.timeYears, d3.utcYears - aliases for year.range.\n\n\n\n\nAn efficient queue for managing thousands of concurrent animations.\n\nd3.now - get the current high-resolution time.\nd3.timer - schedule a new timer.\ntimer.restart - reset the timer‚Äôs start time and callback.\ntimer.stop - stop the timer.\nd3.timerFlush - immediately execute any eligible timers.\nd3.timeout - schedule a timer that stops on its first callback.\nd3.interval - schedule a timer that is called with a configurable period.\n\n\n\n\nAnimated transitions for selections.\n\nselection.transition - schedule a transition for the selected elements.\nselection.interrupt - interrupt and cancel transitions on the selected elements.\nd3.transition - schedule a transition on the root document element.\ntransition.select - schedule a transition on the selected elements.\ntransition.selectAll - schedule a transition on the selected elements.\ntransition.filter - filter elements based on data.\ntransition.merge - merge this transition with another.\ntransition.selection - returns a selection for this transition.\ntransition.transition - schedule a new transition following this one.\ntransition.call - call a function with this transition.\ntransition.nodes - returns an array of all selected elements.\ntransition.node - returns the first (non-null) element.\ntransition.size - returns the count of elements.\ntransition.empty - returns true if this transition is empty.\ntransition.each - call a function for each element.\ntransition.on - add or remove transition event listeners.\ntransition.end - await the end of a transition.\ntransition.attr - tween the given attribute using the default interpolator.\ntransition.attrTween - tween the given attribute using a custom interpolator.\ntransition.style - tween the given style property using the default interpolator.\ntransition.styleTween - tween the given style property using a custom interpolator.\ntransition.text - set the text content when the transition starts.\ntransition.remove - remove the selected elements when the transition ends.\ntransition.tween - run custom code during the transition.\ntransition.delay - specify per-element delay in milliseconds.\ntransition.duration - specify per-element duration in milliseconds.\ntransition.ease - specify the easing function.\nd3.active - select the active transition for a given node.\nd3.interrupt - interrupt the active transition for a given node.\n\n\n\n\nCompute the Voronoi diagram of a given set of points.\n\nd3.voronoi - create a new Voronoi generator.\nvoronoi - generate a new Voronoi diagram for the given points.\nvoronoi.polygons - compute the Voronoi polygons for the given points.\nvoronoi.triangles - compute the Delaunay triangles for the given points.\nvoronoi.links - compute the Delaunay links for the given points.\nvoronoi.x - set the x accessor.\nvoronoi.y - set the y accessor.\nvoronoi.extent - set the observed extent of points.\nvoronoi.size - set the observed extent of points.\ndiagram.polygons - compute the polygons for this Voronoi diagram.\ndiagram.triangles - compute the triangles for this Voronoi diagram.\ndiagram.links - compute the links for this Voronoi diagram.\ndiagram.find - find the closest point in this Voronoi diagram.\n\n\n\n\nPan and zoom SVG, HTML or Canvas using mouse or touch input.\n\nd3.zoom - create a zoom behavior.\nzoom - apply the zoom behavior to the selected elements.\nzoom.transform - change the transform for the selected elements.\nzoom.translateTo - translate the transform for the selected elements.\nzoom.translateBy - translate the transform for the selected elements.\nzoom.scaleBy - scale the transform for the selected elements.\nzoom.scaleTo - scale the transform for the selected elements.\nzoom.filter - control which input events initiate zooming.\nzoom.touchable - set the touch support detector.\nzoom.wheelDelta - override scaling for wheel events.\nzoom.clickDistance - set the click distance threshold.\nzoom.extent - set the extent of the viewport.\nzoom.scaleExtent - set the allowed scale range.\nzoom.translateExtent - set the extent of the zoomable world.\nzoom.constrain - override the transform constraint logic.\nzoom.duration - set the duration of zoom transitions.\nzoom.interpolate - control the interpolation of zoom transitions.\nzoom.on - listen for zoom events.\nd3.zoomTransform - get the zoom transform for a given element.\ntransform.scale - scale a transform by the specified amount.\ntransform.translate - translate a transform by the specified amount.\ntransform.apply - apply the transform to the given point.\ntransform.applyX - apply the transform to the given x-coordinate.\ntransform.applyY - apply the transform to the given y-coordinate.\ntransform.invert - unapply the transform to the given point.\ntransform.invertX - unapply the transform to the given x-coordinate.\ntransform.invertY - unapply the transform to the given y-coordinate.\ntransform.rescaleX - apply the transform to an x-scale‚Äôs domain.\ntransform.rescaleY - apply the transform to a y-scale‚Äôs domain.\ntransform.toString - format the transform as an SVG transform string.\nd3.zoomIdentity - the identity transform."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#arrays-d3-array",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#arrays-d3-array",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Array manipulation, ordering, searching, summarizing, etc.\n\n\nMethods for computing basic summary statistics.\n\nd3.min - compute the minimum value in an array.\nd3.max - compute the maximum value in an array.\nd3.extent - compute the minimum and maximum value in an array.\nd3.sum - compute the sum of an array of numbers.\nd3.mean - compute the arithmetic mean of an array of numbers.\nd3.median - compute the median of an array of numbers (the 0.5-quantile).\nd3.quantile - compute a quantile for a sorted array of numbers.\nd3.variance - compute the variance of an array of numbers.\nd3.deviation - compute the standard deviation of an array of numbers.\n\n\n\n\nMethods for searching arrays for a specific element.\n\nd3.scan - linear search for an element using a comparator.\nd3.bisect - binary search for a value in a sorted array.\nd3.bisectRight - binary search for a value in a sorted array.\nd3.bisectLeft - binary search for a value in a sorted array.\nd3.bisector - bisect using an accessor or comparator.\nbisector.left - bisectLeft, with the given comparator.\nbisector.right - bisectRight, with the given comparator.\nd3.ascending - compute the natural order of two values.\nd3.descending - compute the natural order of two values.\n\n\n\n\nMethods for transforming arrays and for generating new arrays.\n\nd3.cross - compute the Cartesian product of two arrays.\nd3.merge - merge multiple arrays into one array.\nd3.pairs - create an array of adjacent pairs of elements.\nd3.permute - reorder an array of elements according to an array of indexes.\nd3.shuffle - randomize the order of an array.\nd3.ticks - generate representative values from a numeric interval.\nd3.tickIncrement - generate representative values from a numeric interval.\nd3.tickStep - generate representative values from a numeric interval.\nd3.range - generate a range of numeric values.\nd3.transpose - transpose an array of arrays.\nd3.zip - transpose a variable number of arrays.\n\n\n\n\nBin discrete samples into continuous, non-overlapping intervals.\n\nd3.histogram - create a new histogram generator.\nhistogram - compute the histogram for the given array of samples.\nhistogram.value - specify a value accessor for each sample.\nhistogram.domain - specify the interval of observable values.\nhistogram.thresholds - specify how values are divided into bins.\nd3.thresholdFreedmanDiaconis - the Freedman‚ÄìDiaconis binning rule.\nd3.thresholdScott - Scott‚Äôs normal reference binning rule.\nd3.thresholdSturges - Sturges‚Äô binning formula."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#axes-d3-axis",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#axes-d3-axis",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Human-readable reference marks for scales.\n\nd3.axisTop - create a new top-oriented axis generator.\nd3.axisRight - create a new right-oriented axis generator.\nd3.axisBottom - create a new bottom-oriented axis generator.\nd3.axisLeft - create a new left-oriented axis generator.\naxis - generate an axis for the given selection.\naxis.scale - set the scale.\naxis.ticks - customize how ticks are generated and formatted.\naxis.tickArguments - customize how ticks are generated and formatted.\naxis.tickValues - set the tick values explicitly.\naxis.tickFormat - set the tick format explicitly.\naxis.tickSize - set the size of the ticks.\naxis.tickSizeInner - set the size of inner ticks.\naxis.tickSizeOuter - set the size of outer (extent) ticks.\naxis.tickPadding - set the padding between ticks and labels."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#brushes-d3-brush",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#brushes-d3-brush",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Select a one- or two-dimensional region using the mouse or touch.\n\nd3.brush - create a new two-dimensional brush.\nd3.brushX - create a brush along the x-dimension.\nd3.brushY - create a brush along the y-dimension.\nbrush - apply the brush to a selection.\nbrush.move - move the brush selection.\nbrush.extent - define the brushable region.\nbrush.filter - control which input events initiate brushing.\nbrush.handleSize - set the size of the brush handles.\nbrush.on - listen for brush events.\nd3.brushSelection - get the brush selection for a given node."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#chords-d3-chord",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#chords-d3-chord",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "d3.chord - create a new chord layout.\nchord - compute the layout for the given matrix.\nchord.padAngle - set the padding between adjacent groups.\nchord.sortGroups - define the group order.\nchord.sortSubgroups - define the source and target order within groups.\nchord.sortChords - define the chord order across groups.\nd3.ribbon - create a ribbon shape generator.\nribbon - generate a ribbon shape.\nribbon.source - set the source accessor.\nribbon.target - set the target accessor.\nribbon.radius - set the ribbon source or target radius.\nribbon.startAngle - set the ribbon source or target start angle.\nribbon.endAngle - set the ribbon source or target end angle.\nribbon.context - set the render context."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#collections-d3-collection",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#collections-d3-collection",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Handy data structures for elements keyed by string.\n\n\nMethods for converting associative arrays (objects) to arrays.\n\nd3.keys - list the keys of an associative array.\nd3.values - list the values of an associated array.\nd3.entries - list the key-value entries of an associative array.\n\n\n\n\nLike ES6 Map, but with string keys and a few other differences.\n\nd3.map - create a new, empty map.\nmap.has - returns true if the map contains the given key.\nmap.get - get the value for the given key.\nmap.set - set the value for the given key.\nmap.remove - remove the entry for given key.\nmap.clear - remove all entries.\nmap.keys - get the array of keys.\nmap.values - get the array of values.\nmap.entries - get the array of entries (key-values objects).\nmap.each - call a function for each entry.\nmap.empty - returns false if the map has at least one entry.\nmap.size - compute the number of entries.\n\n\n\n\nLike ES6 Set, but with string keys and a few other differences.\n\nd3.set - create a new, empty set.\nset.has - returns true if the set contains the given value.\nset.add - add the given value.\nset.remove - remove the given value.\nset.clear - remove all values.\nset.values - get the array of values.\nset.each - call a function for each value.\nset.empty - returns true if the set has at least one value.\nset.size - compute the number of values.\n\n\n\n\nGroup data into arbitrary hierarchies.\n\nd3.nest - create a new nest generator.\nnest.key - add a level to the nest hierarchy.\nnest.sortKeys - sort the current nest level by key.\nnest.sortValues - sort the leaf nest level by value.\nnest.rollup - specify a rollup function for leaf values.\nnest.map - generate the nest, returning a map.\nnest.object - generate the nest, returning an associative array.\nnest.entries - generate the nest, returning an array of key-values tuples."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#colors-d3-color",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#colors-d3-color",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Color manipulation and color space conversion.\n\nd3.color - parse the given CSS color specifier.\ncolor.rgb - compute the RGB equivalent of this color.\ncolor.brighter - create a brighter copy of this color.\ncolor.darker - create a darker copy of this color.\ncolor.displayable - returns true if the color is displayable on standard hardware.\ncolor.hex - returns the hexadecimal RGB string representation of this color.\ncolor.toString - returns the RGB string representation of this color.\nd3.rgb - create a new RGB color.\nd3.hsl - create a new HSL color.\nd3.lab - create a new Lab color.\nd3.hcl - create a new HCL color.\nd3.lch - create a new HCL color.\nd3.gray - create a new Lab gray.\nd3.cubehelix - create a new Cubehelix color."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#color-schemes-d3-scale-chromatic",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#color-schemes-d3-scale-chromatic",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Color ramps and palettes for quantitative, ordinal and categorical scales.\n\n\n\nd3.schemeCategory10 -\nd3.schemeAccent -\nd3.schemeDark2 -\nd3.schemePaired -\nd3.schemePastel1 -\nd3.schemePastel2 -\nd3.schemeSet1 -\nd3.schemeSet2 -\nd3.schemeSet3 -\n\n\n\n\n\nd3.interpolateBrBG -\nd3.interpolatePiYG -\nd3.interpolatePRGn -\nd3.interpolatePuOr -\nd3.interpolateRdBu -\nd3.interpolateRdGy -\nd3.interpolateRdYlBu -\nd3.interpolateRdYlGn -\nd3.interpolateSpectral -\nd3.schemeBrBG -\nd3.schemePiYG -\nd3.schemePRGn -\nd3.schemePuOr -\nd3.schemeRdBu -\nd3.schemeRdGy -\nd3.schemeRdYlBu -\nd3.schemeRdYlGn -\nd3.schemeSpectral -\n\n\n\n\n\nd3.interpolateBlues -\nd3.interpolateGreens -\nd3.interpolateGreys -\nd3.interpolateOranges -\nd3.interpolatePurples -\nd3.interpolateReds -\nd3.schemeBlues -\nd3.schemeGreens -\nd3.schemeGreys -\nd3.schemeOranges -\nd3.schemePurples -\nd3.schemeReds -\n\n\n\n\n\nd3.interpolateBuGn -\nd3.interpolateBuPu -\nd3.interpolateCool -\nd3.interpolateCubehelixDefault -\nd3.interpolateGnBu -\nd3.interpolateInferno -\nd3.interpolateMagma -\nd3.interpolateOrRd -\nd3.interpolatePlasma -\nd3.interpolatePuBu -\nd3.interpolatePuBuGn -\nd3.interpolatePuRd -\nd3.interpolateRdPu -\nd3.interpolateViridis -\nd3.interpolateWarm -\nd3.interpolateYlGn -\nd3.interpolateYlGnBu -\nd3.interpolateYlOrBr -\nd3.interpolateYlOrRd -\nd3.schemeBuGn -\nd3.schemeBuPu -\nd3.schemeGnBu -\nd3.schemeOrRd -\nd3.schemePuBu -\nd3.schemePuBuGn -\nd3.schemePuRd -\nd3.schemeRdPu -\nd3.schemeYlGn -\nd3.schemeYlGnBu -\nd3.schemeYlOrBr -\nd3.schemeYlOrRd -\n\n\n\n\n\nd3.interpolateRainbow - the ‚Äúless-angry‚Äù rainbow\nd3.interpolateSinebow - the ‚Äúsinebow‚Äù smooth rainbow"
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#contours-d3-contour",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#contours-d3-contour",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Compute contour polygons using marching squares.\n\nd3.contours - create a new contour generator.\ncontours - compute the contours for a given grid of values.\ncontours.contour -\ncontours.size -\ncontours.smooth -\ncontours.thresholds -\nd3.contourDensity - create a new density estimator.\ndensity - estimate the density of a given array of samples.\ndensity.x -\ndensity.y -\ndensity.size -\ndensity.cellSize -\ndensity.thresholds -\ndensity.bandwidth -\ndensity.weight -"
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#dispatches-d3-dispatch",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#dispatches-d3-dispatch",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Separate concerns using named callbacks.\n\nd3.dispatch - create a custom event dispatcher.\ndispatch.on - register or unregister an event listener.\ndispatch.copy - create a copy of a dispatcher.\ndispatch.call - dispatch an event to registered listeners.\ndispatch.apply - dispatch an event to registered listeners."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#dragging-d3-drag",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#dragging-d3-drag",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Drag and drop SVG, HTML or Canvas using mouse or touch input.\n\nd3.drag - create a drag behavior.\ndrag - apply the drag behavior to a selection.\ndrag.container - set the coordinate system.\ndrag.filter - ignore some initiating input events.\ndrag.touchable - set the touch support detector.\ndrag.subject - set the thing being dragged.\ndrag.clickDistance - set the click distance threshold.\ndrag.on - listen for drag events.\nevent.on - listen for drag events on the current gesture.\nd3.dragDisable - prevent native drag-and-drop and text selection.\nd3.dragEnable - enable native drag-and-drop and text selection."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#delimiter-separated-values-d3-dsv",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#delimiter-separated-values-d3-dsv",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Parse and format delimiter-separated values, most commonly CSV and TSV.\n\nd3.dsvFormat - create a new parser and formatter for the given delimiter.\ndsv.parse - parse the given string, returning an array of objects.\ndsv.parseRows - parse the given string, returning an array of rows.\ndsv.format - format the given array of objects.\ndsv.formatBody - format the given array of objects.\ndsv.formatRows - format the given array of rows.\nd3.autoType - automatically infer value types for the given object.\nd3.csvParse - parse the given CSV string, returning an array of objects.\nd3.csvParseRows - parse the given CSV string, returning an array of rows.\nd3.csvFormat - format the given array of objects as CSV.\nd3.csvFormatBody - format the given array of objects as CSV.\nd3.csvFormatRows - format the given array of rows as CSV.\nd3.tsvParse - parse the given TSV string, returning an array of objects.\nd3.tsvParseRows - parse the given TSV string, returning an array of rows.\nd3.tsvFormat - format the given array of objects as TSV.\nd3.tsvFormatBody - format the given array of objects as TSV.\nd3.tsvFormatRows - format the given array of rows as TSV."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#easings-d3-ease",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#easings-d3-ease",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Easing functions for smooth animation.\n\nease - ease the given normalized time.\nd3.easeLinear - linear easing; the identity function.\nd3.easePolyIn - polynomial easing; raises time to the given power.\nd3.easePolyOut - reverse polynomial easing.\nd3.easePolyInOut - symmetric polynomial easing.\npoly.exponent - specify the polynomial exponent.\nd3.easeQuad - an alias for easeQuadInOut.\nd3.easeQuadIn - quadratic easing; squares time.\nd3.easeQuadOut - reverse quadratic easing.\nd3.easeQuadInOut - symmetric quadratic easing.\nd3.easeCubic - an alias for easeCubicInOut.\nd3.easeCubicIn - cubic easing; cubes time.\nd3.easeCubicOut - reverse cubic easing.\nd3.easeCubicInOut - symmetric cubic easing.\nd3.easeSin - an alias for easeSinInOut.\nd3.easeSinIn - sinusoidal easing.\nd3.easeSinOut - reverse sinusoidal easing.\nd3.easeSinInOut - symmetric sinusoidal easing.\nd3.easeExp - an alias for easeExpInOut.\nd3.easeExpIn - exponential easing.\nd3.easeExpOut - reverse exponential easing.\nd3.easeExpInOut - symmetric exponential easing.\nd3.easeCircle - an alias for easeCircleInOut.\nd3.easeCircleIn - circular easing.\nd3.easeCircleOut - reverse circular easing.\nd3.easeCircleInOut - symmetric circular easing.\nd3.easeElastic - an alias for easeElasticOut.\nd3.easeElasticIn - elastic easing, like a rubber band.\nd3.easeElasticOut - reverse elastic easing.\nd3.easeElasticInOut - symmetric elastic easing.\nelastic.amplitude - specify the elastic amplitude.\nelastic.period - specify the elastic period.\nd3.easeBack - an alias for easeBackInOut.\nd3.easeBackIn - anticipatory easing, like a dancer bending his knees before jumping.\nd3.easeBackOut - reverse anticipatory easing.\nd3.easeBackInOut - symmetric anticipatory easing.\nback.overshoot - specify the amount of overshoot.\nd3.easeBounce - an alias for easeBounceOut.\nd3.easeBounceIn - bounce easing, like a rubber ball.\nd3.easeBounceOut - reverse bounce easing.\nd3.easeBounceInOut - symmetric bounce easing."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#fetches-d3-fetch",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#fetches-d3-fetch",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Convenience methods on top of the Fetch API.\n\nd3.blob - get a file as a blob.\nd3.buffer - get a file as an array buffer.\nd3.csv - get a comma-separated values (CSV) file.\nd3.dsv - get a delimiter-separated values (CSV) file.\nd3.image - get an image.\nd3.json - get a JSON file.\nd3.text - get a plain text file.\nd3.tsv - get a tab-separated values (TSV) file."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#forces-d3-force",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#forces-d3-force",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Force-directed graph layout using velocity Verlet integration.\n\nd3.forceSimulation - create a new force simulation.\nsimulation.restart - reheat and restart the simulation‚Äôs timer.\nsimulation.stop - stop the simulation‚Äôs timer.\nsimulation.tick - advance the simulation one step.\nsimulation.nodes - set the simulation‚Äôs nodes.\nsimulation.alpha - set the current alpha.\nsimulation.alphaMin - set the minimum alpha threshold.\nsimulation.alphaDecay - set the alpha exponential decay rate.\nsimulation.alphaTarget - set the target alpha.\nsimulation.velocityDecay - set the velocity decay rate.\nsimulation.force - add or remove a force.\nsimulation.find - find the closest node to the given position.\nsimulation.on - add or remove an event listener.\nforce - apply the force.\nforce.initialize - initialize the force with the given nodes.\nd3.forceCenter - create a centering force.\ncenter.x - set the center x-coordinate.\ncenter.y - set the center y-coordinate.\nd3.forceCollide - create a circle collision force.\ncollide.radius - set the circle radius.\ncollide.strength - set the collision resolution strength.\ncollide.iterations - set the number of iterations.\nd3.forceLink - create a link force.\nlink.links - set the array of links.\nlink.id - link nodes by numeric index or string identifier.\nlink.distance - set the link distance.\nlink.strength - set the link strength.\nlink.iterations - set the number of iterations.\nd3.forceManyBody - create a many-body force.\nmanyBody.strength - set the force strength.\nmanyBody.theta - set the Barnes‚ÄìHut approximation accuracy.\nmanyBody.distanceMin - limit the force when nodes are close.\nmanyBody.distanceMax - limit the force when nodes are far.\nd3.forceX - create an x-positioning force.\nx.strength - set the force strength.\nx.x - set the target x-coordinate.\nd3.forceY - create an y-positioning force.\ny.strength - set the force strength.\ny.y - set the target y-coordinate.\nd3.forceRadial - create a radial positioning force.\nradial.strength - set the force strength.\nradial.radius - set the target radius.\nradial.x - set the target center x-coordinate.\nradial.y - set the target center y-coordinate."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#number-formats-d3-format",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#number-formats-d3-format",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Format numbers for human consumption.\n\nd3.format - alias for locale.format on the default locale.\nd3.formatPrefix - alias for locale.formatPrefix on the default locale.\nd3.formatSpecifier - parse a number format specifier.\nd3.formatLocale - define a custom locale.\nd3.formatDefaultLocale - define the default locale.\nlocale.format - create a number format.\nlocale.formatPrefix - create a SI-prefix number format.\nd3.precisionFixed - compute decimal precision for fixed-point notation.\nd3.precisionPrefix - compute decimal precision for SI-prefix notation.\nd3.precisionRound - compute significant digits for rounded notation."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#geographies-d3-geo",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#geographies-d3-geo",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Geographic projections, shapes and math.\n\n\n\nd3.geoPath - create a new geographic path generator.\npath - project and render the specified feature.\npath.area - compute the projected planar area of a given feature.\npath.bounds - compute the projected planar bounding box of a given feature.\npath.centroid - compute the projected planar centroid of a given feature.\npath.measure - compute the projected planar length of a given feature.\npath.projection - set the geographic projection.\npath.context - set the render context.\npath.pointRadius - set the radius to display point features.\n\n\n\n\n\nprojection - project the specified point from the sphere to the plane.\nprojection.invert - unproject the specified point from the plane to the sphere.\nprojection.stream - wrap the specified stream to project geometry.\nprojection.clipAngle - set the radius of the clip circle.\nprojection.clipExtent - set the viewport clip extent, in pixels.\nprojection.angle - set the post-projection rotation.\nprojection.scale - set the scale factor.\nprojection.translate - set the translation offset.\nprojection.fitExtent - set the scale and translate to fit a GeoJSON object.\nprojection.fitSize - set the scale and translate to fit a GeoJSON object.\nprojection.fitWidth - set the scale and translate to fit a GeoJSON object.\nprojection.fitHeight - set the scale and translate to fit a GeoJSON object.\nprojection.center - set the center point.\nprojection.rotate - set the three-axis spherical rotation angles.\nprojection.precision - set the precision threshold for adaptive sampling.\nprojection.preclip - set the spherical clipping stream transform.\nprojection.postclip - set the planar clipping stream transform.\nd3.geoClipAntimeridian - cuts spherical geometries that cross the antimeridian.\nd3.geoClipCircle - clips spherical geometries to a small circle.\nd3.geoClipRectangle - clips planar geometries to a rectangular viewport.\nd3.geoAlbers - the Albers equal-area conic projection.\nd3.geoAlbersUsa - a composite Albers projection for the United States.\nd3.geoAzimuthalEqualArea - the azimuthal equal-area projection.\nd3.geoAzimuthalEquidistant - the azimuthal equidistant projection.\nd3.geoConicConformal - the conic conformal projection.\nd3.geoConicEqualArea - the conic equal-area (Albers) projection.\nd3.geoConicEquidistant - the conic equidistant projection.\nconic.parallels - set the two standard parallels.\nd3.geoEqualEarth - the Equal Earth projection.\nd3.geoEquirectangular - the equirectangular (plate carre√©) projection.\nd3.geoGnomonic - the gnomonic projection.\nd3.geoMercator - the spherical Mercator projection.\nd3.geoOrthographic - the azimuthal orthographic projection.\nd3.geoStereographic - the azimuthal stereographic projection.\nd3.geoTransverseMercator - the transverse spherical Mercator projection.\nproject - project the specified point from the sphere to the plane.\nproject.invert - unproject the specified point from the plane to the sphere.\nd3.geoProjection - create a custom projection.\nd3.geoProjectionMutator - create a custom configurable projection.\nd3.geoAzimuthalEqualAreaRaw - the raw azimuthal equal-area projection.\nd3.geoAzimuthalEquidistantRaw - the raw azimuthal equidistant projection.\nd3.geoConicConformalRaw - the raw conic conformal projection.\nd3.geoConicEqualAreaRaw - the raw conic equal-area (Albers) projection.\nd3.geoConicEquidistantRaw - the raw conic equidistant projection.\nd3.geoEquirectangularRaw - the raw equirectangular (plate carre√©) projection.\nd3.geoGnomonicRaw - the raw gnomonic projection.\nd3.geoMercatorRaw - the raw Mercator projection.\nd3.geoOrthographicRaw - the raw azimuthal orthographic projection.\nd3.geoStereographicRaw - the raw azimuthal stereographic projection.\nd3.geoTransverseMercatorRaw - the raw transverse spherical Mercator projection.\n\n\n\n\n\nd3.geoArea - compute the spherical area of a given feature.\nd3.geoBounds - compute the latitude-longitude bounding box for a given feature.\nd3.geoCentroid - compute the spherical centroid of a given feature.\nd3.geoContains - test whether a point is inside a given feature.\nd3.geoDistance - compute the great-arc distance between two points.\nd3.geoLength - compute the length of a line string or the perimeter of a polygon.\nd3.geoInterpolate - interpolate between two points along a great arc.\nd3.geoRotation - create a rotation function for the specified angles.\nrotation - rotate the given point around the sphere.\nrotation.invert - unrotate the given point around the sphere.\n\n\n\n\n\nd3.geoCircle - create a circle generator.\ncircle - generate a piecewise circle as a Polygon.\ncircle.center - specify the circle center in latitude and longitude.\ncircle.radius - specify the angular radius in degrees.\ncircle.precision - specify the precision of the piecewise circle.\nd3.geoGraticule - create a graticule generator.\ngraticule - generate a MultiLineString of meridians and parallels.\ngraticule.lines - generate an array of LineStrings of meridians and parallels.\ngraticule.outline - generate a Polygon of the graticule‚Äôs extent.\ngraticule.extent - get or set the major & minor extents.\ngraticule.extentMajor - get or set the major extent.\ngraticule.extentMinor - get or set the minor extent.\ngraticule.step - get or set the major & minor step intervals.\ngraticule.stepMajor - get or set the major step intervals.\ngraticule.stepMinor - get or set the minor step intervals.\ngraticule.precision - get or set the latitudinal precision.\nd3.geoGraticule10 - generate the default 10¬∞ global graticule.\n\n\n\n\n\nd3.geoStream - convert a GeoJSON object to a geometry stream.\nstream.point - indicates a point with the specified coordinates.\nstream.lineStart - indicates the start of a line or ring.\nstream.lineEnd - indicates the end of a line or ring.\nstream.polygonStart - indicates the start of a polygon.\nstream.polygonEnd - indicates the end of a polygon.\nstream.sphere - indicates the sphere.\n\n\n\n\n\nd3.geoIdentity - scale, translate or clip planar geometry.\nidentity.reflectX - reflect the x-dimension.\nidentity.reflectY - reflect the y-dimension.\nd3.geoTransform - define a custom geometry transform."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#hierarchies-d3-hierarchy",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#hierarchies-d3-hierarchy",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Layout algorithms for visualizing hierarchical data.\n\nd3.hierarchy - constructs a root node from hierarchical data.\nnode.ancestors - generate an array of ancestors.\nnode.descendants - generate an array of descendants.\nnode.leaves - generate an array of leaves.\nnode.path - generate the shortest path to another node.\nnode.links - generate an array of links.\nnode.sum - evaluate and aggregate quantitative values.\nnode.sort - sort all descendant siblings.\nnode.count - count the number of leaves.\nnode.each - breadth-first traversal.\nnode.eachAfter - post-order traversal.\nnode.eachBefore - pre-order traversal.\nnode.copy - copy a hierarchy.\nd3.stratify - create a new stratify operator.\nstratify - construct a root node from tabular data.\nstratify.id - set the node id accessor.\nstratify.parentId - set the parent node id accessor.\nd3.cluster - create a new cluster (dendrogram) layout.\ncluster - layout the specified hierarchy in a dendrogram.\ncluster.size - set the layout size.\ncluster.nodeSize - set the node size.\ncluster.separation - set the separation between leaves.\nd3.tree - create a new tidy tree layout.\ntree - layout the specified hierarchy in a tidy tree.\ntree.size - set the layout size.\ntree.nodeSize - set the node size.\ntree.separation - set the separation between nodes.\nd3.treemap - create a new treemap layout.\ntreemap - layout the specified hierarchy as a treemap.\ntreemap.tile - set the tiling method.\ntreemap.size - set the layout size.\ntreemap.round - set whether the output coordinates are rounded.\ntreemap.padding - set the padding.\ntreemap.paddingInner - set the padding between siblings.\ntreemap.paddingOuter - set the padding between parent and children.\ntreemap.paddingTop - set the padding between the parent‚Äôs top edge and children.\ntreemap.paddingRight - set the padding between the parent‚Äôs right edge and children.\ntreemap.paddingBottom - set the padding between the parent‚Äôs bottom edge and children.\ntreemap.paddingLeft - set the padding between the parent‚Äôs left edge and children.\nd3.treemapBinary - tile using a balanced binary tree.\nd3.treemapDice - tile into a horizontal row.\nd3.treemapSlice - tile into a vertical column.\nd3.treemapSliceDice - alternate between slicing and dicing.\nd3.treemapSquarify - tile using squarified rows per Bruls et. al.\nd3.treemapResquarify - like d3.treemapSquarify, but performs stable updates.\nsquarify.ratio - set the desired rectangle aspect ratio.\nd3.partition - create a new partition (icicle or sunburst) layout.\npartition - layout the specified hierarchy as a partition diagram.\npartition.size - set the layout size.\npartition.round - set whether the output coordinates are rounded.\npartition.padding - set the padding.\nd3.pack - create a new circle-packing layout.\npack - layout the specified hierarchy using circle-packing.\npack.radius - set the radius accessor.\npack.size - set the layout size.\npack.padding - set the padding.\nd3.packSiblings - pack the specified array of circles.\nd3.packEnclose - enclose the specified array of circles."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#interpolators-d3-interpolate",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#interpolators-d3-interpolate",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Interpolate numbers, colors, strings, arrays, objects, whatever!\n\nd3.interpolate - interpolate arbitrary values.\nd3.interpolateArray - interpolate arrays of arbitrary values.\nd3.interpolateDate - interpolate dates.\nd3.interpolateNumber - interpolate numbers.\nd3.interpolateObject - interpolate arbitrary objects.\nd3.interpolateRound - interpolate integers.\nd3.interpolateString - interpolate strings with embedded numbers.\nd3.interpolateTransformCss - interpolate 2D CSS transforms.\nd3.interpolateTransformSvg - interpolate 2D SVG transforms.\nd3.interpolateZoom - zoom and pan between two views.\nd3.interpolateRgb - interpolate RGB colors.\nd3.interpolateRgbBasis - generate a B-spline through a set of colors.\nd3.interpolateRgbBasisClosed - generate a closed B-spline through a set of colors.\nd3.interpolateHsl - interpolate HSL colors.\nd3.interpolateHslLong - interpolate HSL colors, the long way.\nd3.interpolateLab - interpolate Lab colors.\nd3.interpolateHcl - interpolate HCL colors.\nd3.interpolateHclLong - interpolate HCL colors, the long way.\nd3.interpolateCubehelix - interpolate Cubehelix colors.\nd3.interpolateCubehelixLong - interpolate Cubehelix colors, the long way.\ninterpolate.gamma - apply gamma correction during interpolation.\nd3.interpolateHue - interpolate a hue angle.\nd3.interpolateDiscrete - generate a discrete interpolator from a set of values.\nd3.interpolateBasis - generate a B-spline through a set of values.\nd3.interpolateBasisClosed - generate a closed B-spline through a set of values.\nd3.piecewise - generate a piecewise linear interpolator from a set of values.\nd3.quantize - generate uniformly-spaced samples from an interpolator."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#paths-d3-path",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#paths-d3-path",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Serialize Canvas path commands to SVG.\n\nd3.path - create a new path serializer.\npath.moveTo - move to the given point.\npath.closePath - close the current subpath.\npath.lineTo - draw a straight line segment.\npath.quadraticCurveTo - draw a quadratic B√©zier segment.\npath.bezierCurveTo - draw a cubic B√©zier segment.\npath.arcTo - draw a circular arc segment.\npath.arc - draw a circular arc segment.\npath.rect - draw a rectangle.\npath.toString - serialize to an SVG path data string."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#polygons-d3-polygon",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#polygons-d3-polygon",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Geometric operations for two-dimensional polygons.\n\nd3.polygonArea - compute the area of the given polygon.\nd3.polygonCentroid - compute the centroid of the given polygon.\nd3.polygonHull - compute the convex hull of the given points.\nd3.polygonContains - test whether a point is inside a polygon.\nd3.polygonLength - compute the length of the given polygon‚Äôs perimeter."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#quadtrees-d3-quadtree",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#quadtrees-d3-quadtree",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Two-dimensional recursive spatial subdivision.\n\nd3.quadtree - create a new, empty quadtree.\nquadtree.x - set the x accessor.\nquadtree.y - set the y accessor.\nquadtree.add - add a datum to a quadtree.\nquadtree.addAll - add an array of data to a quadtree.\nquadtree.remove - remove a datum from a quadtree.\nquadtree.removeAll - remove an array of data from a quadtree.\nquadtree.copy - create a copy of a quadtree.\nquadtree.root - get the quadtree‚Äôs root node.\nquadtree.data - retrieve all data from the quadtree.\nquadtree.size - count the number of data in the quadtree.\nquadtree.find - quickly find the closest datum in a quadtree.\nquadtree.visit - selectively visit nodes in a quadtree.\nquadtree.visitAfter - visit all nodes in a quadtree.\nquadtree.cover - extend the quadtree to cover a point.\nquadtree.extent - extend the quadtree to cover an extent."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#random-numbers-d3-random",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#random-numbers-d3-random",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Generate random numbers from various distributions.\n\nd3.randomUniform - from a uniform distribution.\nd3.randomNormal - from a normal distribution.\nd3.randomLogNormal - from a log-normal distribution.\nd3.randomBates - from a Bates distribution.\nd3.randomIrwinHall - from an Irwin‚ÄìHall distribution.\nd3.randomExponential - from an exponential distribution.\nrandom.source - set the source of randomness."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#scales-d3-scale",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#scales-d3-scale",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Encodings that map abstract data to visual representation.\n\n\nMap a continuous, quantitative domain to a continuous range.\n\ncontinuous - compute the range value corresponding to a given domain value.\ncontinuous.invert - compute the domain value corresponding to a given range value.\ncontinuous.domain - set the input domain.\ncontinuous.range - set the output range.\ncontinuous.rangeRound - set the output range and enable rounding.\ncontinuous.clamp - enable clamping to the domain or range.\ncontinuous.interpolate - set the output interpolator.\ncontinuous.unknown - set the output value for unknown inputs.\ncontinuous.ticks - compute representative values from the domain.\ncontinuous.tickFormat - format ticks for human consumption.\ncontinuous.nice - extend the domain to nice round numbers.\ncontinuous.copy - create a copy of this scale.\nd3.scaleLinear - create a quantitative linear scale.\nd3.scalePow - create a quantitative power scale.\npow - compute the range value corresponding to a given domain value.\npow.invert - compute the domain value corresponding to a given range value.\npow.exponent - set the power exponent.\npow.domain - set the input domain.\npow.range - set the output range.\npow.rangeRound - set the output range and enable rounding.\npow.clamp - enable clamping to the domain or range.\npow.interpolate - set the output interpolator.\npow.ticks - compute representative values from the domain.\npow.tickFormat - format ticks for human consumption.\npow.nice - extend the domain to nice round numbers.\npow.copy - create a copy of this scale.\nd3.scaleSqrt - create a quantitative power scale with exponent 0.5.\nd3.scaleLog - create a quantitative logarithmic scale.\nlog - compute the range value corresponding to a given domain value.\nlog.invert - compute the domain value corresponding to a given range value.\nlog.base - set the logarithm base.\nlog.domain - set the input domain.\nlog.range - set the output range.\nlog.rangeRound - set the output range and enable rounding.\nlog.clamp - enable clamping to the domain or range.\nlog.interpolate - set the output interpolator.\nlog.ticks - compute representative values from the domain.\nlog.tickFormat - format ticks for human consumption.\nlog.nice - extend the domain to nice round numbers.\nlog.copy - create a copy of this scale.\nd3.scaleSymlog - create a symmetric logarithmic scale.\nd3.scaleIdentity - create a quantitative identity scale.\nd3.scaleTime - create a linear scale for time.\ntime - compute the range value corresponding to a given domain value.\ntime.invert - compute the domain value corresponding to a given range value.\ntime.domain - set the input domain.\ntime.range - set the output range.\ntime.rangeRound - set the output range and enable rounding.\ntime.clamp - enable clamping to the domain or range.\ntime.interpolate - set the output interpolator.\ntime.ticks - compute representative values from the domain.\ntime.tickFormat - format ticks for human consumption.\ntime.nice - extend the domain to nice round times.\ntime.copy - create a copy of this scale.\nd3.scaleUtc - create a linear scale for UTC.\nd3.tickFormat - format ticks for human consumption.\n\n\n\n\nMap a continuous, quantitative domain to a continuous, fixed interpolator.\n\nd3.scaleSequential - create a sequential scale.\nsequential.interpolator - set the scale‚Äôs output interpolator.\nd3.scaleSequentialLog -\nd3.scaleSequentialPow -\nd3.scaleSequentialSqrt -\nd3.scaleSequentialSymlog -\nd3.scaleSequentialQuantile -\n\n\n\n\nMap a continuous, quantitative domain to a continuous, fixed interpolator.\n\nd3.scaleDiverging - create a diverging scale.\ndiverging.interpolator - set the scale‚Äôs output interpolator.\nd3.scaleDivergingLog -\nd3.scaleDivergingPow -\nd3.scaleDivergingSqrt -\nd3.scaleDivergingSymlog -\n\n\n\n\nMap a continuous, quantitative domain to a discrete range.\n\nd3.scaleQuantize - create a uniform quantizing linear scale.\nquantize - compute the range value corresponding to a given domain value.\nquantize.invertExtent - compute the domain values corresponding to a given range value.\nquantize.domain - set the input domain.\nquantize.range - set the output range.\nquantize.nice - extend the domain to nice round numbers.\nquantize.ticks - compute representative values from the domain.\nquantize.tickFormat - format ticks for human consumption.\nquantize.copy - create a copy of this scale.\nd3.scaleQuantile - create a quantile quantizing linear scale.\nquantile - compute the range value corresponding to a given domain value.\nquantile.invertExtent - compute the domain values corresponding to a given range value.\nquantile.domain - set the input domain.\nquantile.range - set the output range.\nquantile.quantiles - get the quantile thresholds.\nquantile.copy - create a copy of this scale.\nd3.scaleThreshold - create an arbitrary quantizing linear scale.\nthreshold - compute the range value corresponding to a given domain value.\nthreshold.invertExtent - compute the domain values corresponding to a given range value.\nthreshold.domain - set the input domain.\nthreshold.range - set the output range.\nthreshold.copy - create a copy of this scale.\n\n\n\n\nMap a discrete domain to a discrete range.\n\nd3.scaleOrdinal - create an ordinal scale.\nordinal - compute the range value corresponding to a given domain value.\nordinal.domain - set the input domain.\nordinal.range - set the output range.\nordinal.unknown - set the output value for unknown inputs.\nordinal.copy - create a copy of this scale.\nd3.scaleImplicit - a special unknown value for implicit domains.\nd3.scaleBand - create an ordinal band scale.\nband - compute the band start corresponding to a given domain value.\nband.domain - set the input domain.\nband.range - set the output range.\nband.rangeRound - set the output range and enable rounding.\nband.round - enable rounding.\nband.paddingInner - set padding between bands.\nband.paddingOuter - set padding outside the first and last bands.\nband.padding - set padding outside and between bands.\nband.align - set band alignment, if there is extra space.\nband.bandwidth - get the width of each band.\nband.step - get the distance between the starts of adjacent bands.\nband.copy - create a copy of this scale.\nd3.scalePoint - create an ordinal point scale.\npoint - compute the point corresponding to a given domain value.\npoint.domain - set the input domain.\npoint.range - set the output range.\npoint.rangeRound - set the output range and enable rounding.\npoint.round - enable rounding.\npoint.padding - set padding outside the first and last point.\npoint.align - set point alignment, if there is extra space.\npoint.bandwidth - returns zero.\npoint.step - get the distance between the starts of adjacent points.\npoint.copy - create a copy of this scale."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#selections-d3-selection",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#selections-d3-selection",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Transform the DOM by selecting elements and joining to data.\n\n\n\nd3.selection - select the root document element.\nd3.select - select an element from the document.\nd3.selectAll - select multiple elements from the document.\nselection.select - select a descendant element for each selected element.\nselection.selectAll - select multiple descendants for each selected element.\nselection.filter - filter elements based on data.\nselection.merge - merge this selection with another.\nd3.matcher - test whether an element matches a selector.\nd3.selector - select an element.\nd3.selectorAll - select elements.\nd3.window - get a node‚Äôs owner window.\nd3.style - get a node‚Äôs current style value.\n\n\n\n\n\nselection.attr - get or set an attribute.\nselection.classed - get, add or remove CSS classes.\nselection.style - get or set a style property.\nselection.property - get or set a (raw) property.\nselection.text - get or set the text content.\nselection.html - get or set the inner HTML.\nselection.append - create, append and select new elements.\nselection.insert - create, insert and select new elements.\nselection.remove - remove elements from the document.\nselection.clone - insert clones of selected elements.\nselection.sort - sort elements in the document based on data.\nselection.order - reorders elements in the document to match the selection.\nselection.raise - reorders each element as the last child of its parent.\nselection.lower - reorders each element as the first child of its parent.\nd3.create - create and select a detached element.\nd3.creator - create an element by name.\n\n\n\n\n\nselection.data - bind elements to data.\nselection.join - enter, update or exit elements based on data.\nselection.enter - get the enter selection (data missing elements).\nselection.exit - get the exit selection (elements missing data).\nselection.datum - get or set element data (without joining).\n\n\n\n\n\nselection.on - add or remove event listeners.\nselection.dispatch - dispatch a custom event.\nd3.event - the current user event, during interaction.\nd3.customEvent - temporarily define a custom event.\nd3.mouse - get the mouse position relative to a given container.\nd3.touch - get a touch position relative to a given container.\nd3.touches - get the touch positions relative to a given container.\nd3.clientPoint - get a position relative to a given container.\n\n\n\n\n\nselection.each - call a function for each element.\nselection.call - call a function with this selection.\nselection.empty - returns true if this selection is empty.\nselection.nodes - returns an array of all selected elements.\nselection.node - returns the first (non-null) element.\nselection.size - returns the count of elements.\n\n\n\n\n\nd3.local - declares a new local variable.\nlocal.set - set a local variable‚Äôs value.\nlocal.get - get a local variable‚Äôs value.\nlocal.remove - delete a local variable.\nlocal.toString - get the property identifier of a local variable.\n\n\n\n\n\nd3.namespace - qualify a prefixed XML name, such as ‚Äúxlink:href‚Äù.\nd3.namespaces - the built-in XML namespaces."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#shapes-d3-shape",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#shapes-d3-shape",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Graphical primitives for visualization.\n\n\nCircular or annular sectors, as in a pie or donut chart.\n\nd3.arc - create a new arc generator.\narc - generate an arc for the given datum.\narc.centroid - compute an arc‚Äôs midpoint.\narc.innerRadius - set the inner radius.\narc.outerRadius - set the outer radius.\narc.cornerRadius - set the corner radius, for rounded corners.\narc.startAngle - set the start angle.\narc.endAngle - set the end angle.\narc.padAngle - set the angle between adjacent arcs, for padded arcs.\narc.padRadius - set the radius at which to linearize padding.\narc.context - set the rendering context.\n\n\n\n\nCompute the necessary angles to represent a tabular dataset as a pie or donut chart.\n\nd3.pie - create a new pie generator.\npie - compute the arc angles for the given dataset.\npie.value - set the value accessor.\npie.sort - set the sort order comparator.\npie.sortValues - set the sort order comparator.\npie.startAngle - set the overall start angle.\npie.endAngle - set the overall end angle.\npie.padAngle - set the pad angle between adjacent arcs.\n\n\n\n\nA spline or polyline, as in a line chart.\n\nd3.line - create a new line generator.\nline - generate a line for the given dataset.\nline.x - set the x accessor.\nline.y - set the y accessor.\nline.defined - set the defined accessor.\nline.curve - set the curve interpolator.\nline.context - set the rendering context.\nd3.lineRadial - create a new radial line generator.\nlineRadial - generate a line for the given dataset.\nlineRadial.angle - set the angle accessor.\nlineRadial.radius - set the radius accessor.\nlineRadial.defined - set the defined accessor.\nlineRadial.curve - set the curve interpolator.\nlineRadial.context - set the rendering context.\n\n\n\n\nAn area, defined by a bounding topline and baseline, as in an area chart.\n\nd3.area - create a new area generator.\narea - generate an area for the given dataset.\narea.x - set the x0 and x1 accessors.\narea.x0 - set the baseline x accessor.\narea.x1 - set the topline x accessor.\narea.y - set the y0 and y1 accessors.\narea.y0 - set the baseline y accessor.\narea.y1 - set the topline y accessor.\narea.defined - set the defined accessor.\narea.curve - set the curve interpolator.\narea.context - set the rendering context.\narea.lineX0 - derive a line for the left edge of an area.\narea.lineX1 - derive a line for the right edge of an area.\narea.lineY0 - derive a line for the top edge of an area.\narea.lineY1 - derive a line for the bottom edge of an area.\nd3.radialArea - create a new radial area generator.\nradialArea - generate an area for the given dataset.\nradialArea.angle - set the start and end angle accessors.\nradialArea.startAngle - set the start angle accessor.\nradialArea.endAngle - set the end angle accessor.\nradialArea.radius - set the inner and outer radius accessors.\nradialArea.innerRadius - set the inner radius accessor.\nradialArea.outerRadius - set the outer radius accessor.\nradialArea.defined - set the defined accessor.\nradialArea.curve - set the curve interpolator.\nradialArea.context - set the rendering context.\nradialArea.lineStartAngle - derive a line for the start edge of an area.\nradialArea.lineEndAngle - derive a line for the end edge of an area.\nradialArea.lineInnerRadius - derive a line for the inner edge of an area.\nradialArea.lineOuterRadius - derive a line for the outer edge of an area.\n\n\n\n\nInterpolate between points to produce a continuous shape.\n\nd3.curveBasis - a cubic basis spline, repeating the end points.\nd3.curveBasisClosed - a closed cubic basis spline.\nd3.curveBasisOpen - a cubic basis spline.\nd3.curveBundle - a straightened cubic basis spline.\nbundle.beta - set the bundle tension beta.\nd3.curveCardinal - a cubic cardinal spline, with one-sided difference at each end.\nd3.curveCardinalClosed - a closed cubic cardinal spline.\nd3.curveCardinalOpen - a cubic cardinal spline.\ncardinal.tension - set the cardinal spline tension.\nd3.curveCatmullRom - a cubic Catmull‚ÄìRom spline, with one-sided difference at each end.\nd3.curveCatmullRomClosed - a closed cubic Catmull‚ÄìRom spline.\nd3.curveCatmullRomOpen - a cubic Catmull‚ÄìRom spline.\ncatmullRom.alpha - set the Catmull‚ÄìRom parameter alpha.\nd3.curveLinear - a polyline.\nd3.curveLinearClosed - a closed polyline.\nd3.curveMonotoneX - a cubic spline that, given monotonicity in x, preserves it in y.\nd3.curveMonotoneY - a cubic spline that, given monotonicity in y, preserves it in x.\nd3.curveNatural - a natural cubic spline.\nd3.curveStep - a piecewise constant function.\nd3.curveStepAfter - a piecewise constant function.\nd3.curveStepBefore - a piecewise constant function.\ncurve.areaStart - start a new area segment.\ncurve.areaEnd - end the current area segment.\ncurve.lineStart - start a new line segment.\ncurve.lineEnd - end the current line segment.\ncurve.point - add a point to the current line segment.\n\n\n\n\nA smooth cubic B√©zier curve from a source to a target.\n\nd3.linkVertical - create a new vertical link generator.\nd3.linkHorizontal - create a new horizontal link generator.\nlink - generate a link.\nlink.source - set the source accessor.\nlink.target - set the target accessor.\nlink.x - set the point x-accessor.\nlink.y - set the point y-accessor.\nlink.context - set the rendering context.\nd3.linkRadial - create a new radial link generator.\nradialLink.angle - set the point angle accessor.\nradialLink.radius - set the point radius accessor.\n\n\n\n\nA categorical shape encoding, as in a scatterplot.\n\nd3.symbol - create a new symbol generator.\nsymbol - generate a symbol for the given datum.\nsymbol.type - set the symbol type.\nsymbol.size - set the size of the symbol in square pixels.\nsymbol.context - set the rendering context.\nd3.symbols - the array of built-in symbol types.\nd3.symbolCircle - a circle.\nd3.symbolCross - a Greek cross with arms of equal length.\nd3.symbolDiamond - a rhombus.\nd3.symbolSquare - a square.\nd3.symbolStar - a pentagonal star (pentagram).\nd3.symbolTriangle - an up-pointing triangle.\nd3.symbolWye - a Y shape.\nd3.pointRadial -\nsymbolType.draw - draw this symbol to the given context.\n\n\n\n\nStack shapes, placing one adjacent to another, as in a stacked bar chart.\n\nd3.stack - create a new stack generator.\nstack - generate a stack for the given dataset.\nstack.keys - set the keys accessor.\nstack.value - set the value accessor.\nstack.order - set the order accessor.\nstack.offset - set the offset accessor.\nd3.stackOrderAppearance - put the earliest series on bottom.\nd3.stackOrderAscending - put the smallest series on bottom.\nd3.stackOrderDescending - put the largest series on bottom.\nd3.stackOrderInsideOut - put earlier series in the middle.\nd3.stackOrderNone - use the given series order.\nd3.stackOrderReverse - use the reverse of the given series order.\nd3.stackOffsetExpand - normalize the baseline to zero and topline to one.\nd3.stackOffsetDiverging - positive above zero; negative below zero.\nd3.stackOffsetNone - apply a zero baseline.\nd3.stackOffsetSilhouette - center the streamgraph around zero.\nd3.stackOffsetWiggle - minimize streamgraph wiggling."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#time-formats-d3-time-format",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#time-formats-d3-time-format",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Parse and format times, inspired by strptime and strftime.\n\nd3.timeFormat - alias for locale.format on the default locale.\nd3.timeParse - alias for locale.parse on the default locale.\nd3.utcFormat - alias for locale.utcFormat on the default locale.\nd3.utcParse - alias for locale.utcParse on the default locale.\nd3.isoFormat - an ISO 8601 UTC formatter.\nd3.isoParse - an ISO 8601 UTC parser.\nd3.timeFormatLocale - define a custom locale.\nd3.timeFormatDefaultLocale - define the default locale.\nlocale.format - create a time formatter.\nlocale.parse - create a time parser.\nlocale.utcFormat - create a UTC formatter.\nlocale.utcParse - create a UTC parser."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#time-intervals-d3-time",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#time-intervals-d3-time",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "A calculator for humanity‚Äôs peculiar conventions of time.\n\nd3.timeInterval - implement a new custom time interval.\ninterval - alias for interval.floor.\ninterval.floor - round down to the nearest boundary.\ninterval.round - round to the nearest boundary.\ninterval.ceil - round up to the nearest boundary.\ninterval.offset - offset a date by some number of intervals.\ninterval.range - generate a range of dates at interval boundaries.\ninterval.filter - create a filtered subset of this interval.\ninterval.every - create a filtered subset of this interval.\ninterval.count - count interval boundaries between two dates.\nd3.timeMillisecond, d3.utcMillisecond - the millisecond interval.\nd3.timeMilliseconds, d3.utcMilliseconds - aliases for millisecond.range.\nd3.timeSecond, d3.utcSecond - the second interval.\nd3.timeSeconds, d3.utcSeconds - aliases for second.range.\nd3.timeMinute, d3.utcMinute - the minute interval.\nd3.timeMinutes, d3.utcMinutes - aliases for minute.range.\nd3.timeHour, d3.utcHour - the hour interval.\nd3.timeHours, d3.utcHours - aliases for hour.range.\nd3.timeDay, d3.utcDay - the day interval.\nd3.timeDays, d3.utcDays - aliases for day.range.\nd3.timeWeek, d3.utcWeek - aliases for sunday.\nd3.timeWeeks, d3.utcWeeks - aliases for week.range.\nd3.timeSunday, d3.utcSunday - the week interval, starting on Sunday.\nd3.timeSundays, d3.utcSundays - aliases for sunday.range.\nd3.timeMonday, d3.utcMonday - the week interval, starting on Monday.\nd3.timeMondays, d3.utcMondays - aliases for monday.range.\nd3.timeTuesday, d3.utcTuesday - the week interval, starting on Tuesday.\nd3.timeTuesdays, d3.utcTuesdays - aliases for tuesday.range.\nd3.timeWednesday, d3.utcWednesday - the week interval, starting on Wednesday.\nd3.timeWednesdays, d3.utcWednesdays - aliases for wednesday.range.\nd3.timeThursday, d3.utcThursday - the week interval, starting on Thursday.\nd3.timeThursdays, d3.utcThursdays - aliases for thursday.range.\nd3.timeFriday, d3.utcFriday - the week interval, starting on Friday.\nd3.timeFridays, d3.utcFridays - aliases for friday.range.\nd3.timeSaturday, d3.utcSaturday - the week interval, starting on Saturday.\nd3.timeSaturdays, d3.utcSaturdays - aliases for saturday.range.\nd3.timeMonth, d3.utcMonth - the month interval.\nd3.timeMonths, d3.utcMonths - aliases for month.range.\nd3.timeYear, d3.utcYear - the year interval.\nd3.timeYears, d3.utcYears - aliases for year.range."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#timers-d3-timer",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#timers-d3-timer",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "An efficient queue for managing thousands of concurrent animations.\n\nd3.now - get the current high-resolution time.\nd3.timer - schedule a new timer.\ntimer.restart - reset the timer‚Äôs start time and callback.\ntimer.stop - stop the timer.\nd3.timerFlush - immediately execute any eligible timers.\nd3.timeout - schedule a timer that stops on its first callback.\nd3.interval - schedule a timer that is called with a configurable period."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#transitions-d3-transition",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#transitions-d3-transition",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Animated transitions for selections.\n\nselection.transition - schedule a transition for the selected elements.\nselection.interrupt - interrupt and cancel transitions on the selected elements.\nd3.transition - schedule a transition on the root document element.\ntransition.select - schedule a transition on the selected elements.\ntransition.selectAll - schedule a transition on the selected elements.\ntransition.filter - filter elements based on data.\ntransition.merge - merge this transition with another.\ntransition.selection - returns a selection for this transition.\ntransition.transition - schedule a new transition following this one.\ntransition.call - call a function with this transition.\ntransition.nodes - returns an array of all selected elements.\ntransition.node - returns the first (non-null) element.\ntransition.size - returns the count of elements.\ntransition.empty - returns true if this transition is empty.\ntransition.each - call a function for each element.\ntransition.on - add or remove transition event listeners.\ntransition.end - await the end of a transition.\ntransition.attr - tween the given attribute using the default interpolator.\ntransition.attrTween - tween the given attribute using a custom interpolator.\ntransition.style - tween the given style property using the default interpolator.\ntransition.styleTween - tween the given style property using a custom interpolator.\ntransition.text - set the text content when the transition starts.\ntransition.remove - remove the selected elements when the transition ends.\ntransition.tween - run custom code during the transition.\ntransition.delay - specify per-element delay in milliseconds.\ntransition.duration - specify per-element duration in milliseconds.\ntransition.ease - specify the easing function.\nd3.active - select the active transition for a given node.\nd3.interrupt - interrupt the active transition for a given node."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#voronoi-diagrams-d3-voronoi",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#voronoi-diagrams-d3-voronoi",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Compute the Voronoi diagram of a given set of points.\n\nd3.voronoi - create a new Voronoi generator.\nvoronoi - generate a new Voronoi diagram for the given points.\nvoronoi.polygons - compute the Voronoi polygons for the given points.\nvoronoi.triangles - compute the Delaunay triangles for the given points.\nvoronoi.links - compute the Delaunay links for the given points.\nvoronoi.x - set the x accessor.\nvoronoi.y - set the y accessor.\nvoronoi.extent - set the observed extent of points.\nvoronoi.size - set the observed extent of points.\ndiagram.polygons - compute the polygons for this Voronoi diagram.\ndiagram.triangles - compute the triangles for this Voronoi diagram.\ndiagram.links - compute the links for this Voronoi diagram.\ndiagram.find - find the closest point in this Voronoi diagram."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/API.html#zooming-d3-zoom",
    "href": "docs/site_libs/d3v5-5.9.2/API.html#zooming-d3-zoom",
    "title": "1 D3 API Reference",
    "section": "",
    "text": "Pan and zoom SVG, HTML or Canvas using mouse or touch input.\n\nd3.zoom - create a zoom behavior.\nzoom - apply the zoom behavior to the selected elements.\nzoom.transform - change the transform for the selected elements.\nzoom.translateTo - translate the transform for the selected elements.\nzoom.translateBy - translate the transform for the selected elements.\nzoom.scaleBy - scale the transform for the selected elements.\nzoom.scaleTo - scale the transform for the selected elements.\nzoom.filter - control which input events initiate zooming.\nzoom.touchable - set the touch support detector.\nzoom.wheelDelta - override scaling for wheel events.\nzoom.clickDistance - set the click distance threshold.\nzoom.extent - set the extent of the viewport.\nzoom.scaleExtent - set the allowed scale range.\nzoom.translateExtent - set the extent of the zoomable world.\nzoom.constrain - override the transform constraint logic.\nzoom.duration - set the duration of zoom transitions.\nzoom.interpolate - control the interpolation of zoom transitions.\nzoom.on - listen for zoom events.\nd3.zoomTransform - get the zoom transform for a given element.\ntransform.scale - scale a transform by the specified amount.\ntransform.translate - translate a transform by the specified amount.\ntransform.apply - apply the transform to the given point.\ntransform.applyX - apply the transform to the given x-coordinate.\ntransform.applyY - apply the transform to the given y-coordinate.\ntransform.invert - unapply the transform to the given point.\ntransform.invertX - unapply the transform to the given x-coordinate.\ntransform.invertY - unapply the transform to the given y-coordinate.\ntransform.rescaleX - apply the transform to an x-scale‚Äôs domain.\ntransform.rescaleY - apply the transform to a y-scale‚Äôs domain.\ntransform.toString - format the transform as an SVG transform string.\nd3.zoomIdentity - the identity transform."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html",
    "title": "1 Changes in D3 5.0",
    "section": "",
    "text": "Released March 22, 2018.\nD3 5.0 introduces only a few non-backwards-compatible changes.\nD3 now uses Promises instead of asynchronous callbacks to load data. Promises simplify the structure of asynchronous code, especially in modern browsers that support async and await. (See this introduction to promises on Observable.) For example, to load a CSV file in v4, you might say:\nd3.csv(\"file.csv\", function(error, data) {\n  if (error) throw error;\n  console.log(data);\n});\nIn v5, using promises:\nd3.csv(\"file.csv\").then(function(data) {\n  console.log(data);\n});\nNote that you don‚Äôt need to rethrow the error‚Äîthe promise will reject automatically, and you can promise.catch if desired. Using await, the code is even simpler:\nconst data = await d3.csv(\"file.csv\");\nconsole.log(data);\nWith the adoption of promises, D3 now uses the Fetch API instead of XMLHttpRequest: the d3-request module has been replaced by d3-fetch. Fetch supports many powerful new features, such as streaming responses. D3 5.0 also deprecates and removes the d3-queue module. Use Promise.all to run a batch of asynchronous tasks in parallel, or a helper library such as p-queue to control concurrency.\nD3 no longer provides the d3.schemeCategory20* categorical color schemes. These twenty-color schemes were flawed because their grouped design could falsely imply relationships in the data: a shared hue can imply that the encoded data are part of a group (a super-category), while relative lightness can imply order. Instead, D3 now includes d3-scale-chromatic, which implements excellent schemes from ColorBrewer, including categorical, diverging, sequential single-hue and sequential multi-hue schemes. These schemes are available in both discrete and continuous variants.\nD3 now provides implementations of marching squares and density estimation via d3-contour! There are two new d3-selection methods: selection.clone for inserting clones of the selected nodes, and d3.create for creating detached elements. Geographic projections now support projection.angle, which has enabled several fantastic new polyhedral projections by Philippe Rivi√®re.\nLastly, D3‚Äôs package.json no longer pins exact versions of the dependent D3 modules. This fixes an issue with duplicate installs of D3 modules."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#arrays-d3-array",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#arrays-d3-array",
    "title": "1 Changes in D3 5.0",
    "section": "2.1 Arrays (d3-array)",
    "text": "2.1 Arrays (d3-array)\nThe new d3.scan method performs a linear scan of an array, returning the index of the least element according to the specified comparator. This is similar to d3.min and d3.max, except you can use it to find the position of an extreme element, rather than just calculate an extreme value.\nvar data = [\n  {name: \"Alice\", value: 2},\n  {name: \"Bob\", value: 3},\n  {name: \"Carol\", value: 1},\n  {name: \"Dwayne\", value: 5}\n];\n\nvar i = d3.scan(data, function(a, b) { return a.value - b.value; }); // 2\ndata[i]; // {name: \"Carol\", value: 1}\nThe new d3.ticks and d3.tickStep methods are useful for generating human-readable numeric ticks. These methods are a low-level alternative to continuous.ticks from d3-scale. The new implementation is also more accurate, returning the optimal number of ticks as measured by relative error.\nvar ticks = d3.ticks(0, 10, 5); // [0, 2, 4, 6, 8, 10]\nThe d3.range method no longer makes an elaborate attempt to avoid floating-point error when step is not an integer. The returned values are strictly defined as start + i * step, where i is an integer. (Learn more about floating point math.) d3.range returns the empty array for infinite ranges, rather than throwing an error.\nThe method signature for optional accessors has been changed to be more consistent with array methods such as array.forEach: the accessor is passed the current element (d), the index (i), and the array (data), with this as undefined. This affects d3.min, d3.max, d3.extent, d3.sum, d3.mean, d3.median, d3.quantile, d3.variance and d3.deviation. The d3.quantile method previously did not take an accessor. Some methods with optional arguments now treat those arguments as missing if they are null or undefined, rather than strictly checking arguments.length.\nThe new d3.histogram API replaces d3.layout.histogram. Rather than exposing bin.x and bin.dx on each returned bin, the histogram exposes bin.x0 and bin.x1, guaranteeing that bin.x0 is exactly equal to bin.x1 on the preceding bin. The ‚Äúfrequency‚Äù and ‚Äúprobability‚Äù modes are no longer supported; each bin is simply an array of elements from the input data, so bin.length is equal to D3 3.x‚Äôs bin.y in frequency mode. To compute a probability distribution, divide the number of elements in each bin by the total number of elements.\nThe histogram.range method has been renamed histogram.domain for consistency with scales. The histogram.bins method has been renamed histogram.thresholds, and no longer accepts an upper value: n thresholds will produce n + 1 bins. If you specify a desired number of bins rather than thresholds, d3.histogram now uses d3.ticks to compute nice bin thresholds. In addition to the default Sturges‚Äô formula, D3 now implements the Freedman-Diaconis rule and Scott‚Äôs normal reference rule."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#axes-d3-axis",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#axes-d3-axis",
    "title": "1 Changes in D3 5.0",
    "section": "2.2 Axes (d3-axis)",
    "text": "2.2 Axes (d3-axis)\nTo render axes properly in D3 3.x, you needed to style them:\n&lt;style&gt;\n\n.axis path,\n.axis line {\n  fill: none;\n  stroke: #000;\n  shape-rendering: crispEdges;\n}\n\n.axis text {\n  font: 10px sans-serif;\n}\n\n&lt;/style&gt;\n&lt;script&gt;\n\nd3.select(\".axis\")\n    .call(d3.svg.axis()\n        .scale(x)\n        .orient(\"bottom\"));\n\n&lt;/script&gt;\nIf you didn‚Äôt, you saw this:\n\nD3 4.0 provides default styles and shorter syntax. In place of d3.svg.axis and axis.orient, D3 4.0 now provides four constructors for each orientation: d3.axisTop, d3.axisRight, d3.axisBottom, d3.axisLeft. These constructors accept a scale, so you can reduce all of the above to:\n&lt;script&gt;\n\nd3.select(\".axis\")\n    .call(d3.axisBottom(x));\n\n&lt;/script&gt;\nAnd get this:\n\nAs before, you can customize the axis appearance either by applying stylesheets or by modifying the axis elements. The default appearance has been changed slightly to offset the axis by a half-pixel; this fixes a crisp-edges rendering issue on Safari where the axis would be drawn two-pixels thick.\nThere‚Äôs now an axis.tickArguments method, as an alternative to axis.ticks that also allows the axis tick arguments to be inspected. The axis.tickSize method has been changed to only allow a single argument when setting the tick size. The axis.innerTickSize and axis.outerTickSize methods have been renamed axis.tickSizeInner and axis.tickSizeOuter, respectively."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#brushes-d3-brush",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#brushes-d3-brush",
    "title": "1 Changes in D3 5.0",
    "section": "2.3 Brushes (d3-brush)",
    "text": "2.3 Brushes (d3-brush)\nReplacing d3.svg.brush, there are now three classes of brush for brushing along the x-dimension, the y-dimension, or both: d3.brushX, d3.brushY, d3.brush. Brushes are no longer dependent on scales; instead, each brush defines a selection in screen coordinates. This selection can be inverted if you want to compute the corresponding data domain. And rather than rely on the scales‚Äô¬†ranges to determine the brushable area, there is now a brush.extent method for setting it. If you do not set the brush extent, it defaults to the full extent of the owner SVG element. The brush.clamp method has also been eliminated; brushing is always restricted to the brushable area defined by the brush extent.\nBrushes no longer store the active brush selection (i.e., the highlighted region; the brush‚Äôs position) internally. The brush‚Äôs position is now stored on any elements to which the brush has been applied. The brush‚Äôs position is available as event.selection within a brush event or by calling d3.brushSelection on a given element. To move the brush programmatically, use brush.move with a given selection or transition; see the brush snapping example. The brush.event method has been removed.\nBrush interaction has been improved. By default, brushes now ignore right-clicks intended for the context menu; you can change this behavior using brush.filter. Brushes also ignore emulated mouse events on iOS. Holding down SHIFT (‚áß) while brushing locks the x- or y-position of the brush. Holding down META (‚åò) while clicking and dragging starts a new selection, rather than translating the existing selection.\nThe default appearance of the brush has also been improved and slightly simplified. Previously it was necessary to apply styles to the brush to give it a reasonable appearance, such as:\n.brush .extent {\n  stroke: #fff;\n  fill-opacity: .125;\n  shape-rendering: crispEdges;\n}\nThese styles are now applied by default as attributes; if you want to customize the brush appearance, you can still apply external styles or modify the brush elements. (D3 4.0 features a similar improvement to axes.) A new brush.handleSize method lets you override the brush handle size; it defaults to six pixels.\nThe brush now consumes handled events, making it easier to combine with other interactive behaviors such as dragging and zooming. The brushstart and brushend events have been renamed to start and end, respectively. The brush event no longer reports a event.mode to distinguish between resizing and dragging the brush."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#chords-d3-chord",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#chords-d3-chord",
    "title": "1 Changes in D3 5.0",
    "section": "2.4 Chords (d3-chord)",
    "text": "2.4 Chords (d3-chord)\nPursuant to the great namespace flattening:\n\nd3.layout.chord ‚Ü¶ d3.chord\nd3.svg.chord ‚Ü¶ d3.ribbon\n\nFor consistency with arc.padAngle, chord.padding has also been renamed to ribbon.padAngle. A new ribbon.context method lets you render chord diagrams to Canvas! See also d3-path."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#collections-d3-collection",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#collections-d3-collection",
    "title": "1 Changes in D3 5.0",
    "section": "2.5 Collections (d3-collection)",
    "text": "2.5 Collections (d3-collection)\nThe d3.set constructor now accepts an existing set for making a copy. If you pass an array to d3.set, you can also pass a value accessor. This accessor takes the standard arguments: the current element (d), the index (i), and the array (data), with this undefined. For example:\nvar yields = [\n  {yield: 22.13333, variety: \"Manchuria\",        year: 1932, site: \"Grand Rapids\"},\n  {yield: 26.76667, variety: \"Peatland\",         year: 1932, site: \"Grand Rapids\"},\n  {yield: 28.10000, variety: \"No. 462\",          year: 1931, site: \"Duluth\"},\n  {yield: 38.50000, variety: \"Svansota\",         year: 1932, site: \"Waseca\"},\n  {yield: 40.46667, variety: \"Svansota\",         year: 1931, site: \"Crookston\"},\n  {yield: 36.03333, variety: \"Peatland\",         year: 1932, site: \"Waseca\"},\n  {yield: 34.46667, variety: \"Wisconsin No. 38\", year: 1931, site: \"Grand Rapids\"}\n];\n\nvar sites = d3.set(yields, function(d) { return d.site; }); // Grand Rapids, Duluth, Waseca, Crookston\nThe d3.map constructor also follows the standard array accessor argument pattern.\nThe map.forEach and set.forEach methods have been renamed to map.each and set.each respectively. The order of arguments for map.each has also been changed to value, key and map, while the order of arguments for set.each is now value, value and set. This is closer to ES6 map.forEach and set.forEach. Also like ES6 Map and Set, map.set and set.add now return the current collection (rather than the added value) to facilitate method chaining. New map.clear and set.clear methods can be used to empty collections.\nThe nest.map method now always returns a d3.map instance. For a plain object, use nest.object instead. When used in conjunction with nest.rollup, nest.entries now returns {key, value} objects for the leaf entries, instead of {key, values}. This makes nest.rollup easier to use in conjunction with hierarchies, as in this Nest Treemap example."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#colors-d3-color",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#colors-d3-color",
    "title": "1 Changes in D3 5.0",
    "section": "2.6 Colors (d3-color)",
    "text": "2.6 Colors (d3-color)\nAll colors now have opacity exposed as color.opacity, which is a number in [0, 1]. You can pass an optional opacity argument to the color space constructors d3.rgb, d3.hsl, d3.lab, d3.hcl or d3.cubehelix.\nYou can now parse rgba(‚Ä¶) and hsla(‚Ä¶) CSS color specifiers or the string ‚Äútransparent‚Äù using d3.color. The ‚Äútransparent‚Äù color is defined as an RGB color with zero opacity and undefined red, green and blue channels; this differs slightly from CSS which defines it as transparent black, but is useful for simplifying color interpolation logic where either the starting or ending color has undefined channels. The color.toString method now likewise returns an rgb(‚Ä¶) or rgba(‚Ä¶) string with integer channel values, not the hexadecimal RGB format, consistent with CSS computed values. This improves performance by short-circuiting transitions when the element‚Äôs starting style matches its ending style.\nThe new d3.color method is the primary method for parsing colors: it returns a d3.color instance in the appropriate color space, or null if the CSS color specifier is invalid. For example:\nvar red = d3.color(\"hsl(0, 80%, 50%)\"); // {h: 0, l: 0.5, s: 0.8, opacity: 1}\nThe parsing implementation is now more robust. For example, you can no longer mix integers and percentages in rgb(‚Ä¶), and it correctly handles whitespace, decimal points, number signs, and other edge cases. The color space constructors d3.rgb, d3.hsl, d3.lab, d3.hcl and d3.cubehelix now always return a copy of the input color, converted to the corresponding color space. While color.rgb remains, rgb.hsl has been removed; use d3.hsl to convert a color to the RGB color space.\nThe RGB color space no longer greedily quantizes and clamps channel values when creating colors, improving accuracy in color space conversion. Quantization and clamping now occurs in color.toString when formatting a color for display. You can use the new color.displayable to test whether a color is out-of-gamut.\nThe rgb.brighter method no longer special-cases black. This is a multiplicative operator, defining a new color r‚Ä≤, g‚Ä≤, b‚Ä≤ where r‚Ä≤ = r √ó pow(0.7, k), g‚Ä≤ = g √ó pow(0.7, k) and b‚Ä≤ = b √ó pow(0.7, k); a brighter black is still black.\nThere‚Äôs a new d3.cubehelix color space, generalizing Dave Green‚Äôs color scheme! (See also d3.interpolateCubehelixDefault from d3-scale.) You can continue to define your own custom color spaces, too; see d3-hsv for an example."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#dispatches-d3-dispatch",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#dispatches-d3-dispatch",
    "title": "1 Changes in D3 5.0",
    "section": "2.7 Dispatches (d3-dispatch)",
    "text": "2.7 Dispatches (d3-dispatch)\nRather than decorating the dispatch object with each event type, the dispatch object now exposes generic dispatch.call and dispatch.apply methods which take the type string as the first argument. For example, in D3 3.x, you might say:\ndispatcher.foo.call(that, \"Hello, Foo!\");\nTo dispatch a foo event in D3 4.0, you‚Äôd say:\ndispatcher.call(\"foo\", that, \"Hello, Foo!\");\nThe dispatch.on method now accepts multiple typenames, allowing you to add or remove listeners for multiple events simultaneously. For example, to send both foo and bar events to the same listener:\ndispatcher.on(\"foo bar\", function(message) {\n  console.log(message);\n});\nThis matches the new behavior of selection.on in d3-selection. The dispatch.on method now validates that the specifier listener is a function, rather than throwing an error in the future.\nThe new implementation d3.dispatch is faster, using fewer closures to improve performance. There‚Äôs also a new dispatch.copy method for making a copy of a dispatcher; this is used by d3-transition to improve the performance of transitions in the common case where all elements in a transition have the same transition event listeners."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#dragging-d3-drag",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#dragging-d3-drag",
    "title": "1 Changes in D3 5.0",
    "section": "2.8 Dragging (d3-drag)",
    "text": "2.8 Dragging (d3-drag)\nThe drag behavior d3.behavior.drag has been renamed to d3.drag. The drag.origin method has been replaced by drag.subject, which allows you to define the thing being dragged at the start of a drag gesture. This is particularly useful with Canvas, where draggable objects typically share a Canvas element (as opposed to SVG, where draggable objects typically have distinct DOM elements); see the circle dragging example.\nA new drag.container method lets you override the parent element that defines the drag gesture coordinate system. This defaults to the parent node of the element to which the drag behavior was applied. For dragging on Canvas elements, you probably want to use the Canvas element as the container.\nDrag events now expose an event.on method for registering temporary listeners for duration of the current drag gesture; these listeners can capture state for the current gesture, such as the thing being dragged. A new event.active property lets you detect whether multiple (multitouch) drag gestures are active concurrently. The dragstart and dragend events have been renamed to start and end. By default, drag behaviors now ignore right-clicks intended for the context menu; use drag.filter to control which events are ignored. The drag behavior also ignores emulated mouse events on iOS. The drag behavior now consumes handled events, making it easier to combine with other interactive behaviors such as zooming.\nThe new d3.dragEnable and d3.dragDisable methods provide a low-level API for implementing drag gestures across browsers and devices. These methods are also used by other D3 components, such as the brush."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#delimiter-separated-values-d3-dsv",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#delimiter-separated-values-d3-dsv",
    "title": "1 Changes in D3 5.0",
    "section": "2.9 Delimiter-Separated Values (d3-dsv)",
    "text": "2.9 Delimiter-Separated Values (d3-dsv)\nPursuant to the great namespace flattening, various CSV and TSV methods have new names:\n\nd3.csv.parse ‚Ü¶ d3.csvParse\nd3.csv.parseRows ‚Ü¶ d3.csvParseRows\nd3.csv.format ‚Ü¶ d3.csvFormat\nd3.csv.formatRows ‚Ü¶ d3.csvFormatRows\nd3.tsv.parse ‚Ü¶ d3.tsvParse\nd3.tsv.parseRows ‚Ü¶ d3.tsvParseRows\nd3.tsv.format ‚Ü¶ d3.tsvFormat\nd3.tsv.formatRows ‚Ü¶ d3.tsvFormatRows\n\nThe d3.csv and d3.tsv methods for loading files of the corresponding formats have not been renamed, however! Those are defined in d3-request.There‚Äôs no longer a d3.dsv method, which served the triple purpose of defining a DSV formatter, a DSV parser and a DSV requestor; instead, there‚Äôs just d3.dsvFormat which you can use to define a DSV formatter and parser. You can use request.response to make a request and then parse the response body, or just use d3.text.\nThe dsv.parse method now exposes the column names and their input order as data.columns. For example:\nd3.csv(\"cars.csv\", function(error, data) {\n  if (error) throw error;\n  console.log(data.columns); // [\"Year\", \"Make\", \"Model\", \"Length\"]\n});\nYou can likewise pass an optional array of column names to dsv.format to format only a subset of columns, or to specify the column order explicitly:\nvar string = d3.csvFormat(data, [\"Year\", \"Model\", \"Length\"]);\nThe parser is a bit faster and the formatter is a bit more robust: inputs are coerced to strings before formatting, fixing an obscure crash, and deprecated support for falling back to dsv.formatRows when the input data is an array of arrays has been removed."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#easings-d3-ease",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#easings-d3-ease",
    "title": "1 Changes in D3 5.0",
    "section": "2.10 Easings (d3-ease)",
    "text": "2.10 Easings (d3-ease)\nD3 3.x used strings, such as ‚Äúcubic-in-out‚Äù, to identify easing methods; these strings could be passed to d3.ease or transition.ease. D3 4.0 uses symbols instead, such as d3.easeCubicInOut. Symbols are simpler and cleaner. They work well with Rollup to produce smaller custom bundles. You can still define your own custom easing function, too, if desired. Here‚Äôs the full list of equivalents:\n\nlinear ‚Ü¶ d3.easeLinear¬π\nlinear-in ‚Ü¶ d3.easeLinear¬π\nlinear-out ‚Ü¶ d3.easeLinear¬π\nlinear-in-out ‚Ü¶ d3.easeLinear¬π\nlinear-out-in ‚Ü¶ d3.easeLinear¬π\npoly-in ‚Ü¶ d3.easePolyIn\npoly-out ‚Ü¶ d3.easePolyOut\npoly-in-out ‚Ü¶ d3.easePolyInOut\npoly-out-in ‚Ü¶ REMOVED¬≤\nquad-in ‚Ü¶ d3.easeQuadIn\nquad-out ‚Ü¶ d3.easeQuadOut\nquad-in-out ‚Ü¶ d3.easeQuadInOut\nquad-out-in ‚Ü¶ REMOVED¬≤\ncubic-in ‚Ü¶ d3.easeCubicIn\ncubic-out ‚Ü¶ d3.easeCubicOut\ncubic-in-out ‚Ü¶ d3.easeCubicInOut\ncubic-out-in ‚Ü¶ REMOVED¬≤\nsin-in ‚Ü¶ d3.easeSinIn\nsin-out ‚Ü¶ d3.easeSinOut\nsin-in-out ‚Ü¶ d3.easeSinInOut\nsin-out-in ‚Ü¶ REMOVED¬≤\nexp-in ‚Ü¶ d3.easeExpIn\nexp-out ‚Ü¶ d3.easeExpOut\nexp-in-out ‚Ü¶ d3.easeExpInOut\nexp-out-in ‚Ü¶ REMOVED¬≤\ncircle-in ‚Ü¶ d3.easeCircleIn\ncircle-out ‚Ü¶ d3.easeCircleOut\ncircle-in-out ‚Ü¶ d3.easeCircleInOut\ncircle-out-in ‚Ü¶ REMOVED¬≤\nelastic-in ‚Ü¶ d3.easeElasticOut¬≤\nelastic-out ‚Ü¶ d3.easeElasticIn¬≤\nelastic-in-out ‚Ü¶ REMOVED¬≤\nelastic-out-in ‚Ü¶ d3.easeElasticInOut¬≤\nback-in ‚Ü¶ d3.easeBackIn\nback-out ‚Ü¶ d3.easeBackOut\nback-in-out ‚Ü¶ d3.easeBackInOut\nback-out-in ‚Ü¶ REMOVED¬≤\nbounce-in ‚Ü¶ d3.easeBounceOut¬≤\nbounce-out ‚Ü¶ d3.easeBounceIn¬≤\nbounce-in-out ‚Ü¶ REMOVED¬≤\nbounce-out-in ‚Ü¶ d3.easeBounceInOut¬≤\n\n¬π The -in, -out and -in-out variants of linear easing are identical, so there‚Äôs just d3.easeLinear. ¬≤ Elastic and bounce easing were inadvertently reversed in 3.x, so 4.0 eliminates -out-in easing!\nFor convenience, there are also default aliases for each easing method. For example, d3.easeCubic is an alias for d3.easeCubicInOut. Most default to -in-out; the exceptions¬†are d3.easeBounce and d3.easeElastic, which default to -out.\nRather than pass optional arguments to d3.ease or transition.ease, parameterizable easing functions now have named parameters: poly.exponent, elastic.amplitude, elastic.period and back.overshoot. For example, in D3 3.x you might say:\nvar e = d3.ease(\"elastic-out-in\", 1.2);\nThe equivalent in D3 4.0 is:\nvar e = d3.easeElastic.amplitude(1.2);\nMany of the easing functions have been optimized for performance and accuracy. Several bugs have been fixed, as well, such as the interpretation of the overshoot parameter for back easing, and the period parameter for elastic easing. Also, d3-transition now explicitly guarantees that the last tick of the transition happens at exactly t = 1, avoiding floating point errors in some easing functions.\nThere‚Äôs now a nice visual reference and an animated reference to the new easing functions, too!"
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#forces-d3-force",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#forces-d3-force",
    "title": "1 Changes in D3 5.0",
    "section": "2.11 Forces (d3-force)",
    "text": "2.11 Forces (d3-force)\nThe force layout d3.layout.force has been renamed to d3.forceSimulation. The force simulation now uses velocity Verlet integration rather than position Verlet, tracking the nodes‚Äô positions (node.x, node.y) and velocities (node.vx, node.vy) rather than their previous positions (node.px, node.py).\nRather than hard-coding a set of built-in forces, the force simulation is now extensible: you specify which forces you want! The approach affords greater flexibility through composition. The new forces are more flexible, too: force parameters can typically be configured per-node or per-link. There are separate positioning forces for x and y that replace force.gravity; x.x and y.y replace force.size. The new link force replaces force.linkStrength and employs better default heuristics to improve stability. The new many-body force replaces force.charge and supports a new minimum-distance parameter and performance improvements thanks to 4.0‚Äôs new quadtrees. There are also brand-new forces for centering nodes and collision resolution.\nThe new forces and simulation have been carefully crafted to avoid nondeterminism. Rather than initializing nodes randomly, if the nodes do not have preset positions, they are placed in a phyllotaxis pattern:\n\nRandom jitter is still needed to resolve link, collision and many-body forces if there are coincident nodes, but at least in the common case, the force simulation (and the resulting force-directed graph layout) is now consistent across browsers and reloads. D3 no longer plays dice!\nThe force simulation has several new methods for greater control over heating, such as simulation.alphaMin and simulation.alphaDecay, and the internal timer. Calling simulation.alpha now has no effect on the internal timer, which is controlled independently via simulation.stop and simulation.restart. The force layout‚Äôs internal timer now starts automatically on creation, removing force.start. As in 3.x, you can advance the simulation manually using simulation.tick. The force.friction parameter is replaced by simulation.velocityDecay. A new simulation.alphaTarget method allows you to set the desired alpha (temperature) of the simulation, such that the simulation can be smoothly reheated during interaction, and then smoothly cooled again. This improves the stability of the graph during interaction.\nThe force layout no longer depends on the drag behavior, though you can certainly create draggable force-directed graphs! Set node.fx and node.fy to fix a node‚Äôs position. As an alternative to a Voronoi SVG overlay, you can now use simulation.find to find the closest node to a pointer."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#number-formats-d3-format",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#number-formats-d3-format",
    "title": "1 Changes in D3 5.0",
    "section": "2.12 Number Formats (d3-format)",
    "text": "2.12 Number Formats (d3-format)\nIf a precision is not specified, the formatting behavior has changed: there is now a default precision of 6 for all directives except none, which defaults to 12. In 3.x, if you did not specify a precision, the number was formatted using its shortest unique representation (per number.toString); this could lead to unexpected digits due to floating point math. The new default precision in 4.0 produces more consistent results:\nvar f = d3.format(\"e\");\nf(42);        // \"4.200000e+1\"\nf(0.1 + 0.2); // \"3.000000e-1\"\nTo trim insignificant trailing zeroes, use the none directive, which is similar g. For example:\nvar f = d3.format(\".3\");\nf(0.12345);   // \"0.123\"\nf(0.10000);   // \"0.1\"\nf(0.1 + 0.2); // \"0.3\"\nUnder the hood, number formatting has improved accuracy with very large and very small numbers by using number.toExponential rather than Math.log to extract the mantissa and exponent. Negative zero (-0, an IEEE 754 construct) and very small numbers that round to zero are now formatted as unsigned zero. The inherently unsafe d3.round method has been removed, along with d3.requote.\nThe d3.formatPrefix method has been changed. Rather than returning an SI-prefix string, it returns an SI-prefix format function for a given specifier and reference value. For example, to format thousands:\nvar f = d3.formatPrefix(\",.0\", 1e3);\nf(1e3); // \"1k\"\nf(1e4); // \"10k\"\nf(1e5); // \"100k\"\nf(1e6); // \"1,000k\"\nUnlike the s format directive, d3.formatPrefix always employs the same SI-prefix, producing consistent results:\nvar f = d3.format(\".0s\");\nf(1e3); // \"1k\"\nf(1e4); // \"10k\"\nf(1e5); // \"100k\"\nf(1e6); // \"1M\"\nThe new ( sign option uses parentheses for negative values. This is particularly useful in conjunction with $. For example:\nd3.format(\"+.0f\")(-42);  // \"-42\"\nd3.format(\"(.0f\")(-42);  // \"(42)\"\nd3.format(\"+$.0f\")(-42); // \"-$42\"\nd3.format(\"($.0f\")(-42); // \"($42)\"\nThe new = align option places any sign and symbol to the left of any padding:\nd3.format(\"&gt;6d\")(-42);  // \"   -42\"\nd3.format(\"=6d\")(-42);  // \"-   42\"\nd3.format(\"&gt;(6d\")(-42); // \"  (42)\"\nd3.format(\"=(6d\")(-42); // \"(  42)\"\nThe b, o, d and x directives now round to the nearest integer, rather than returning the empty string for non-integers:\nd3.format(\"b\")(41.9); // \"101010\"\nd3.format(\"o\")(41.9); // \"52\"\nd3.format(\"d\")(41.9); // \"42\"\nd3.format(\"x\")(41.9); // \"2a\"\nThe c directive is now for character data (i.e., literal strings), not for character codes. The is useful if you just want to apply padding and alignment and don‚Äôt care about formatting numbers. For example, the infamous left-pad (as well as center- and right-pad!) can be conveniently implemented as:\nd3.format(\"&gt;10c\")(\"foo\"); // \"       foo\"\nd3.format(\"^10c\")(\"foo\"); // \"   foo    \"\nd3.format(\"&lt;10c\")(\"foo\"); // \"foo       \"\nThere are several new methods for computing suggested decimal precisions; these are used by d3-scale for tick formatting, and are helpful for implementing custom number formats: d3.precisionFixed, d3.precisionPrefix and d3.precisionRound. There‚Äôs also a new d3.formatSpecifier method for parsing, validating and debugging format specifiers; it‚Äôs also good for deriving related format specifiers, such as when you want to substitute the precision automatically.\nYou can now set the default locale using d3.formatDefaultLocale! The locales are published as JSON to npm."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#geographies-d3-geo",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#geographies-d3-geo",
    "title": "1 Changes in D3 5.0",
    "section": "2.13 Geographies (d3-geo)",
    "text": "2.13 Geographies (d3-geo)\nPursuant to the great namespace flattening, various methods have new names:\n\nd3.geo.graticule ‚Ü¶ d3.geoGraticule\nd3.geo.circle ‚Ü¶ d3.geoCircle\nd3.geo.area ‚Ü¶ d3.geoArea\nd3.geo.bounds ‚Ü¶ d3.geoBounds\nd3.geo.centroid ‚Ü¶ d3.geoCentroid\nd3.geo.distance ‚Ü¶ d3.geoDistance\nd3.geo.interpolate ‚Ü¶ d3.geoInterpolate\nd3.geo.length ‚Ü¶ d3.geoLength\nd3.geo.rotation ‚Ü¶ d3.geoRotation\nd3.geo.stream ‚Ü¶ d3.geoStream\nd3.geo.path ‚Ü¶ d3.geoPath\nd3.geo.projection ‚Ü¶ d3.geoProjection\nd3.geo.projectionMutator ‚Ü¶ d3.geoProjectionMutator\nd3.geo.albers ‚Ü¶ d3.geoAlbers\nd3.geo.albersUsa ‚Ü¶ d3.geoAlbersUsa\nd3.geo.azimuthalEqualArea ‚Ü¶ d3.geoAzimuthalEqualArea\nd3.geo.azimuthalEquidistant ‚Ü¶ d3.geoAzimuthalEquidistant\nd3.geo.conicConformal ‚Ü¶ d3.geoConicConformal\nd3.geo.conicEqualArea ‚Ü¶ d3.geoConicEqualArea\nd3.geo.conicEquidistant ‚Ü¶ d3.geoConicEquidistant\nd3.geo.equirectangular ‚Ü¶ d3.geoEquirectangular\nd3.geo.gnomonic ‚Ü¶ d3.geoGnomonic\nd3.geo.mercator ‚Ü¶ d3.geoMercator\nd3.geo.orthographic ‚Ü¶ d3.geoOrthographic\nd3.geo.stereographic ‚Ü¶ d3.geoStereographic\nd3.geo.transverseMercator ‚Ü¶ d3.geoTransverseMercator\n\nAlso renamed for consistency:\n\ncircle.origin ‚Ü¶ circle.center\ncircle.angle ‚Ü¶ circle.radius\ngraticule.majorExtent ‚Ü¶ graticule.extentMajor\ngraticule.minorExtent ‚Ü¶ graticule.extentMinor\ngraticule.majorStep ‚Ü¶ graticule.stepMajor\ngraticule.minorStep ‚Ü¶ graticule.stepMinor\n\nProjections now have more appropriate defaults. For example, d3.geoOrthographic has a 90¬∞ clip angle by default, showing only the front hemisphere, and d3.geoGnomonic has a default 60¬∞ clip angle. The default projection for d3.geoPath is now null rather than d3.geoAlbersUsa; a null projection is used with pre-projected geometry and is typically faster to render.\n‚ÄúFallback projections‚Äù‚Äîwhen you pass a function rather than a projection to path.projection‚Äîare no longer supported. For geographic projections, use d3.geoProjection or d3.geoProjectionMutator to define a custom projection. For arbitrary geometry transformations, implement the stream interface; see also d3.geoTransform. The ‚Äúraw‚Äù projections (e.g., d3.geo.equirectangular.raw) are no longer exported."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#hierarchies-d3-hierarchy",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#hierarchies-d3-hierarchy",
    "title": "1 Changes in D3 5.0",
    "section": "2.14 Hierarchies (d3-hierarchy)",
    "text": "2.14 Hierarchies (d3-hierarchy)\nPursuant to the great namespace flattening:\n\nd3.layout.cluster ‚Ü¶ d3.cluster\nd3.layout.hierarchy ‚Ü¶ d3.hierarchy\nd3.layout.pack ‚Ü¶ d3.pack\nd3.layout.partition ‚Ü¶ d3.partition\nd3.layout.tree ‚Ü¶ d3.tree\nd3.layout.treemap ‚Ü¶ d3.treemap\n\nAs an alternative to using JSON to represent hierarchical data (such as the ‚Äúflare.json format‚Äù used by many D3 examples), the new d3.stratify operator simplifies the conversion of tabular data to hierarchical data! This is convenient if you already have data in a tabular format, such as the result of a SQL query or a CSV file:\nname,parent\nEve,\nCain,Eve\nSeth,Eve\nEnos,Seth\nNoam,Seth\nAbel,Eve\nAwan,Eve\nEnoch,Awan\nAzura,Eve\nTo convert this to a root node:\nvar root = d3.stratify()\n    .id(function(d) { return d.name; })\n    .parentId(function(d) { return d.parent; })\n    (nodes);\nThe resulting root can be passed to d3.tree to produce a tree diagram like this:\n\nRoot nodes can also be created from JSON data using d3.hierarchy. The hierarchy layouts now take these root nodes as input rather than operating directly on JSON data, which helps to provide a cleaner separation between the input data and the computed layout. (For example, use node.copy to isolate layout changes.) It also simplifies the API: rather than each hierarchy layout needing to implement value and sorting accessors, there are now generic node.sum and node.sort methods that work with any hierarchy layout.\nThe new d3.hierarchy API also provides a richer set of methods for manipulating hierarchical data. For example, to generate an array of all nodes in topological order, use node.descendants; for just leaf nodes, use node.leaves. To highlight the ancestors of a given node on mouseover, use node.ancestors. To generate an array of {source, target} links for a given hierarchy, use node.links; this replaces treemap.links and similar methods on the other layouts. The new node.path method replaces d3.layout.bundle; see also d3.curveBundle for hierarchical edge bundling.\nThe hierarchy layouts have been rewritten using new, non-recursive traversal methods (node.each, node.eachAfter and node.eachBefore), improving performance on large datasets. The d3.tree layout no longer uses a node._ field to store temporary state during layout.\nTreemap tiling is now extensible via treemap.tile! The default squarified tiling algorithm, d3.treemapSquarify, has been completely rewritten, improving performance and fixing bugs in padding and rounding. The treemap.sticky method has been replaced with the d3.treemapResquarify, which is identical to d3.treemapSquarify except it performs stable neighbor-preserving updates. The treemap.ratio method has been replaced with squarify.ratio. And there‚Äôs a new d3.treemapBinary for binary treemaps!\nTreemap padding has also been improved. The treemap now distinguishes between outer padding that separates a parent from its children, and inner padding that separates adjacent siblings. You can set the top-, right-, bottom- and left-outer padding separately. There are new examples for the traditional nested treemap and for L√º and Fogarty‚Äôs cascaded treemap. And there‚Äôs a new example demonstrating d3.nest with d3.treemap.\nThe space-filling layouts d3.treemap and d3.partition now output x0, x1, y0, y1 on each node instead of x0, dx, y0, dy. This improves accuracy by ensuring that the edges of adjacent cells are exactly equal, rather than sometimes being slightly off due to floating point math. The partition layout now supports rounding and padding.\nThe circle-packing layout, d3.pack, has been completely rewritten to better implement Wang et al.‚Äôs algorithm, fixing major bugs and improving results! Welzl‚Äôs algorithm is now used to compute the exact smallest enclosing circle for each parent, rather than the approximate answer used by Wang et al.¬†The 3.x output is shown on the left; 4.0 is shown on the right:\n \nA non-hierarchical implementation is also available as d3.packSiblings, and the smallest enclosing circle implementation is available as d3.packEnclose. Pack padding now applies between a parent and its children, as well as between adjacent siblings. In addition, you can now specify padding as a function that is computed dynamically for each parent."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#internals",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#internals",
    "title": "1 Changes in D3 5.0",
    "section": "2.15 Internals",
    "text": "2.15 Internals\nThe d3.rebind method has been removed. (See the 3.x source.) If you want to wrap a getter-setter method, the recommend pattern is to implement a wrapper method and check the return value. For example, given a component that uses an internal dispatch, component.on can rebind dispatch.on as follows:\ncomponent.on = function() {\n  var value = dispatch.on.apply(dispatch, arguments);\n  return value === dispatch ? component : value;\n};\nThe d3.functor method has been removed. (See the 3.x source.) If you want to promote a constant value to a function, the recommended pattern is to implement a closure that returns the constant value. If desired, you can use a helper method as follows:\nfunction constant(x) {\n  return function() {\n    return x;\n  };\n}\nGiven a value x, to promote x to a function if it is not already:\nvar fx = typeof x === \"function\" ? x : constant(x);"
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#interpolators-d3-interpolate",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#interpolators-d3-interpolate",
    "title": "1 Changes in D3 5.0",
    "section": "2.16 Interpolators (d3-interpolate)",
    "text": "2.16 Interpolators (d3-interpolate)\nThe d3.interpolate method no longer delegates to d3.interpolators, which has been removed; its behavior is now defined by the library. It is now slightly faster in the common case that b is a number. It only uses d3.interpolateRgb if b is a valid CSS color specifier (and not approximately one). And if the end value b is null, undefined, true or false, d3.interpolate now returns a constant function which always returns b.\nThe behavior of d3.interpolateObject and d3.interpolateArray has changed slightly with respect to properties or elements in the start value a that do not exist in the end value b: these properties and elements are now ignored, such that the ending value of the interpolator at t = 1 is now precisely equal to b. So, in 3.x:\nd3.interpolateObject({foo: 2, bar: 1}, {foo: 3})(0.5); // {bar: 1, foo: 2.5} in 3.x\nWhereas in 4.0, a.bar is ignored:\nd3.interpolateObject({foo: 2, bar: 1}, {foo: 3})(0.5); // {foo: 2.5} in 4.0\nIf a or b are undefined or not an object, they are now implicitly converted to the empty object or empty array as appropriate, rather than throwing a TypeError.\nThe d3.interpolateTransform interpolator has been renamed to d3.interpolateTransformSvg, and there is a new d3.interpolateTransformCss to interpolate CSS transforms! This allows d3-transition to automatically interpolate both the SVG transform attribute and the CSS transform style property. (Note, however, that only 2D CSS transforms are supported.) The d3.transform method has been removed.\nColor space interpolators now interpolate opacity (see d3-color) and return rgb(‚Ä¶) or rgba(‚Ä¶) CSS color specifier strings rather than using the RGB hexadecimal format. This is necessary to support opacity interpolation, but is also beneficial because it matches CSS computed values. When a channel in the start color a is undefined, color interpolators now use the corresponding channel value from the end color b, or vice versa. This logic previously applied to some channels (such as saturation in HSL), but now applies to all channels in all color spaces, and is especially useful when interpolating to or from transparent.\nThere are now ‚Äúlong‚Äù versions of cylindrical color space interpolators: d3.interpolateHslLong, d3.interpolateHclLong and d3.interpolateCubehelixLong. These interpolators use linear interpolation of hue, rather than using the shortest path around the 360¬∞ hue circle. See d3.interpolateRainbow for an example. The Cubehelix color space is now supported by d3-color, and so there are now d3.interpolateCubehelix and d3.interpolateCubehelixLong interpolators.\nGamma-corrected color interpolation is now supported for both RGB and Cubehelix color spaces as interpolate.gamma. For example, to interpolate from purple to orange with a gamma of 2.2 in RGB space:\nvar interpolate = d3.interpolateRgb.gamma(2.2)(\"purple\", \"orange\");\nThere are new interpolators for uniform non-rational B-splines! These are useful for smoothly interpolating between an arbitrary sequence of values from t = 0 to t = 1, such as to generate a smooth color gradient from a discrete set of colors. The d3.interpolateBasis and d3.interpolateBasisClosed interpolators generate one-dimensional B-splines, while d3.interpolateRgbBasis and d3.interpolateRgbBasisClosed generate three-dimensional B-splines through RGB color space. These are used by d3-scale-chromatic to generate continuous color scales from ColorBrewer‚Äôs discrete color schemes, such as PiYG.\nThere‚Äôs also now a d3.quantize method for generating uniformly-spaced discrete samples from a continuous interpolator. This is useful for taking one of the built-in color scales (such as d3.interpolateViridis) and quantizing it for use with d3.scaleQuantize, d3.scaleQuantile or d3.scaleThreshold."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#paths-d3-path",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#paths-d3-path",
    "title": "1 Changes in D3 5.0",
    "section": "2.17 Paths (d3-path)",
    "text": "2.17 Paths (d3-path)\nThe d3.path serializer implements the CanvasPathMethods API, allowing you to write code that can render to either Canvas or SVG. For example, given some code that draws to a canvas:\nfunction drawCircle(context, radius) {\n  context.moveTo(radius, 0);\n  context.arc(0, 0, radius, 0, 2 * Math.PI);\n}\nYou can render to SVG as follows:\nvar context = d3.path();\ndrawCircle(context, 40);\npathElement.setAttribute(\"d\", context.toString());\nThe path serializer enables d3-shape to support both Canvas and SVG; see line.context and area.context, for example."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#polygons-d3-polygon",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#polygons-d3-polygon",
    "title": "1 Changes in D3 5.0",
    "section": "2.18 Polygons (d3-polygon)",
    "text": "2.18 Polygons (d3-polygon)\nThere‚Äôs no longer a d3.geom.polygon constructor; instead you just pass an array of vertices to the polygon methods. So instead of polygon.area and polygon.centroid, there‚Äôs d3.polygonArea and d3.polygonCentroid. There are also new d3.polygonContains and d3.polygonLength methods. There‚Äôs no longer an equivalent to polygon.clip, but if Sutherland‚ÄìHodgman clipping is needed, please file a feature request.\nThe d3.geom.hull operator has been simplified: instead of an operator with hull.x and hull.y accessors, there‚Äôs just the d3.polygonHull method which takes an array of points and returns the convex hull."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#quadtrees-d3-quadtree",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#quadtrees-d3-quadtree",
    "title": "1 Changes in D3 5.0",
    "section": "2.19 Quadtrees (d3-quadtree)",
    "text": "2.19 Quadtrees (d3-quadtree)\nThe d3.geom.quadtree method has been replaced by d3.quadtree. 4.0 removes the concept of quadtree ‚Äúgenerators‚Äù (configurable functions that build a quadtree from an array of data); there are now just quadtrees, which you can create via d3.quadtree and add data to via quadtree.add and quadtree.addAll. This code in 3.x:\nvar quadtree = d3.geom.quadtree()\n    .extent([[0, 0], [width, height]])\n    (data);\nCan be rewritten in 4.0 as:\nvar quadtree = d3.quadtree()\n    .extent([[0, 0], [width, height]])\n    .addAll(data);\nThe new quadtree implementation is vastly improved! It is no longer recursive, avoiding stack overflows when there are large numbers of coincident points. The internal storage is now more efficient, and the implementation is also faster; constructing a quadtree of 1M normally-distributed points takes about one second in 4.0, as compared to three seconds in 3.x.\nThe change in internal node structure affects quadtree.visit: use node.length to distinguish leaf nodes from internal nodes. For example, to iterate over all data in a quadtree:\nquadtree.visit(function(node) {\n  if (!node.length) {\n    do {\n      console.log(node.data);\n    } while (node = node.next)\n  }\n});\nThere‚Äôs a new quadtree.visitAfter method for visiting nodes in post-order traversal. This feature is used in d3-force to implement the Barnes‚ÄìHut approximation.\nYou can now remove data from a quadtree using quadtree.remove and quadtree.removeAll. When adding data to a quadtree, the quadtree will now expand its extent by repeated doubling if the new point is outside the existing extent of the quadtree. There are also quadtree.extent and quadtree.cover methods for explicitly expanding the extent of the quadtree after creation.\nQuadtrees support several new utility methods: quadtree.copy returns a copy of the quadtree sharing the same data; quadtree.data generates an array of all data in the quadtree; quadtree.size returns the number of data points in the quadtree; and quadtree.root returns the root node, which is useful for manual traversal of the quadtree. The quadtree.find method now takes an optional search radius, which is useful for pointer-based selection in force-directed graphs."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#queues-d3-queue",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#queues-d3-queue",
    "title": "1 Changes in D3 5.0",
    "section": "2.20 Queues (d3-queue)",
    "text": "2.20 Queues (d3-queue)\nFormerly known as Queue.js and queue-async, d3.queue is now included in the default bundle, making it easy to load data files in parallel. It has been rewritten with fewer closures to improve performance, and there are now stricter checks in place to guarantee well-defined behavior. You can now use instanceof d3.queue and inspect the queue‚Äôs internal private state."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#random-numbers-d3-random",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#random-numbers-d3-random",
    "title": "1 Changes in D3 5.0",
    "section": "2.21 Random Numbers (d3-random)",
    "text": "2.21 Random Numbers (d3-random)\nPursuant to the great namespace flattening, the random number generators have new names:\n\nd3.random.normal ‚Ü¶ d3.randomNormal\nd3.random.logNormal ‚Ü¶ d3.randomLogNormal\nd3.random.bates ‚Ü¶ d3.randomBates\nd3.random.irwinHall ‚Ü¶ d3.randomIrwinHall\n\nThere are also new random number generators for exponential and uniform distributions. The normal and log-normal random generators have been optimized."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#requests-d3-request",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#requests-d3-request",
    "title": "1 Changes in D3 5.0",
    "section": "2.22 Requests (d3-request)",
    "text": "2.22 Requests (d3-request)\nThe d3.xhr method has been renamed to d3.request. Basic authentication is now supported using request.user and request.password. You can now configure a timeout using request.timeout.\nIf an error occurs, the corresponding ProgressEvent of type ‚Äúerror‚Äù is now passed to the error listener, rather than the XMLHttpRequest. Likewise, the ProgressEvent is passed to progress event listeners, rather than using d3.event. If d3.xml encounters an error parsing XML, this error is now reported to error listeners rather than returning a null response.\nThe d3.request, d3.text and d3.xml methods no longer take an optional mime type as the second argument; use request.mimeType instead. For example:\nd3.xml(\"file.svg\").mimeType(\"image/svg+xml\").get(function(error, svg) {\n  ‚Ä¶\n});\nWith the exception of d3.html and d3.xml, Node is now supported via node-XMLHttpRequest."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#scales-d3-scale",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#scales-d3-scale",
    "title": "1 Changes in D3 5.0",
    "section": "2.23 Scales (d3-scale)",
    "text": "2.23 Scales (d3-scale)\nPursuant to the great namespace flattening:\n\nd3.scale.linear ‚Ü¶ d3.scaleLinear\nd3.scale.sqrt ‚Ü¶ d3.scaleSqrt\nd3.scale.pow ‚Ü¶ d3.scalePow\nd3.scale.log ‚Ü¶ d3.scaleLog\nd3.scale.quantize ‚Ü¶ d3.scaleQuantize\nd3.scale.threshold ‚Ü¶ d3.scaleThreshold\nd3.scale.quantile ‚Ü¶ d3.scaleQuantile\nd3.scale.identity ‚Ü¶ d3.scaleIdentity\nd3.scale.ordinal ‚Ü¶ d3.scaleOrdinal\nd3.time.scale ‚Ü¶ d3.scaleTime\nd3.time.scale.utc ‚Ü¶ d3.scaleUtc\n\nScales now generate ticks in the same order as the domain: if you have a descending domain, you now get descending ticks. This change affects the order of tick elements generated by axes. For example:\nd3.scaleLinear().domain([10, 0]).ticks(5); // [10, 8, 6, 4, 2, 0]\nLog tick formatting now assumes a default count of ten, not Infinity, if not specified. Log scales with domains that span many powers (such as from 1e+3 to 1e+29) now return only one tick per power rather than returning base ticks per power. Non-linear quantitative scales are slightly more accurate.\nYou can now control whether an ordinal scale‚Äôs domain is implicitly extended when the scale is passed a value that is not already in its domain. By default, ordinal.unknown is d3.scaleImplicit, causing unknown values to be added to the domain:\nvar x = d3.scaleOrdinal()\n    .domain([0, 1])\n    .range([\"red\", \"green\", \"blue\"]);\n\nx.domain(); // [0, 1]\nx(2); // \"blue\"\nx.domain(); // [0, 1, 2]\nBy setting ordinal.unknown, you instead define the output value for unknown inputs. This is particularly useful for choropleth maps where you want to assign a color to missing data.\nvar x = d3.scaleOrdinal()\n    .domain([0, 1])\n    .range([\"red\", \"green\", \"blue\"])\n    .unknown(undefined);\n\nx.domain(); // [0, 1]\nx(2); // undefined\nx.domain(); // [0, 1]\nThe ordinal.rangeBands and ordinal.rangeRoundBands methods have been replaced with a new subclass of ordinal scale: band scales. The following code in 3.x:\nvar x = d3.scale.ordinal()\n    .domain([\"a\", \"b\", \"c\"])\n    .rangeBands([0, width]);\nIs equivalent to this in 4.0:\nvar x = d3.scaleBand()\n    .domain([\"a\", \"b\", \"c\"])\n    .range([0, width]);\nThe new band.padding, band.paddingInner and band.paddingOuter methods replace the optional arguments to ordinal.rangeBands. The new band.bandwidth and band.step methods replace ordinal.rangeBand. There‚Äôs also a new band.align method which you can use to control how the extra space outside the bands is distributed, say to shift columns closer to the y-axis.\nSimilarly, the ordinal.rangePoints and ordinal.rangeRoundPoints methods have been replaced with a new subclass of ordinal scale: point scales. The following code in 3.x:\nvar x = d3.scale.ordinal()\n    .domain([\"a\", \"b\", \"c\"])\n    .rangePoints([0, width]);\nIs equivalent to this in 4.0:\nvar x = d3.scalePoint()\n    .domain([\"a\", \"b\", \"c\"])\n    .range([0, width]);\nThe new point.padding method replaces the optional padding argument to ordinal.rangePoints. Like ordinal.rangeBand with ordinal.rangePoints, the point.bandwidth method always returns zero; a new point.step method returns the interval between adjacent points.\nThe ordinal scale constructor now takes an optional range for a shorter alternative to ordinal.range. This is especially useful now that the categorical color scales have been changed to simple arrays of colors rather than specialized ordinal scale constructors:\n\nd3.scale.category10 ‚Ü¶ d3.schemeCategory10\nd3.scale.category20 ‚Ü¶ d3.schemeCategory20\nd3.scale.category20b ‚Ü¶ d3.schemeCategory20b\nd3.scale.category20c ‚Ü¶ d3.schemeCategory20c\n\nThe following code in 3.x:\nvar color = d3.scale.category10();\nIs equivalent to this in 4.0:\nvar color = d3.scaleOrdinal(d3.schemeCategory10);\nSequential scales, are a new class of scales with a fixed output interpolator instead of a range. Typically these scales are used to implement continuous sequential or diverging color schemes. Inspired by Matplotlib‚Äôs new perceptually-motived colormaps, 4.0 now features viridis, inferno, magma, plasma interpolators for use with sequential scales. Using d3.quantize, these interpolators can also be applied to quantile, quantize and threshold scales.\n   \n4.0 also ships new Cubehelix schemes, including Dave Green‚Äôs default and a cyclical rainbow inspired by Matteo Niccoli:\n   \nFor even more sequential and categorical color schemes, see d3-scale-chromatic.\nFor an introduction to scales, see Introducing d3-scale."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#selections-d3-selection",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#selections-d3-selection",
    "title": "1 Changes in D3 5.0",
    "section": "2.24 Selections (d3-selection)",
    "text": "2.24 Selections (d3-selection)\nSelections no longer subclass Array using prototype chain injection; they are now plain objects, improving performance. The internal fields (selection._groups, selection._parents) are private; please use the documented public API to manipulate selections. The new selection.nodes method generates an array of all nodes in a selection.\nSelections are now immutable: the elements and parents in a selection never change. (The elements‚Äô attributes and content will of course still be modified!) The selection.sort and selection.data methods now return new selections rather than modifying the selection in-place. In addition, selection.append no longer merges entering nodes into the update selection; use selection.merge to combine enter and update after a data join. For example, the following general update pattern in 3.x:\nvar circle = svg.selectAll(\"circle\").data(data) // UPDATE\n    .style(\"fill\", \"blue\");\n\ncircle.exit().remove(); // EXIT\n\ncircle.enter().append(\"circle\") // ENTER; modifies UPDATE! üå∂\n    .style(\"fill\", \"green\");\n\ncircle // ENTER + UPDATE\n    .style(\"stroke\", \"black\");\nWould be rewritten in 4.0 as:\nvar circle = svg.selectAll(\"circle\").data(data) // UPDATE\n    .style(\"fill\", \"blue\");\n\ncircle.exit().remove(); // EXIT\n\ncircle.enter().append(\"circle\") // ENTER\n    .style(\"fill\", \"green\")\n  .merge(circle) // ENTER + UPDATE\n    .style(\"stroke\", \"black\");\nThis change is discussed further in What Makes Software Good.\nIn 3.x, the selection.enter and selection.exit methods were undefined until you called selection.data, resulting in a TypeError if you attempted to access them. In 4.0, now they simply return the empty selection if the selection has not been joined to data.\nIn 3.x, selection.append would always append the new element as the last child of its parent. A little-known trick was to use selection.insert without specifying a before selector when entering nodes, causing the entering nodes to be inserted before the following element in the update selection. In 4.0, this is now the default behavior of selection.append; if you do not specify a before selector to selection.insert, the inserted element is appended as the last child. This change makes the general update pattern preserve the relative order of elements and data. For example, given the following DOM:\n&lt;div&gt;a&lt;/div&gt;\n&lt;div&gt;b&lt;/div&gt;\n&lt;div&gt;f&lt;/div&gt;\nAnd the following code:\nvar div = d3.select(\"body\").selectAll(\"div\")\n  .data([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], function(d) { return d || this.textContent; });\n\ndiv.enter().append(\"div\")\n    .text(function(d) { return d; });\nThe resulting DOM will be:\n&lt;div&gt;a&lt;/div&gt;\n&lt;div&gt;b&lt;/div&gt;\n&lt;div&gt;c&lt;/div&gt;\n&lt;div&gt;d&lt;/div&gt;\n&lt;div&gt;e&lt;/div&gt;\n&lt;div&gt;f&lt;/div&gt;\nThus, the entering c, d and e are inserted before f, since f is the following element in the update selection. Although this behavior is sufficient to preserve order if the new data‚Äôs order is stable, if the data changes order, you must still use selection.order to reorder elements.\nThere is now only one class of selection. 3.x implemented enter selections using a special class with different behavior for enter.append and enter.select; a consequence of this design was that enter selections in 3.x lacked certain methods. In 4.0, enter selections are simply normal selections; they have the same methods and the same behavior. Placeholder enter nodes now implement node.appendChild, node.insertBefore, node.querySelector, and node.querySelectorAll.\nThe selection.data method has been changed slightly with respect to duplicate keys. In 3.x, if multiple data had the same key, the duplicate data would be ignored and not included in enter, update or exit; in 4.0 the duplicate data is always put in the enter selection. In both 3.x and 4.0, if multiple elements have the same key, the duplicate elements are put in the exit selection. Thus, 4.0‚Äôs behavior is now symmetric for enter and exit, and the general update pattern will now produce a DOM that matches the data even if there are duplicate keys.\nSelections have several new methods! Use selection.raise to move the selected elements to the front of their siblings, so that they are drawn on top; use selection.lower to move them to the back. Use selection.dispatch to dispatch a custom event to event listeners.\nWhen called in getter mode, selection.data now returns the data for all elements in the selection, rather than just the data for the first group of elements. The selection.call method no longer sets the this context when invoking the specified function; the selection is passed as the first argument to the function, so use that. The selection.on method now accepts multiple whitespace-separated typenames, so you can add or remove multiple listeners simultaneously. For example:\nselection.on(\"mousedown touchstart\", function() {\n  console.log(d3.event.type);\n});\nThe arguments passed to callback functions has changed slightly in 4.0 to be more consistent. The standard arguments are the element‚Äôs datum (d), the element‚Äôs index (i), and the element‚Äôs group (nodes), with this as the element. The slight exception to this convention is selection.data, which is evaluated for each group rather than each element; it is passed the group‚Äôs parent datum (d), the group index (i), and the selection‚Äôs parents (parents), with this as the group‚Äôs parent.\nThe new d3.local provides a mechanism for defining local variables: state that is bound to DOM elements, and available to any descendant element. This can be a convenient alternative to using selection.each or storing local state in data.\nThe d3.ns.prefix namespace prefix map has been renamed to d3.namespaces, and the d3.ns.qualify method has been renamed to d3.namespace. Several new low-level methods are now available, as well. d3.matcher is used internally by selection.filter; d3.selector is used by selection.select; d3.selectorAll is used by selection.selectAll; d3.creator is used by selection.append and selection.insert. The new d3.window returns the owner window for a given element, window or document. The new d3.customEvent temporarily sets d3.event while invoking a function, allowing you to implement controls which dispatch custom events; this is used by d3-drag, d3-zoom and d3-brush.\nFor the sake of parsimony, the multi-value methods‚Äîwhere you pass an object to set multiple attributes, styles or properties simultaneously‚Äîhave been extracted to d3-selection-multi and are no longer part of the default bundle. The multi-value map methods have also been renamed to plural form to reduce overload: selection.attrs, selection.styles and selection.properties."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#shapes-d3-shape",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#shapes-d3-shape",
    "title": "1 Changes in D3 5.0",
    "section": "2.25 Shapes (d3-shape)",
    "text": "2.25 Shapes (d3-shape)\nPursuant to the great namespace flattening:\n\nd3.svg.line ‚Ü¶ d3.line\nd3.svg.line.radial ‚Ü¶ d3.radialLine\nd3.svg.area ‚Ü¶ d3.area\nd3.svg.area.radial ‚Ü¶ d3.radialArea\nd3.svg.arc ‚Ü¶ d3.arc\nd3.svg.symbol ‚Ü¶ d3.symbol\nd3.svg.symbolTypes ‚Ü¶ d3.symbolTypes\nd3.layout.pie ‚Ü¶ d3.pie\nd3.layout.stack ‚Ü¶ d3.stack\nd3.svg.diagonal ‚Ü¶ REMOVED (see d3/d3-shape#27)\nd3.svg.diagonal.radial ‚Ü¶ REMOVED\n\nShapes are no longer limited to SVG; they can now render to Canvas! Shape generators now support an optional context: given a CanvasRenderingContext2D, you can render a shape as a canvas path to be filled or stroked. For example, a canvas pie chart might use an arc generator:\nvar arc = d3.arc()\n    .outerRadius(radius - 10)\n    .innerRadius(0)\n    .context(context);\nTo render an arc for a given datum d:\ncontext.beginPath();\narc(d);\ncontext.fill();\nSee line.context, area.context and arc.context for more. Under the hood, shapes use d3-path to serialize canvas path methods to SVG path data when the context is null; thus, shapes are optimized for rendering to canvas. You can also now derive lines from areas. The line shares most of the same accessors, such as line.defined and line.curve, with the area from which it is derived. For example, to render the topline of an area, use area.lineY1; for the baseline, use area.lineY0.\n4.0 introduces a new curve API for specifying how line and area shapes interpolate between data points. The line.interpolate and area.interpolate methods have been replaced with line.curve and area.curve. Curves are implemented using the curve interface rather than as a function that returns an SVG path data string; this allows curves to render to either SVG or Canvas. In addition, line.curve and area.curve now take a function which instantiates a curve for a given context, rather than a string. The full list of equivalents:\n\nlinear ‚Ü¶ d3.curveLinear\nlinear-closed ‚Ü¶ d3.curveLinearClosed\nstep ‚Ü¶ d3.curveStep\nstep-before ‚Ü¶ d3.curveStepBefore\nstep-after ‚Ü¶ d3.curveStepAfter\nbasis ‚Ü¶ d3.curveBasis\nbasis-open ‚Ü¶ d3.curveBasisOpen\nbasis-closed ‚Ü¶ d3.curveBasisClosed\nbundle ‚Ü¶ d3.curveBundle\ncardinal ‚Ü¶ d3.curveCardinal\ncardinal-open ‚Ü¶ d3.curveCardinalOpen\ncardinal-closed ‚Ü¶ d3.curveCardinalClosed\nmonotone ‚Ü¶ d3.curveMonotoneX\n\nBut that‚Äôs not all! 4.0 now provides parameterized Catmull‚ÄìRom splines as proposed by Yuksel et al.. These are available as d3.curveCatmullRom, d3.curveCatmullRomClosed and d3.curveCatmullRomOpen.\n  \nEach curve type can define its own named parameters, replacing line.tension and area.tension. For example, Catmull‚ÄìRom splines are parameterized using catmullRom.alpha and defaults to 0.5, which corresponds to a centripetal spline that avoids self-intersections and overshoot. For a uniform Catmull‚ÄìRom spline instead:\nvar line = d3.line()\n    .curve(d3.curveCatmullRom.alpha(0));\n4.0 fixes the interpretation of the cardinal spline tension parameter, which is now specified as cardinal.tension and defaults to zero for a uniform Catmull‚ÄìRom spline; a tension of one produces a linear curve. The first and last segments of basis and cardinal curves have also been fixed! The undocumented interpolate.reverse field has been removed. Curves can define different behavior for toplines and baselines by counting the sequence of curve.lineStart within curve.areaStart. See the d3.curveStep implementation for an example.\n4.0 fixes numerous bugs in the monotone curve implementation, and introduces d3.curveMonotoneY; this is like d3.curveMonotoneX, except it requires that the input points are monotone in y rather than x, such as for a vertically-oriented line chart. The new d3.curveNatural produces a natural cubic spline. The default Œ≤ for d3.curveBundle is now 0.85, rather than 0.7, matching the values used by Holten. 4.0 also has a more robust implementation of arc padding; see arc.padAngle and arc.padRadius.\n4.0 introduces a new symbol type API. Symbol types are passed to symbol.type in place of strings. The equivalents are:\n\ncircle ‚Ü¶ d3.symbolCircle\ncross ‚Ü¶ d3.symbolCross\ndiamond ‚Ü¶ d3.symbolDiamond\nsquare ‚Ü¶ d3.symbolSquare\ntriangle-down ‚Ü¶ REMOVED\ntriangle-up ‚Ü¶ d3.symbolTriangle\nADDED ‚Ü¶ d3.symbolStar\nADDED ‚Ü¶ d3.symbolWye\n\nThe full set of symbol types is now:\n\nLastly, 4.0 overhauls the stack layout API, replacing d3.layout.stack with d3.stack. The stack generator no longer needs an x-accessor. In addition, the API has been simplified: the stack generator now accepts tabular input, such as this array of objects:\nvar data = [\n  {month: new Date(2015, 0, 1), apples: 3840, bananas: 1920, cherries: 960, dates: 400},\n  {month: new Date(2015, 1, 1), apples: 1600, bananas: 1440, cherries: 960, dates: 400},\n  {month: new Date(2015, 2, 1), apples:  640, bananas:  960, cherries: 640, dates: 400},\n  {month: new Date(2015, 3, 1), apples:  320, bananas:  480, cherries: 640, dates: 400}\n];\nTo generate the stack layout, first define a stack generator, and then apply it to the data:\nvar stack = d3.stack()\n    .keys([\"apples\", \"bananas\", \"cherries\", \"dates\"])\n    .order(d3.stackOrderNone)\n    .offset(d3.stackOffsetNone);\n\nvar series = stack(data);\nThe resulting array has one element per series. Each series has one point per month, and each point has a lower and upper value defining the baseline and topline:\n[\n  [[   0, 3840], [   0, 1600], [   0,  640], [   0,  320]], // apples\n  [[3840, 5760], [1600, 3040], [ 640, 1600], [ 320,  800]], // bananas\n  [[5760, 6720], [3040, 4000], [1600, 2240], [ 800, 1440]], // cherries\n  [[6720, 7120], [4000, 4400], [2240, 2640], [1440, 1840]], // dates\n]\nEach series in then typically passed to an area generator to render an area chart, or used to construct rectangles for a bar chart. Stack generators no longer modify the input data, so stack.out has been removed.\nFor an introduction to shapes, see Introducing d3-shape."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#time-formats-d3-time-format",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#time-formats-d3-time-format",
    "title": "1 Changes in D3 5.0",
    "section": "2.26 Time Formats (d3-time-format)",
    "text": "2.26 Time Formats (d3-time-format)\nPursuant to the great namespace flattening, the format constructors have new names:\n\nd3.time.format ‚Ü¶ d3.timeFormat\nd3.time.format.utc ‚Ü¶ d3.utcFormat\nd3.time.format.iso ‚Ü¶ d3.isoFormat\n\nThe format.parse method has also been removed in favor of separate d3.timeParse, d3.utcParse and d3.isoParse parser constructors. Thus, this code in 3.x:\nvar parseTime = d3.time.format(\"%c\").parse;\nCan be rewritten in 4.0 as:\nvar parseTime = d3.timeParse(\"%c\");\nThe multi-scale time format d3.time.format.multi has been replaced by d3.scaleTime‚Äôs tick format. Time formats now coerce inputs to dates, and time parsers coerce inputs to strings. The %Z directive now allows more flexible parsing of time zone offsets, such as -0700, -07:00, -07, and Z. The %p directive is now parsed correctly when the locale‚Äôs period name is longer than two characters (e.g., ‚Äúa.m.‚Äù).\nThe default U.S. English locale now uses 12-hour time and a more concise representation of the date. This aligns with local convention and is consistent with date.toLocaleString in Chrome, Firefox and Node:\nvar now = new Date;\nd3.timeFormat(\"%c\")(new Date); // \"6/23/2016, 2:01:33 PM\"\nd3.timeFormat(\"%x\")(new Date); // \"6/23/2016\"\nd3.timeFormat(\"%X\")(new Date); // \"2:01:38 PM\"\nYou can now set the default locale using d3.timeFormatDefaultLocale! The locales are published as JSON to npm.\nThe performance of time formatting and parsing has been improved, and the UTC formatter and parser have a cleaner implementation (that avoids temporarily overriding the Date global)."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#time-intervals-d3-time",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#time-intervals-d3-time",
    "title": "1 Changes in D3 5.0",
    "section": "2.27 Time Intervals (d3-time)",
    "text": "2.27 Time Intervals (d3-time)\nPursuant to the great namespace flattening, the local time intervals have been renamed:\n\nADDED ‚Ü¶ d3.timeMillisecond\nd3.time.second ‚Ü¶ d3.timeSecond\nd3.time.minute ‚Ü¶ d3.timeMinute\nd3.time.hour ‚Ü¶ d3.timeHour\nd3.time.day ‚Ü¶ d3.timeDay\nd3.time.sunday ‚Ü¶ d3.timeSunday\nd3.time.monday ‚Ü¶ d3.timeMonday\nd3.time.tuesday ‚Ü¶ d3.timeTuesday\nd3.time.wednesday ‚Ü¶ d3.timeWednesday\nd3.time.thursday ‚Ü¶ d3.timeThursday\nd3.time.friday ‚Ü¶ d3.timeFriday\nd3.time.saturday ‚Ü¶ d3.timeSaturday\nd3.time.week ‚Ü¶ d3.timeWeek\nd3.time.month ‚Ü¶ d3.timeMonth\nd3.time.year ‚Ü¶ d3.timeYear\n\nThe UTC time intervals have likewise been renamed:\n\nADDED ‚Ü¶ d3.utcMillisecond\nd3.time.second.utc ‚Ü¶ d3.utcSecond\nd3.time.minute.utc ‚Ü¶ d3.utcMinute\nd3.time.hour.utc ‚Ü¶ d3.utcHour\nd3.time.day.utc ‚Ü¶ d3.utcDay\nd3.time.sunday.utc ‚Ü¶ d3.utcSunday\nd3.time.monday.utc ‚Ü¶ d3.utcMonday\nd3.time.tuesday.utc ‚Ü¶ d3.utcTuesday\nd3.time.wednesday.utc ‚Ü¶ d3.utcWednesday\nd3.time.thursday.utc ‚Ü¶ d3.utcThursday\nd3.time.friday.utc ‚Ü¶ d3.utcFriday\nd3.time.saturday.utc ‚Ü¶ d3.utcSaturday\nd3.time.week.utc ‚Ü¶ d3.utcWeek\nd3.time.month.utc ‚Ü¶ d3.utcMonth\nd3.time.year.utc ‚Ü¶ d3.utcYear\n\nThe local time range aliases have been renamed:\n\nd3.time.seconds ‚Ü¶ d3.timeSeconds\nd3.time.minutes ‚Ü¶ d3.timeMinutes\nd3.time.hours ‚Ü¶ d3.timeHours\nd3.time.days ‚Ü¶ d3.timeDays\nd3.time.sundays ‚Ü¶ d3.timeSundays\nd3.time.mondays ‚Ü¶ d3.timeMondays\nd3.time.tuesdays ‚Ü¶ d3.timeTuesdays\nd3.time.wednesdays ‚Ü¶ d3.timeWednesdays\nd3.time.thursdays ‚Ü¶ d3.timeThursdays\nd3.time.fridays ‚Ü¶ d3.timeFridays\nd3.time.saturdays ‚Ü¶ d3.timeSaturdays\nd3.time.weeks ‚Ü¶ d3.timeWeeks\nd3.time.months ‚Ü¶ d3.timeMonths\nd3.time.years ‚Ü¶ d3.timeYears\n\nThe UTC time range aliases have been renamed:\n\nd3.time.seconds.utc ‚Ü¶ d3.utcSeconds\nd3.time.minutes.utc ‚Ü¶ d3.utcMinutes\nd3.time.hours.utc ‚Ü¶ d3.utcHours\nd3.time.days.utc ‚Ü¶ d3.utcDays\nd3.time.sundays.utc ‚Ü¶ d3.utcSundays\nd3.time.mondays.utc ‚Ü¶ d3.utcMondays\nd3.time.tuesdays.utc ‚Ü¶ d3.utcTuesdays\nd3.time.wednesdays.utc ‚Ü¶ d3.utcWednesdays\nd3.time.thursdays.utc ‚Ü¶ d3.utcThursdays\nd3.time.fridays.utc ‚Ü¶ d3.utcFridays\nd3.time.saturdays.utc ‚Ü¶ d3.utcSaturdays\nd3.time.weeks.utc ‚Ü¶ d3.utcWeeks\nd3.time.months.utc ‚Ü¶ d3.utcMonths\nd3.time.years.utc ‚Ü¶ d3.utcYears\n\nThe behavior of interval.range (and the convenience aliases such as d3.timeDays) has been changed when step is greater than one. Rather than filtering the returned dates using the field number, interval.range now behaves like d3.range: it simply skips, returning every stepth date. For example, the following code in 3.x returns only odd days of the month:\nd3.time.days(new Date(2016, 4, 28), new Date(2016, 5, 5), 2);\n// [Sun May 29 2016 00:00:00 GMT-0700 (PDT),\n//  Tue May 31 2016 00:00:00 GMT-0700 (PDT),\n//  Wed Jun 01 2016 00:00:00 GMT-0700 (PDT),\n//  Fri Jun 03 2016 00:00:00 GMT-0700 (PDT)]\nNote the returned array of dates does not start on the start date because May 28 is even. Also note that May 31 and June 1 are one day apart, not two! The behavior of d3.timeDays in 4.0 is probably closer to what you expect:\nd3.timeDays(new Date(2016, 4, 28), new Date(2016, 5, 5), 2);\n// [Sat May 28 2016 00:00:00 GMT-0700 (PDT),\n//  Mon May 30 2016 00:00:00 GMT-0700 (PDT),\n//  Wed Jun 01 2016 00:00:00 GMT-0700 (PDT),\n//  Fri Jun 03 2016 00:00:00 GMT-0700 (PDT)]\nIf you want a filtered view of a time interval (say to guarantee that two overlapping ranges are consistent, such as when generating time scale ticks), you can use the new interval.every method or its more general cousin interval.filter:\nd3.timeDay.every(2).range(new Date(2016, 4, 28), new Date(2016, 5, 5));\n// [Sun May 29 2016 00:00:00 GMT-0700 (PDT),\n//  Tue May 31 2016 00:00:00 GMT-0700 (PDT),\n//  Wed Jun 01 2016 00:00:00 GMT-0700 (PDT),\n//  Fri Jun 03 2016 00:00:00 GMT-0700 (PDT)]\nTime intervals now expose an interval.count method for counting the number of interval boundaries after a start date and before or equal to an end date. This replaces d3.time.dayOfYear and related methods in 3.x. For example, this code in 3.x:\nvar now = new Date;\nd3.time.dayOfYear(now); // 165\nCan be rewritten in 4.0 as:\nvar now = new Date;\nd3.timeDay.count(d3.timeYear(now), now); // 165\nLikewise, in place of 3.x‚Äôs d3.time.weekOfYear, in 4.0 you would say:\nd3.timeWeek.count(d3.timeYear(now), now); // 24\nThe new interval.count is of course more general. For example, you can use it to compute hour-of-week for a heatmap:\nd3.timeHour.count(d3.timeWeek(now), now); // 64\nHere are all the equivalences from 3.x to 4.0:\n\nd3.time.dayOfYear ‚Ü¶ d3.timeDay.count\nd3.time.sundayOfYear ‚Ü¶ d3.timeSunday.count\nd3.time.mondayOfYear ‚Ü¶ d3.timeMonday.count\nd3.time.tuesdayOfYear ‚Ü¶ d3.timeTuesday.count\nd3.time.wednesdayOfYear ‚Ü¶ d3.timeWednesday.count\nd3.time.thursdayOfYear ‚Ü¶ d3.timeThursday.count\nd3.time.fridayOfYear ‚Ü¶ d3.timeFriday.count\nd3.time.saturdayOfYear ‚Ü¶ d3.timeSaturday.count\nd3.time.weekOfYear ‚Ü¶ d3.timeWeek.count\nd3.time.dayOfYear.utc ‚Ü¶ d3.utcDay.count\nd3.time.sundayOfYear.utc ‚Ü¶ d3.utcSunday.count\nd3.time.mondayOfYear.utc ‚Ü¶ d3.utcMonday.count\nd3.time.tuesdayOfYear.utc ‚Ü¶ d3.utcTuesday.count\nd3.time.wednesdayOfYear.utc ‚Ü¶ d3.utcWednesday.count\nd3.time.thursdayOfYear.utc ‚Ü¶ d3.utcThursday.count\nd3.time.fridayOfYear.utc ‚Ü¶ d3.utcFriday.count\nd3.time.saturdayOfYear.utc ‚Ü¶ d3.utcSaturday.count\nd3.time.weekOfYear.utc ‚Ü¶ d3.utcWeek.count\n\nD3 4.0 now also lets you define custom time intervals using d3.timeInterval. The d3.timeYear, d3.utcYear, d3.timeMillisecond and d3.utcMillisecond intervals have optimized implementations of interval.every, which is necessary to generate time ticks for very large or very small domains efficiently. More generally, the performance of time intervals has been improved, and time intervals now do a better job with respect to daylight savings in various locales."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#timers-d3-timer",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#timers-d3-timer",
    "title": "1 Changes in D3 5.0",
    "section": "2.28 Timers (d3-timer)",
    "text": "2.28 Timers (d3-timer)\nIn D3 3.x, the only way to stop a timer was for its callback to return true. For example, this timer stops after one second:\nd3.timer(function(elapsed) {\n  console.log(elapsed);\n  return elapsed &gt;= 1000;\n});\nIn 4.0, use timer.stop instead:\nvar t = d3.timer(function(elapsed) {\n  console.log(elapsed);\n  if (elapsed &gt;= 1000) {\n    t.stop();\n  }\n});\nThe primary benefit of timer.stop is that timers are not required to self-terminate: they can be stopped externally, allowing for the immediate and synchronous disposal of associated resources, and the separation of concerns. The above is equivalent to:\nvar t = d3.timer(function(elapsed) {\n  console.log(elapsed);\n});\n\nd3.timeout(function() {\n  t.stop();\n}, 1000);\nThis improvement extends to d3-transition: now when a transition is interrupted, its resources are immediately freed rather than having to wait for transition to start.\n4.0 also introduces a new timer.restart method for restarting timers, for replacing the callback of a running timer, or for changing its delay or reference time. Unlike timer.stop followed by d3.timer, timer.restart maintains the invocation priority of an existing timer: it guarantees that the order of invocation of active timers remains the same. The d3.timer.flush method has been renamed to d3.timerFlush.\nSome usage patterns in D3 3.x could cause the browser to hang when a background page returned to the foreground. For example, the following code schedules a transition every second:\nsetInterval(function() {\n  d3.selectAll(\"div\").transition().call(someAnimation); // BAD\n}, 1000);\nIf such code runs in the background for hours, thousands of queued transitions will try to run simultaneously when the page is foregrounded. D3 4.0 avoids this hang by freezing time in the background: when a page is in the background, time does not advance, and so no queue of timers accumulates to run when the page returns to the foreground. Use d3.timer instead of transitions to schedule a long-running animation, or use d3.timeout and d3.interval in place of setTimeout and setInterval to prevent transitions from being queued in the background:\nd3.interval(function() {\n  d3.selectAll(\"div\").transition().call(someAnimation); // GOOD\n}, 1000);\nBy freezing time in the background, timers are effectively ‚Äúunaware‚Äù of being backgrounded. It‚Äôs like nothing happened! 4.0 also now uses high-precision time (performance.now) where available; the current time is available as d3.now."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#transitions-d3-transition",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#transitions-d3-transition",
    "title": "1 Changes in D3 5.0",
    "section": "2.29 Transitions (d3-transition)",
    "text": "2.29 Transitions (d3-transition)\nThe selection.transition method now takes an optional transition instance which can be used to synchronize a new transition with an existing transition. (This change is discussed further in What Makes Software Good?) For example:\nvar t = d3.transition()\n    .duration(750)\n    .ease(d3.easeLinear);\n\nd3.selectAll(\".apple\").transition(t)\n    .style(\"fill\", \"red\");\n\nd3.selectAll(\".orange\").transition(t)\n    .style(\"fill\", \"orange\");\nTransitions created this way inherit timing from the closest ancestor element, and thus are synchronized even when the referenced transition has variable timing such as a staggered delay. This method replaces the deeply magical behavior of transition.each in 3.x; in 4.0, transition.each is identical to selection.each. Use the new transition.on method to listen to transition events.\nThe meaning of transition.delay has changed for chained transitions created by transition.transition. The specified delay is now relative to the previous transition in the chain, rather than the first transition in the chain; this makes it easier to insert interstitial pauses. For example:\nd3.selectAll(\".apple\")\n  .transition() // First fade to green.\n    .style(\"fill\", \"green\")\n  .transition() // Then red.\n    .style(\"fill\", \"red\")\n  .transition() // Wait one second. Then brown, and remove.\n    .delay(1000)\n    .style(\"fill\", \"brown\")\n    .remove();\nTime is now frozen in the background; see d3-timer for more information. While it was previously the case that transitions did not run in the background, now they pick up where they left off when the page returns to the foreground. This avoids page hangs by not scheduling an unbounded number of transitions in the background. If you want to schedule an infinitely-repeating transition, use transition events, or use d3.timeout and d3.interval in place of setTimeout and setInterval.\nThe selection.interrupt method now cancels all scheduled transitions on the selected elements, in addition to interrupting any active transition. When transitions are interrupted, any resources associated with the transition are now released immediately, rather than waiting until the transition starts, improving performance. (See also timer.stop.) The new d3.interrupt method is an alternative to selection.interrupt for quickly interrupting a single node.\nThe new d3.active method allows you to select the currently-active transition on a given node, if any. This is useful for modifying in-progress transitions and for scheduling infinitely-repeating transitions. For example, this transition continuously oscillates between red and blue:\nd3.select(\"circle\")\n  .transition()\n    .on(\"start\", function repeat() {\n        d3.active(this)\n            .style(\"fill\", \"red\")\n          .transition()\n            .style(\"fill\", \"blue\")\n          .transition()\n            .on(\"start\", repeat);\n      });\nThe life cycle of a transition is now more formally defined and enforced. For example, attempting to change the duration of a running transition now throws an error rather than silently failing. The transition.remove method has been fixed if multiple transition names are in use: the element is only removed if it has no scheduled transitions, regardless of name. The transition.ease method now always takes an easing function, not a string. When a transition ends, the tweens are invoked one last time with t equal to exactly 1, regardless of the associated easing function.\nAs with selections in 4.0, all transition callback functions now receive the standard arguments: the element‚Äôs datum (d), the element‚Äôs index (i), and the element‚Äôs group (nodes), with this as the element. This notably affects transition.attrTween and transition.styleTween, which no longer pass the tween function the current attribute or style value as the third argument. The transition.attrTween and transition.styleTween methods can now be called in getter modes for debugging or to share tween definitions between transitions.\nHomogenous transitions are now optimized! If all elements in a transition share the same tween, interpolator, or event listeners, this state is now shared across the transition rather than separately allocated for each element. 4.0 also uses an optimized default interpolator in place of d3.interpolate for transition.attr and transition.style. And transitions can now interpolate both CSS and SVG transforms.\nFor reusable components that support transitions, such as axes, a new transition.selection method returns the selection that corresponds to a given transition. There is also a new transition.merge method that is equivalent to selection.merge.\nFor the sake of parsimony, the multi-value map methods have been extracted to d3-selection-multi and are no longer part of the default bundle. The multi-value map methods have also been renamed to plural form to reduce overload: transition.attrs and transition.styles."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#voronoi-diagrams-d3-voronoi",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#voronoi-diagrams-d3-voronoi",
    "title": "1 Changes in D3 5.0",
    "section": "2.30 Voronoi Diagrams (d3-voronoi)",
    "text": "2.30 Voronoi Diagrams (d3-voronoi)\nThe d3.geom.voronoi method has been renamed to d3.voronoi, and the voronoi.clipExtent method has been renamed to voronoi.extent. The undocumented polygon.point property in 3.x, which is the element in the input data corresponding to the polygon, has been renamed to polygon.data.\nCalling voronoi now returns the full Voronoi diagram, which includes topological information: each Voronoi edge exposes edge.left and edge.right specifying the sites on either side of the edge, and each Voronoi cell is defined as an array of these edges and a corresponding site. The Voronoi diagram can be used to efficiently compute both the Voronoi and Delaunay tessellations for a set of points: diagram.polygons, diagram.links, and diagram.triangles. The new topology is also useful in conjunction with TopoJSON; see the Voronoi topology example.\nThe voronoi.polygons and diagram.polygons now require an extent; there is no longer an implicit extent of ¬±1e6. The voronoi.links, voronoi.triangles, diagram.links and diagram.triangles are now affected by the clip extent: as the Delaunay is computed as the dual of the Voronoi, two sites are only linked if the clipped cells are touching. To compute the Delaunay triangulation without respect to clipping, set the extent to null.\nThe Voronoi generator finally has well-defined behavior for coincident vertices: the first of a set of coincident points has a defined cell, while the subsequent duplicate points have null cells. The returned array of polygons is sparse, so by using array.forEach or array.map, you can easily skip undefined cells. The Voronoi generator also now correctly handles the case where no cell edges intersect the extent."
  },
  {
    "objectID": "docs/site_libs/d3v5-5.9.2/CHANGES.html#zooming-d3-zoom",
    "href": "docs/site_libs/d3v5-5.9.2/CHANGES.html#zooming-d3-zoom",
    "title": "1 Changes in D3 5.0",
    "section": "2.31 Zooming (d3-zoom)",
    "text": "2.31 Zooming (d3-zoom)\nThe zoom behavior d3.behavior.zoom has been renamed to d3.zoom. Zoom behaviors no longer store the active zoom transform (i.e., the visible region; the scale and translate) internally. The zoom transform is now stored on any elements to which the zoom behavior has been applied. The zoom transform is available as event.transform within a zoom event or by calling d3.zoomTransform on a given element. To zoom programmatically, use zoom.transform with a given selection or transition; see the zoom transitions example. The zoom.event method has been removed.\nTo make programmatic zooming easier, there are several new convenience methods on top of zoom.transform: zoom.translateBy, zoom.scaleBy and zoom.scaleTo. There is also a new API for describing zoom transforms. Zoom behaviors are no longer dependent on scales, but you can use transform.rescaleX, transform.rescaleY, transform.invertX or transform.invertY to transform a scale‚Äôs domain. 3.x‚Äôs event.scale is replaced with event.transform.k, and event.translate is replaced with event.transform.x and event.transform.y. The zoom.center method has been removed in favor of programmatic zooming.\nThe zoom behavior finally supports simple constraints on panning! The new zoom.translateExtent lets you define the viewable extent of the world: the currently-visible extent (the extent of the viewport, as defined by zoom.extent) is always contained within the translate extent. The zoom.size method has been replaced by zoom.extent, and the default behavior is now smarter: it defaults to the extent of the zoom behavior‚Äôs owner element, rather than being hardcoded to 960√ó500. (This also improves the default path chosen during smooth zoom transitions!)\nThe zoom behavior‚Äôs interaction has also improved. It now correctly handles concurrent wheeling and dragging, as well as concurrent touching and mousing. The zoom behavior now ignores wheel events at the limits of its scale extent, allowing you to scroll past a zoomable area. The zoomstart and zoomend events have been renamed start and end. By default, zoom behaviors now ignore right-clicks intended for the context menu; use zoom.filter to control which events are ignored. The zoom behavior also ignores emulated mouse events on iOS. The zoom behavior now consumes handled events, making it easier to combine with other interactive behaviors such as dragging."
  }
]